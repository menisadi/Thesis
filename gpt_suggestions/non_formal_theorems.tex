\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

\newtheorem{theorem}{Theorem}

\begin{document}

\paragraph{theorem 1.2.1}
% Rewriting our result in non-formal phrasing.
We begin with an algorithmically efficient version of the learner-to-compression scheme conversion in Moarn and Yehudayoff (2016).
Namely, our compression scheme size is linear in the dimension and the dual-dimension of the class. 
Furthermore, the compression scheme is computable in linear time in the initial sample size. 
For comparison,  a naive implementation of the   (2016) existence proof yields a runtime which is exponential in the dimension and the dual-dimension of the class, which means that it can be double exponential in the dimension of the class. this is without taking into account the cost of computing the minimax distribution over the game matrix defined in the proof. 

Next, we extend the result in Theorem 1.2.1 from classification to regression.
We provide an efficient  compression scheme for real-valued functions, which is computable in linear time in the initial sample size. The size of the compression is linear in the fat-shattering dimension and the dual-dimension of the class. 

