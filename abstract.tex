\chapter*{Abstract}

Machine learning is a rapidly growing field of computer science that has the potential to revolutionize various industries. With the ability to process vast amounts of data and improve performance over time, it has become a powerful tool for solving complex problems and driving innovation, and is being used in a wide range of industry applications.


In this thesis, we examine two interrelated subjects within the realm of machine learning - privacy and compression. 
The explosion of data generation and usage in recent years has brought about unprecedented opportunities for machine learning and AI-based applications. However, as data becomes increasingly personal and sensitive, the need for protecting individual privacy has become paramount. 
Differential privacy, a mathematical framework for quantifying the privacy of the computational process of an algorithm  with respect to its input, has emerged as a leading technique for addressing this challenge. 
If we wish to allow the continuation of innovation and progress in the field, and perhaps even expand it, users must be assured that the use of the databases will not allow the privacy of any individual to be compromised.
Another aspect whose importance is becoming more apparent, as databases increase, is the issue of compression. In the last few years, we have witnessed the rise of models based on huge amounts of information. Whether these are large natural language models with billions of parameters, visual analysis algorithms or image generators trained using terabytes of images from all over the web, the size of the systems and databases is starting to pose a problem.
First, it creates enormous challenges in the computational and engineering aspects involved in training such models. Nowadays, it is evident that there are many tasks in which it is impossible for anyone other than giant technology companies to make progress, since only they can carry out computations of this size.
One idea that can offer the possibility of a change in this paradigm is compression. The basis of this old idea is that patterns in the information, such as those identified by machine learning tools, could allow to compress the data into a relatively small number of records that contain all the knowledge needed for the labeling pattern. 
The same logic applies in the other direction as well. If a small number of such records can be identified, then it is possible to leverage this identification process to produce learning processes. This idea arose naturally over the years in the development of popular algorithms such as the Support Vector Machine and the Condensed Nearest Neighbor.
Although the idea of compression is a significant tool in the toolbox of research and development in the field of machine learning, the exact relationship between the concept of compression and learning includes a number of fundamental unsolved questions. This dissertation investigates the intersection of these three areas by addressing four research questions that aim to deepen our understanding of the connections between privacy, compression, and machine learning.
The first topic, that will be the basis of this thesis, stems from the following question: 
\absqs{To what extent are machine learning and compression conceptually intertwined, both qualitatively and quantitatively?}
The two concepts are known to be highly connected, and for some settings even equivalent. First, we investigate whether this equivalence can be extended to more fundamental cases and, specifically, to real-valued functions also known as regression problems. 
To tackle the challenge we start by constructing an efficient method to convert any learning algorithm to a compression scheme. We then extend this technique from the basic case of classification, meaning learning binary functions, to the broader one of regression. Thus, we obtain the first general compressed regression result, guaranteeing that the information lost is arbitrarily small.


The field of differential privacy has experienced a boom in recent years, but at the same time, there are fundamental tasks whose understanding is incomplete. In the second part of the dissertation, we explore the line of work related to the question:
\absqs{How much data is needed in order to learn from data, while guaranteeing that privacy is not violated?}
This quantification of the data size needed is referred to as the sample complexity of the problem. We examine one such task - learning axis-aligned rectangles. It is known that the dependency on the dimension must be at least linear, but prior works attaining such optimal dependency required the sample complexity to grow logarithmically in the space size. We present a novel algorithm that achieves both, as the data it requires scales linearly in the dimension and asymptotically smaller than the log of the space size. The technique used in order to attain this improvement involves the deletion of "exposed" data-points sequentially, so that the influence of each individual on the final hypothesis is limited inherently in the algorithm design.


In the third part of the dissertation, we investigate the very definition of private learning. The standard definition of learning is aimed at providing accuracy guarantees, under the assumption that the underlying distribution of the data is the worst possible each time. There is a growing voice in the research advocating that this definition is too pessimistic, i.e. it doesn’t reflect the actual properties of real-life data. This worst-case paradigm is blamed for being part of the well-known gap between theory and practice in machine learning. Moreover, this pessimistic prospect gets amplified under the privacy requirement, e.g. there are fundamental problems which are simply impossible to learn under the privacy constraint, in contrast to an easy solution without restrictions. We join this line of work, advocate the use of a more flexible model called “Universal Learning”, and investigate its advantages over the classical model. 
Finally, in the last part of this thesis, we move on to deal with adaptive data analysis. The research in this field revolves around the attempt to produce analysis tools in a  formal model for learning that aims to be closer to the process that takes place in laboratories and in various practices of data analysis. Instead of a static model of analysis where one question is asked and one answer received, in the adaptive model many research and statistical queries are asked in such a way that each query arises from the information accumulated during the prior analytical process. In such a model, which is very close to a realistic process, statistical constructions from the usual static model are not valid. Research in this area combines ideas from the privacy and compression literature, since both can be used for designing reliable algorithms under adaptive models. Under this complex yet important setting, we explore the problem of extending the tools and results from the adaptive data analysis literature to the setting of correlated examples. We provide results both for privacy-based and compression-based tools.


Overall, this dissertation aims to deepen the understanding of the intersection between differential privacy, machine learning and compression schemes, by addressing these research questions. By doing so, we hope to contribute to the development of more efficient and privacy-preserving machine learning algorithms.




