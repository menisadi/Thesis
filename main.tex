\documentclass[12pt,a4paper,oneside,onecolumn]{book}
\usepackage{pdfpages}
 

\input{header}


\graphicspath{{figures/}}
\def\etal{\emph{et al}.}
\newcommand{\imageW}[2] {
  \includegraphics[width=#2]{#1}
}
\newcommand{\draftcolor}{black}
\newcommand{\TODO}[1]{\todo[inline]{#1}}
\newcommand{\subdate}{March 23st, 2023}
\newcommand{\n}[1]{\textcolor{\draftcolor}{#1}}
\newcommand{\nc}{\draftcolor} %



\pdfstringdefDisableCommands{%
  \def\beginL{}%
  \def\endL{}%
  \def\hspace#1{}%
}

\hyphenation{meta-meric}
\hyphenation{meta-merism}
\hyphenation{ter-med}
\hyphenation{re-sponse}


\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt}

\titleformat{\chapter}[hang]{\bf\LARGE}{\thechapter}{2pc}{}
\title{A Study of Privacy and Compression in Learning Theory}



\onehalfspacing

\begin{document}

\hypersetup{pageanchor=false}

\include{acknowledgements}
\include{abstract}


\frontmatter

\setcounter{tocdepth}{2}
\tableofcontents
\listofalgorithms

\mainmatter
\hypersetup{pageanchor=true}


\chapter{Introduction}

%
%

%
%
%
%
%


 %

%

Machine learning, in general, is a field combining statistics and algorithm design. The main goal is to try and leverage information in order to produce systems and software capable of performing diverse tasks.
%
%
This field has experienced tremendous growth in recent years. Systems that grow out of machine learning find many and quite varied uses: from analyzing medical tests, to chatbots and automatic art generators, to autonomous vehicles. 
Most of the involvement in the field, both applied and research, revolves around the construction of sophisticated models and new tools that will allow the industry to continue fast-forward toward more goals and peaks. 
At the same time, there is great interest in understanding the limitations of the core concepts and possibilities inherent in each tool and in each situation. To this end, we must create a precise mathematical system that will define the situations and challenges we face and enable their systematic and meticulous research. 
First, the question \emph{"What is learning?"} must be answered - or, in a more detailed way, \emph{"What requirements must an algorithm meet in order to be considered a learning algorithm?"}. 
Intuitively, the idea is that the algorithm should improve as more information is given. However, the definition of “improvement”, situations in which the algorithm is expected to operate -  plus other significant issues - needs to be well-defined. 
Once an appropriate definition is chosen, the next critical question is \emph{"Which problems or situations are learnable and which are not?"}. Naturally, this question is not easy to answer, and many tools are required to shed light on this challenge.
 
One of the most famous and classic principles in theories dealing with learning and drawing conclusions based on information is Occam's Razor. 
This principle can be found in Aristotle's writings from two thousand years ago, stating:
\emph{"We may assume the superiority ceteris paribus of the demonstration which derives from fewer postulates or hypotheses"}  \citep{10.2307/j.ctv1228h9n.7}.
This idea was later popularized by 
William of Ockham who phrased it as \emph{"Plurality should not be posited without necessity"} \citep{duignan_1998},
defines a theory or pattern deduced from data as "good" if it is simple. 
%
%
If we wish to measure simplicity in a more precise manner, a possible option is explanation length, i.e. the shorter, the better.  
In the language of computer science, we could say that a system that has a pattern or regularity is one whose results can be compressed and described in a shorter way than detailing the results themselves.
From this idea, and in light of theories and results developed in the field of statistical learning, it has become evident that this type of compression is deeply connected to learning.

%
%
%
%

%
%

Yet, in order to study this connection, the notions of learning and of compression must be properly defined.
Since the foundation of the learning theory field, several notions of learning were proposed in an attempt to capture the characteristics of learning.
One of the main notions at the core of learning theory research, is that of Probably Approximately Correct (PAC) learning. 
We can informally describe a PAC-learning algorithm as acting on given data and outputting a hypothesis which will accurately predict, with high probability, the label of almost any newly sampled data point. 

One of the main problems in learning theory is characterizing \emph{sample complexity}, which is the amount of data required in order to guarantee PAC-learning for a given class of functions.
It is known that the sample complexity of learning a class of binary functions is proportional to its VC dimension (which we will define at Definition~\ref{def:vc-dimension}). 
For classes of real-valued functions, an analogous result was proven using the notion of \emph{Fat-Shattering} dimension (\citet{alon97scalesensitive}).
Nevertheless, various other notions of "learnability" have been found beneficial and insightful. One of them is, indeed, the idea of compression.

\section{Compression}
 %
%
%

As progressively more novel learning algorithms have been designed, one of the common aspects of note is that at the core lies a particular kind of data labeling compression: the principle of finding "representative" subsets of the data
as part of a more general \emph{Occam learning} paradigm.
Most notable is the SVM algorithm, which derives its name from the set of supporting vectors that uniquely defines the linear separator returned by the algorithm \citep{cortes1995support}.

%
%

Following this path, \citet{littlestone1986relating} established a formal framework for discussion of \emph{sample compression schemes} from the learning point of view.
In addition, they showed that for the case of binary-labeled classes compression implies learnability\!
\footnote{Lately there is growing interest in the properties and the generalization bounds of compressing-based learning algorithms, see for example \citet{gkn-aistats16, DBLP:journals/ml/GraepelHS05, cummings2016adaptive}}.

A fundamental question
posed by \citet{littlestone1986relating} in the same paper concerns the reverse implication:
Can every learner be converted into a sample compression scheme?
Or, in a more quantitative formulation:
Does every learnable class admit a constant-size
sample compression scheme?
A series of partial results 
(\citet{floyd1989space,helmbold1992learning,DBLP:journals/ml/FloydW95,ben1998combinatorial,DBLP:journals/jmlr/KuzminW07};\citet{DBLP:journals/jcss/RubinsteinBR09,DBLP:journals/jmlr/RubinsteinR12,MR3047077,livni2013honest,moran2017teaching})
culminated in \citet{moran2016sample},
which resolved the latter question.\!
\footnote{
  The refined conjecture of \citet{littlestone1986relating}, that any concept class $\class$ with VC-dimension $\vc{\class}$
  admits a compression scheme of size $\bigO{\vc{\class}}$, remains open.}

The usefulness of this link is that, while learning is a statistical notion, compression is a combinatorial one. 
Thus, linking the two by such an  equivalence could help move questions about learning to the combinatorial world, opening the research to other directions and to a wide range of tools previously not relevant to this area.

In the same way, the connection between compression and learnability can be investigated in various settings and regimes. In recent years, it has been proven to be an extremely useful tool for constructing learning algorithms for scenarios far from the classical PAC model, such as adversarial learning \citep{DBLP:conf/colt/MontasserHS19} and parametric distribution learning \citet{ashtiani2020near}.
But, at the same time, the connection in the other direction - converting learning algorithms and learnable problems into compressing schemes - was left almost untouched. Since the main binary case had been in the center of interest for so many years, and as this very setting had an equivalent open problem in category theory, natural extensions and variations had been almost not studied at all. Moreover, the tools used in order to attack and eventually solve the conjecture seem to rely on the binary nature of the problem. 
This leads to our starting point for this part of the theis, which is the following:
\begin{question}{\em 
    Are learning and compression equivalent definitions also for regression problems?}
\end{question}

\subsection{Our Contribution}
Our first contribution was to extend \citeauthor{moran2016sample}'s fundamental result, relating compression and learning to the case of real-valued function classes. We begin with an algorithmically efficient version
of the learner-to-compression scheme conversion in
\citet{moran2016sample}:

\begin{theorem}[Efficient compression for classification, informal]
  \label{thm:classification}
  Let $\class$ be a concept class
  over some instance space $\domain$
  with VC-dimension $\vc{\class}$, dual VC-dimension $\dualvc{\class}$,
  and suppose that $\calA$ is a (proper, consistent) PAC-learner for $\class$.
There is
a randomized sample compression scheme for $\class$ of size
$O(k\log k)$, where $k=O(\vc{\class}\dualvc{\class})$.
Furthermore,
on a sample of any size $m$,
the compression set may be computed in
expected
time $\bigO{m\log m}$.
\footnote{For clarity, the linear dependency between the runtime of our algorithm and algorithm $\calA$ has been omitted.}
\end{theorem}
For comparison, 
a naive implementation of the
\citet{moran2016sample}
existence proof
yields a runtime of order
$
m^{c\vc{\class}}
+
m^{c\dualvc{\class}}$
(for some universal constants $c,c'$),
which can be doubly exponential when $\dualvc{\class}=2^\vc{\class}$;
this is
without taking into account the cost of
computing the minimax distribution on the $m^{c\vc{\class}}\times m$
game matrix.

Next, we extend the result in
Theorem~\ref{thm:classification}
from classification to regression:
\begin{theorem}[Efficient compression for regression, informal]
\label{thm:regression}
Let $\F\subset[0,1]^\domain$
be a function class
with
$t$-fat-shattering dimension $\fat{t}{\F}$,
dual $t$-fat-shattering dimension $\dualfat{t}{\F}$,
and suppose that $\calA$ is an ERM (i.e., proper, almost consistent) learner for $\F$.
There is
a randomized
uniformly $\eps$-approximate sample compression scheme for $\F$ of size 
$\bigO{k\tilde{m} \log(k \tilde{m}) }$, 
where
$\tilde m = \mathcal{O}\big( \fat{c\eps}{\F} \log(1/\eps) \big)$
and
\small{$k = \mathcal{O}\big(\dualfat{c\eps}{\F}\log(\dualfat{c\eps}{\F}/\eps)\big)$}.
Furthermore,
on a sample of any size $m$,
the compression set may be computed in
expected
time $\bigO{m \log(m) + k}$.
\footnote{As in Theorem\ref{thm:classification}, the linear dependency between the runtime of our algorithm and algorithm $\calA$ has been omitted for clarity.}
\end{theorem}  

A key component in the above result is our construction
of a generic $(\dev,\adv)$-weak learner.
\begin{definition}
  \label{def:weak}
  For $\dev \in [0,1]$ and $\adv \in [0,1/2]$,
  we say that $f:\domain\to\R$ is  
  an \emph{$(\dev,\adv)$-weak hypothesis}
  (with respect to distribution $D$ and target $f^*\in\F$)
if $$\Pr_{X\sim D}(|f(X)-f^*(X)|>\dev)\le \frac12-\adv.$$
\end{definition}

\begin{theorem}[Generic weak learner]
  \label{thm:gen-weak-learn}
  Let $\F\subset[0,1]^\domain$ be a
  function class
  with $t$-fat-shattering dimension $\fat{t}{\F}$.
For some universal numerical constants $c_{1},c_{2},c_{3} \in (0,\infty)$, 
for any $\dev,\delta \in (0,1)$ and $\adv \in (0,1/4)$,
any $f^*\in\F$,
and any distribution $D$,
letting $X_{1},\ldots,X_{m}$ be drawn iid from $D$, where 
\begin{equation*}
  m = \left\lceil c_{1} \left(  \fat{c_{2} \dev }{\F} \ln\!\left( \frac{c_{3}}{\dev} \right) + \ln\!\left(\frac{1}{\delta}\right) \right) \right\rceil,
\end{equation*}
with probability at least $1-\delta$, every $f \in \F$ with
$
\max_{i\in[m]}
|f(X_{i}) - f^{*}(X_{i})| \leq \alpha \dev$
for $\alpha \in [0,1)$,
is an $(\dev,\adv)$-weak hypothesis
with respect to $D$ and $f^*$.
\end{theorem}

As one can see, our results allow us to use any hypothesis $f \in \F$ with 
$\max_{i \in [m]} |f(X_{i}) - f^{*}(X_{i})|$ bounded below $\dev$: for instance, 
bounded by $\dev / 2$.

This result sheds new light on an open
question of \citet{DBLP:journals/siamcomp/Simon97}. 
Moreover, the ideas used in our construction proved fruitful in new settings, robust learning \citet{pmlr-v99-montasser19a}.
In order to demonstrate the efficacy of the above results, we show applications to two regression problems:
learning Lipschitz and bounded-variation functions.


Another direction of learning theory and its characteristics which has emerged in the last two decades is trustworthy machine learning. This includes various aspects and implications of using the ideas and tools of learning theory, such as the robust-learning mentioned above (see \citet{DBLP:conf/alt/AttiasKM19, https://doi.org/10.48550/arxiv.1706.06083, https://doi.org/10.48550/arxiv.1412.6572}), fairness (see \citet{dwork2012fairness, pmlr-v80-kearns18a}) and, most notably, privacy.

In light of the above, compression schemes evolved into a crucial  concept on which classical vanilla learning theory diverges in an essential way from the area of learning under privacy concerns.
The work in this chapter is joint with Steve Hanneke and Aryeh Kontorovich (ALT \citeyear{pmlr-v98-hanneke19a}).
\section{Privacy}

%
%

As progressively more technology products become based upon machine learning tools, and more branches of science turn to a more "evidence based" methodology, the use of data becomes increasingly dominant. A lot of these data sets consist of personal information, such as medical records, customer preferences, music listening history or user behavior within sites. Such personal data is being collected by more companies in order to gain insight and improve products, or to conduct scientific studies.

The importance of preserving the privacy of such data is common knowledge. It has induced laws restricting the information that can be gathered by companies, and regulations regarding who may use it in research, and in what manner. Privacy is important when treating personal data, but crucial when a leak could cause actual harm or embarrassment to an individual.
The straightforward definition of data leakage is unauthorized exposure of the data itself. This type of concern is at the core of cryptography and security research. 
A different type of data leakage, which is less intuitive, comes from releasing information about the data. Partial information about data, summary statistics or drawn conclusions are often regarded as safe for public release. 
To understand why this might reveal sensitive information, consider the case of language models.
Huge language models have become essential in the recent remarkable progress in the field of natural language processing. It is known that models of this size do "memorize" part of their training data \citet{liu2020early, arpit2017closer}. 
Although this memorized data seems to be concealed within the model, which often serves as a black box, it was shown that clever inference attacks can recover properties of the training data, such as the membership of sentences or recovery of strings contained in the data \citep{DBLP:journals/corr/ShokriSS16, DBLP:journals/corr/abs-2012-07805}. 
This might be a serious problem if attackers could use those models to infer private textual information such as social numbers, medical status, and other personal data.

To ensure that the result of such statistical and computational analysis will preserve privacy even against unknown future attacks, a mathematical definition and guarantee of privacy is desirable.
In this thesis, we will focus on such a privacy notion called \emph{Differential Privacy} \citet{DMNS06}.

Consider a data set $S$ consisting of $n$ rows, each row representing the information about a specific individual.
A (randomized) data analysis mechanism $\mechanism$ will be regarded as privacy-preserving if its output distribution will not be significantly affected by any individual row. This intuitively guarantees that whatever can be learned about an individual by the mechanism output can be learned if the personal data is arbitrarily modified.

\begin{definition}[\citet{DMNS06,dwork2008differential,10.1007/11761679_29}]
\label{def:dp}
A randomized algorithm $\mechanism$ is $(\eps,\delta)$-{\em differentially private} if for every two databases $S,S'$ that differ on one row (such databases are called {\em neighboring}), and every set of outcomes $\outcomes$, we have
$\Pr[\mechanism(S)\in \outcomes]\leq e^{\eps}\cdot \Pr[\mechanism(S')\in \outcomes]+\delta.$ 
The definition is referred to as {\em pure} differential privacy when $\delta=0$, and {\em approximate} differential privacy when $\delta>0$.
\end{definition}
Note that differential privacy is a property of the mechanism and not of its outcome. 

%
%
Differential privacy began with the revolutionary work of  Dinur, Dwork, Nissim,  McSherry and Smith \citep{dwork2006calibrating, 10.1145/773153.773173}, 
%
%
who presented for the first time a mathematical formulation of algorithmic information privacy. 
Their model is based on the assumption that we trust the data curator or the server administrator who collects and holds the data, but not the rest of the public and not even the researchers or analysts who make professional use of the database. Still, we wish to provide meaningful statistics and algorithmic tools based on the gathered data.
The main idea, to prevent the violation of the privacy of the information in the database, was to introduce additional randomness into the analytic  process, usually by injecting limited noise into the calculations. This addition would mask the influence of any individual in the database on the results. 
Although this noise often affects accuracy, the hope is that since it is bounded, this defect in accuracy will vanish as the sample size increases. 
Several studies in recent years have shown that for many basic tasks, this is indeed the case. On the other hand, other tasks turned out to be problematic in the sense that there is a gap, sometimes unbridgeable, between the classical level of accuracy and that which can be achieved under private requirements.

Following this line, over the last decade we have witnessed an explosion of research in differential privacy, by now also being largely employed by major corporates such as DeepMind \citet{jax-privacy2022github}, Alphabet \citet{DBLP:journals/corr/ErlingssonKP14} and IBM \citet{diffprivlib}, and it was even embedded in the query system of the 2020 United States census \citet{DBLP:journals/corr/abs-2107-10659}.
% 
%
for privacy-preserving data analysis. In particular, there has been a lot of interest in designing \emph{private learning algorithms}, which guarantee differential privacy for their training data. Intuitively, this guarantees that the outcome of the learner (the identified hypothesis) leaks very little information in any particular point from the training set. Works in this area include  \citep{KLNRS08,BBKN12,BNS13,BNS13b,BNS15,BNSV15,FeldmanX15,BunNS19,BeimelMNS19,KaplanMMS19,KaplanLMNS20,AlonBMS20,KaplanMST20,BunLM20,AlonLMM19}, and much more.
At the same time, the boundaries and limitations of private learning were studied thoroughly.
The main question is to characterize the \emph{sample complexity} of private learning; more specifically, the cost of requiring learners to preserve privacy, i.e. by how much the sample complexity increases as a result of this requirement.

%
%

Several works demonstrated that, under pure differential privacy, some learning problems which non-privately can be learned  with constant sample complexity require $\bigO{\log(|\domain|)}$, when $\domain$ is the domain from which data points are sampled.

More generally, \citeauthor{BunLM20} showed that the complexity of private learning grows at most double exponentially with respect to the Littlestone, dimension and, specifically, the ability to learn privately is equivalent to the class having a finite Littlestone dimension. This characterization widens the gap, as the Littlestone dimension can grow in an unbounded manner relative to the VC dimension of the same class. In particular, there are known classes whose VC dimension is finite and even constant, but Littlestone dimension is infinite.

One of the most fundamental learning problems in which such a gap emerges is the class of thresholds or, more generally, the class of axis aligned rectangles.

\subsection{Learning axis aligned rectangles}
\label{sec:intro-aar}

In this thesis, we revisit this fundamental open question of the  sample complexity of learning axis-aligned rectangles with privacy.
Non-privately, learning axis-aligned rectangles is one of the most simple and basic learning tasks which can be solved using compression ideas. As it is easily and intuitively compressible, it is often given as {\em the first} example for PAC learning in courses or books.
Nevertheless, under privacy constraints the problem is not only impossible for infinite domains but also for finite domains much more work is needed in order to solve it. 
More formally,
recall that the VC dimension of the class of all axis-aligned rectangles over $\R^d$ is $O(d)$, and hence a sample of size $O(d)$ suffices to learn axis-aligned rectangles non-privately (we omit throughout the introduction the dependency of the sample complexity in the accuracy, confidence, and privacy parameters). In contrast, it turns out that, with differential privacy, learning axis-aligned rectangles over $\R^d$ is impossible, even when $d=1$ \citep{FeldmanX15,BNSV15,AlonLMM19}. In more detail, let $\domain=\{1,2,\dots\}$ be a finite (one dimensional) grid, and consider the task of learning axis-aligned rectangles over the finite $d$-dimensional grid $\domain^d\subseteq\R^d$. In other words, consider the task of learning axis-aligned rectangles under the promise that the underlying distribution is supported on (a subset of) the finite grid $\domain^d$. 

For pure private learning, \citet{FeldmanX15} showed a lower bound of $\Omega\left(d\cdot\log|\domain|\right)$ on the sample complexity of this task. 
This lower bound is tight, as a pure-private learner with sample complexity $\Theta\left(d\cdot\log|\domain|\right)$ can be obtained using the generic upper bound of \citet{KLNRS08}.
This should be contrasted with the non-private sample complexity, which is independent of $|\domain|$.

For approximate-private learning, \citet{BNS13b} showed that the dependency of the sample complexity in $|\domain|$ can be significantly reduced. This, however, came at the cost of increasing the dependency in the dimension $d$. Specifically, the private learner of \citet{BNS13b} has sample complexity $\tilde{O}\left( d^3 \cdot 8^{\log^*|\domain|} \right)$.
We mention that a dependency on $\log^*|\domain|$ is known to be necessary \citep{BNSV15,AlonLMM19}. 
Recently, \citet{BeimelMNS19} and \citet{KaplanMST20} studied the related problem of privately learning {\em halfspaces} over a finite grid $\domain^d$, and presented algorithms with sample complexity $\tilde{O}\left( d^{2.5} \cdot 8^{\log^*|\domain|} \right)$. Their algorithms can be used to privately learn axis-aligned rectangles over $\domain^d$ with sample complexity $\tilde{O}\left( d^{1.5} \cdot 8^{\log^*|\domain|} \right)$. This can be further improved using the recent results of \citet{KaplanLMNS20}, and obtain a differentially private algorithm for learning axis-aligned rectangles over $\domain^d$ with sample complexity $\tilde{O}\left( d^{1.5} \cdot \left( \log^*|\domain|\right)^{1.5} \right)$. We consider this bound to be the baseline for our work, and we will elaborate on it later on Section~\ref{sec:litrature}.

To summarize, our current understanding of the task of privately learning axis-aligned rectangles over $\domain^d$ gives us two kinds of upper bounds in the sample complexity: Either $d\cdot\log|\domain|$ \;or\; $d^{1.5} \cdot \left( \log^*|\domain|\right)^{1.5}$. That is, current algorithms either require sample complexity that scales with $\log|\domain|$, or else it scales super linearly in the dimension $d$. Our starting point for this part of our work was to find a differentially private algorithm for learning axis-aligned rectangles with sample complexity that scales linearly in $d$ and is asymptotically smaller than $\log|\domain|$.

This naturally leads to the following question.

\begin{question}
\label{qs:axis-aligned}
{\em Is there a differentially private algorithm for learning axis-aligned rectangles with sample complexity that scales linearly in $d$ and asymptotically smaller than $\log|\domain|$?}
\end{question}

%
%

%
%

\subsubsection{Our Contribution}

We answer question~\ref{qs:axis-aligned} in the affirmative, and present the following theorem.

\begin{theorem}[informal]
\label{thm:informal}
There exists a differentially private algorithm for learning axis-aligned rectangles over $\domain^d$ with sample complexity $\tilde{O}\left( d\cdot \left( \log^*|\domain| \right)^{1.5} \right)$.
\end{theorem}

We do so by presenting a novel private algorithm for the problem 
which achieves this sample complexity.

In order to attain this improvement, 
a new algorithmic technique had to be developed.
We elaborate on this in the main part of the thesis, we now present an intuitive simplified version of the technique.
The main idea includes the deletion of "exposed" data-points on the go, in a manner designed to avoid the cost of the adaptive composition theorems (see Chapter~\ref{sec:litrature}), as each iteration can't affect other iterations, and by that avoid the super-linear growth in complexity suffered by former solutions.
At each iteration, one axis is examined and a set of "candidate edge points" chosen,
from which one point is picked using an interior-point solver.
The core of this technique may be of individual interest, introducing a new method for constructing statistically-efficient private algorithms.

Nevertheless, this idea alone, removing "exposed"data points on the go, 
is not enough.
The failure point is that by deleting {\em one} point from the data, we can create a "domino effect" that affects (one by one) many of the candidate sets throughout execution. 
Recall that differential privacy requires analysis of runtime upon neighboring datasets (see Definition~\ref{def:dp} above).
Consider two neighboring datasets $S$ and $S'=S\cup\{(x',y')\}$ for some labeled point $(x',y')\in \domain^d\times\{0,1\}$. Suppose that during the execution on $S'$, $x'$ gets picked as a "candidate point", thus the additional point $x'$ participates "only" in the first iteration of the algorithm, and is afterwards deleted. 
However, since the size of the candidate sets is fixed, during the execution on $S$ (without the point $x'$) it holds that {\em a different point $z$} gets included instead of $x'$, and this point $z$ is then deleted from $S$ (but it is not deleted from $S'$ during the execution on $S'$). 
Therefore, also during the second iteration, we have that $S$ and $S'$ are not identical (they still differ on one point) and this domino effect can continue throughout the execution. 
That is, a single data point can affect many of the executions of the algorithm, and we would still need to pay in composition to argue privacy.

Our solution is based on two modifications to this approach.
First, we add noise to the size of the candidate sets, but we only use the $n$ "inner" points from these sets. 
Second, we delete elements from $S$ not based on them being inside those sets, but only based on the (privately computed) interval stretching from the first chosen edge point to the second (at a given axis). 
This allows us to indeed separate the privacy analysis to each axis in its own, without any influence between iteration, hence avoiding the need for applying composition theorems and achieving an improved sample complexity.
The work in this chapter is joint with Uri Stemmer (NeurIPS \citeyear{NEURIPS2021_ee0e9524}).

\subsection{Universal Private Learning}
Continuing this line of research, we understand that private learning is inherently harder than classical. This is true theoretically, as just explained, but also practically, as designing and implementing differential private algorithms proved to be challenging. Despite tremendous efforts, in a lot of cases the state-of-the-art is still far from satisfactory. 
For example, the recent deployment of differential privacy by the US Census only guarantees a privacy parameter of $\eps= 19.61$ \citep{bureau_2021}, 
 %
which translates to a relatively weak privacy guarantee.\footnote{Observe that smaller privacy parameters $\eps,\delta$ translate to stronger privacy guarantees in Definition~\ref{def:dp}, in the sense that a single input point would have a smaller effect on the outcome distribution. On the flip side, reducing the privacy parameters is typically obtained by adding more noise and uncertainty to the computation, which often translates to a loss in accuracy.}
One possible explanation for these challenges is that most of the works on DP learning are inspired and explained by {worst-case} mathematical models such as the theory of PAC Learning \citep{valiant1984theory}, which is based on a {\em distribution-free perspective}. While it gives rise to a clean and compelling mathematical picture, one may argue that the PAC
model fails to capture at a fundamental level the true behavior of many practical learning problems (regardless of privacy consideration). A key criticism of the PAC model is that the distribution-independent definition of learnability is too pessimistic: real-world data is rarely worst-case, and experiments show that practical learning rates can be much faster than predicted by PAC theory \citep{CohnT90,CohnT92}. It therefore appears that the worst-case nature of the PAC model hides key features that are observed in practice. Furthermore, these shortcomings seem to be amplified in the context of {\em private} PAC learning.
Those challenges, which are strengthened by theoretical results (e.g. the sample complexity gap mentioned in Section~\ref{sec:intro-aar})  
%
%
seem to reflect the worst-case distribution-free nature of the PAC model rather than fundamental limitations of DP learning. 
This thesis, 
%
%
therefore, advocates the study of distribution-dependent private-learning, as this can lead to a more optimistic (and realistic) landscape of differentially private learning. We investigate a distribution dependent model known as \emph{universal learning} and ask the following fundamental question:
\begin{question}{\em
For which problems or classes there exists a learning rule, such that for every distribution, the rule's learning rate converges with the best risk in class as the number of examples tends to infinity.}
\end{question}
%
%

As before, we might need to avoid natural universal learners which are compression based, such as K-nearest neighbors \citet{devroye2013probabilistic} and OptiNet \citep{DBLP:journals/corr/abs-1906-09855}, as their behavior is inherently influenced by the sample at hand and thus might be too sensitive to preserve privacy.
%
%

\subsubsection{Our Contribution}

We uncover the following general result:

\begin{theorem}\label{thm:mainIntro}
For every $d\in\N$ and every $\eps\leq1$ there is an $(\eps,0)$-differentially private universal consistent (UC) learner over $\R^d$.
\end{theorem}

Recall that, as we mentioned, learning one-dimensional linear classifiers over $\R$ with differential privacy is impossible in the PAC model. Theorem~\ref{thm:mainIntro} circumvents this impossibility result: not only are one-dimensional linear classifiers learnable in the UC model, but in fact {\em every} class (over $\R^d$) is learnable in this setting, and furthermore, there is a single (universal consistent) algorithm that learns them all (w.r.t.\ any distribution).

To obtain Theorem~\ref{thm:mainIntro} we design a simple variant for the classical {\em histogram rule} \citep{Glick73,GordonO78,GordonO80,devroye2013probabilistic} that partitions $\R^d$ into cubes of the same size (where the size decreases with the sample size $n$), and makes a decision according to the majority vote within each cube. This algorithm is particularly suitable for differential privacy, and can be made private simply by adding noise to the votes within each cube. In the analysis, we show that this does not break the universal consistency of the histogram rule.
Furthermore, We generalize those results to any separable metric spaces with bounded doubling dimension.

\begin{remark}
For simplicity, in Theorem~\ref{thm:mainIntro} we fixed $\eps$ to be a constant (independent of the sample size $n$). Our algorithm trivially extends to a setting where $\eps$ decreases with the sample size.
\end{remark}

We extend our results to the more general setting of {\em density estimation} in the UC model (with respect to the total variation metric). That is, we seek a differentially private algorithm satisfying the following definition.

\begin{definition}[Universal consistent density estimation, informal \citep{devroye1985nonparametric}]\label{def:introUCinformal}
Let $\domain$ be a domain and let $\alg$ be an algorithm whose output is a density function over $\domain$. Algorithm $\alg$ is a {\em universal consistent (UC) density estimator} over $\domain$ if for every $\alpha,\beta$ and for every distribution $\measure$ over $\domain$ there is a constant $n=n(\alpha,\beta,\measure)$ such that
$
\Pr_{\substack{S\sim\measure^n\\f\leftarrow\alg(S)}}[\tv{f}{\measure}
>\alpha]<\beta.
$
\end{definition}

Unlike our private UC learner, which satisfies differential privacy with $\delta=0$ (this is sometimes referred to a {\em pure} differential privacy), our private UC density estimator only satisfies differential privacy with $\delta>0$. When using $\delta>0$, it is commonly agreed that the definition of differential privacy only provides meaningful guarantees as long as $\delta\ll1/n$. Therefore, unlike with our private UC learner, we must let $\delta$ decay with the sample size $n$. Our result is the following.

\begin{theorem}\label{thm:introDE}
Let $d\in\N$, let $\eps\leq1$, and let $\delta:\N\rightarrow[0,1]$ be a function satisfying $\delta(n)=\omega(2^{-\sqrt{n}})$. There is an $(\eps,\delta(n))$-differentially private universal consistent (UC) density estimator over $\R^d$.
\end{theorem}

This work is an important first step towards understanding differentially private universal learning. 

The work in this chapter is joint with Olivier Bousquet, Haim Kaplan, Aryeh Kontorovich, Yishay Mansour, Shay Moran and Uri Stemmer \citeyear{https://doi.org/10.48550/arxiv.2212.04216}.


Finally, we turn our attention to the close subject of \emph{adaptive data analysis}. 


\section{Adaptive data analysis}

Statistical validity is a well known crucial aspect of modern science.
In the past several years, the natural and social science communities
have come to realize that such validity was not in fact preserved
in numerous peer-reviewed and widely cited studies, leading to many false discoveries. Known as the {\em replication crisis}, 
this phenomenon threatens to undermine the very basis for the public's trust in science.

One of the main explanations for the prevalence of false discovery arises from the inherent {\em adaptivity} in the process of data
analysis. To illustrate this issue, consider a data analyst interested
in testing a specific research hypothesis. The analyst acquires relevant data, evaluates
the hypothesis, and (say) learns that it is false. Based on the findings, the analyst now
decides on a second hypothesis to be tested, and evaluates it on the {\em same data} (acquiring
fresh data might be too expensive or even impossible). That is, the analyst chooses the
hypotheses {\em adaptively}, where this choice depends on previous interactions with the data.
As a result, the findings are no longer supported by classical statistical theory, which
assumes that the tested hypotheses are fixed before the data is gathered, and the analyst runs the risk of overfitting to the data.

In order to tackle this setting, we first make it explicit. We give here the formulation presented by \citet{dwork2015preserving}. We consider a two-player game between a mechanism $\mechanism$ and an adversary $\adversary$, defined as follows (see Section~\ref{sec:perlim-ada} for precise definitions).
\begin{enumerate}
    \item The adversary $\adversary$ fixes a measure $\measure$ over $\domain^n$ (satisfying some conditions).
    \item The mechanism $\mechanism$ obtains a sample $S\sim\measure$ containing $n$ (possibly correlated) observations.
    \item For $k$ rounds $j=1,2,\dots,k$:
    \begin{itemize}
        \item The adversary chooses a {\em query} $h_j:\domain\rightarrow\{0,1\}$, possibly as a function of all previous answers given by the mechanism.
        \item The mechanism obtains $h_j$ and responds with an answer $z_j\in\R$, which is given to $\adversary$.
    \end{itemize}
\end{enumerate}

We say that $\mechanism$ is {\em $(\alpha,\beta)$-empirically-accurate} if with probability at least $1-\beta$ for every $j$ it holds that $|z_j-h_j(S)|\leq\alpha$, where $h_j(S)=\frac{1}{n}\sum_{x\in S}h_j(x)$ is the empirical average of $h_j$ on the sample $S$. We say that $\mechanism$ is {\em $(\alpha,\beta)$-statistically-accurate} if with probability at least $1-\beta$ for every $j$ it holds that $|z_j-h_j(\mu)|\leq\alpha$, where $h_j(\mu)=
_{T\sim \mu}\left[h_j(T)\right]=\E_{T\sim \mu}\left[\frac{1}{n}\sum_{x\in T}h_j(x)\right]$ is the "true" value of the query $h_j$ on the underlying distribution $\mu$. Our goal is to design mechanisms $\mechanism$ providing statistical-accuracy.

Starting from \cite{dwork2015reusable,dwork2015generalization}, it has been demonstrated that  
various
notions of {\em algorithmic stability},
and in particular 
{\em differential privacy},
allow for methods which maintain statistical validity under the adaptive setting.
The vast majority of the works in this area, however,  strongly rely on the assumption that the data is sampled in an i.i.d.\ fashion.
This scenario excludes some natural and essential problems in learning theory
such as Markov chains, active learning, and autoregressive models
\citep{kontram06, jap/1421763330,kontorovich2017concentration,settles2009active,hanneke2014theory,10.1162/00335530151144131}.

A notable exception is a stability notion introduced by \citet{DBLP:journals/corr/BassilyF16}, called {\em typical-stability}.
This beautiful and natural notion has the advantage that, under some conditions on the underlying distribution, it can guarantee statistical validity even for non-i.i.d.\ settings. However, one downside of the results of \citet{DBLP:journals/corr/BassilyF16} is that they do not recover the i.i.d.\ generalization bounds in the limiting regime where the dependencies decay to zero. In particular, in the i.i.d.\ setting, it is possible to efficiently answer $\tilde{O}(n^2)$ adaptive queries given a sample of size $n$. In contrast, the results of \citet{DBLP:journals/corr/BassilyF16} only allow to answer $\tilde{O}(n)$ adaptive queries, {\em even if the dependencies in the data decay to zero}. 
Motivated by this gap and the above results, we ask the following question:

\begin{question}
Can the tools and results from the  adaptive data analysis literature be extended to the correlated examples setting, 
giving a meaningful bound while also recovering the i.i.d.\ generalization bounds in the limiting regime where the dependencies decay to zero?
\end{question}

%
%

\subsection{Our Contribution}

Our first contribution to this line of work is to extend existing generalization results for differential privacy from the i.i.d.\ setting to the correlated setting.
To that end, we introduce
a notion we call \emph{Gibbs dependence} to quantify the  dependencies between 
the covariates of a given joint
distribution. 
We complement this result with a tight negative example. 
Our second contribution is 
to extend the connection between transcript-compression and adaptive data analysis also to the non-iid setting.
Finally, we demonstrate an application of our results for when the underlying measure can be described as a Markov chain.

\paragraph{Gibbs Dependence}
\label{subsec:into-gd-dp}

We extend the connection between differential privacy and generalization to the case where the observations are correlated. We quantify the correlations in the data using a new notion, called \emph{Gibbs dependence}, which is closely related to the classical 
\emph{Dobrushin} interdependence coefficient \citep{kontorovich2017concentration,levin2017markov}.
Intuitively, a measure which has $\mixing$-Gibbs dependency
is such that knowledge about almost the entire sample
does not provide too much information about the remaining portion.
Formally, 

\begin{definition}%
  For a probability measure $\measure$
  over a product space $\X^n$,
  define
    \[\mixing(\measure)
     = 
     \sup_{x\in\X^n}
     \E_{i\sim[n]}
     \tv{\measure_i(\cdot)}{\measure_i(\cdot\mid x^{-i})},
     \]    
  where $\measure_i(\cdot)$ is the $i^{th}$ marginal measure 
  and $\measure_i(\cdot\mid x^{-i})$ is the $i${th} marginal measure
  conditioned on all the coordinates other than $i$
  (given some $n$-tuple $x$).

  Given $\mixing$
  we say the $\measure$ has $\mixing$-Gibbs dependence
  if $\mixing(\measure) \leq \mixing$. 
\end{definition}

A naive way for leveraging our notion of Gibbs dependence would be to "union bound" the correlations across the $n$ different coordinates. Specifically, one could show that if $\measure$ has Gibbs-dependence $\psi$ then $\tv{\measure}{\measure^{*}}\leq n\psi$, where $\measure^*$ is the product distribution in which every coordinate is sampled independently of the corresponding marginal distribution in $\measure$. 
Thus, if $\psi\ll\frac{1}{n}$ then one could argue about generalization w.r.t.\ $\measure$ by applying existing generalization bounds w.r.t.\ $\measure^*$ in the independent case (since in this regime we have $\tv{\measure}{\measure^{*}}\ll1$). This argument, however, only works when the dependencies in $\measure$ are {\em very weak} (i.e., when $\psi\ll\frac{1}{n}$). 
We contribute to this line of work by showing that differential privacy still provides generalization even if $\psi$ is much larger, e.g., a constant independent of the sample size $n$. 
Specifically, 

\begin{theorem}
  \label{thm:main-result-dp}
  Let $\mechanism$ be an $(\varepsilon,\delta)$-differentially-private mechanism
  which is $(\alpha,\beta)$-empirically-accurate for $k$ rounds given $n$ samples. 
  If $n\geq \frac{\log(2k\varepsilon/\delta)}{\varepsilon^2}$, then $\mechanism$ is also
  $(\alpha+10\varepsilon+2\mixing,\beta + \frac{\delta}{\varepsilon})$-statistically-accurate.
\end{theorem}

\begin{remark}
For the case when $\mixing$ is zero, and hence $\measure$ is a product measure (see Example~\ref{apn:prod} below), Theorem~\ref{thm:main-result-dp} recovers the results achieved by differential privacy for i.i.d.\ samples \cite{dwork2015preserving,bassily2016algorithmic}. Thus, Theorem~\ref{thm:main-result-dp} generalizes the connection between differential privacy and generalization to the correlated setting.
\end{remark}

Intuitively, the above theorem states that if the underlying distribution has Gibbs-dependence $\mixing$ then the additional generalization error incurred by DP algorithms (compared to the iid setting) is at most $O(\mixing)$. We complement this result with a tight negative example, showing that there exists a distribution $\measure$ with Gibbs-dependence $\mixing$ and a DP algorithm $\alg$ that obtains generalization error $\Omega(\psi)$. This means that, in terms of the Gibbs-dependence, our result is tight.

By applying Theorem~\ref{thm:main-result-dp} with a known DP mechanism for answering queries while providing empirical accuracy, we get the following corollary.

\begin{corollary}\label{cor:main}
There is a computationally efficient mechanism $\mechanism$ that is $(\alpha+2\psi,\beta)$-statistically-accurate for $k$ adaptively chosen queries given a sample (an $n$-tuple) from an underlying measure with Gibbs-dependency $\psi$ satisfying 
$
n\geq \tilde{O}\left( \frac{\sqrt{k}}{\alpha^2}\log\frac{1}{\beta} \right).
$
\end{corollary}

This generalizes the state-of-the-art bounds for the i.i.d.\ setting, where $\psi=0$. In particular, Corollary~\ref{cor:main} shows that mild dependencies in the data, say $\psi=\alpha$, come {\em for free} in terms of the achievable bounds for adaptive data analysis. We emphasize that $\psi=\alpha$ captures non-negligible dependencies. In particular, $\alpha$ could be constant, independent of the sample size $n$.

\paragraph{Transcript Compression}

The second direction we examine is that of \emph{transcript compression}.

Compression has been used in the context of adaptive data analysis. \citet{dwork2015generalization} used the definition of \emph{bounded description length} (referred to here as \emph{transcript compression}) to present an algorithm that is able to adaptively answer queries when the data is i.i.d.\ sampled.
This notion of compression differs from the one mentioned above; it is more involved but, as shown by \citet{dwork2015generalization} and also by our results, it is suited for tasks requiring low sensitivity tools.

Our contribution here is in generalizing this idea by showing that the same definition, when used in the right setting, allows maintaining adaptive accuracy even when the distribution includes dependencies.

Following the approach of \citet{DBLP:journals/corr/BassilyF16}, we aim to provide the following guarantee: 
As long as the analyst chooses functions which, in the non-adaptive setting, are concentrated around their expected value, then the answers given by the mechanism should be accurate. Intuitively, the idea is that functions with large variance are hard to approximate even in the non-adaptive setting, and hence, we should not require our mechanism to approximate them well in the adaptive setting.

This is formalized as follows. For every query $q$ and every distribution 
$\measure$, we write $\gamma(q,\measure,\delta)$ to denote the length of a confidence interval around the expectation of $q$ with confidence level $(1-\delta)$. That is, $\gamma(q,\measure,\delta)$ is such that when sampling $T\sim\measure$, with probability at least $(1-\delta)$ it holds that $q(T)$ is within $\gamma(q,\measure,\delta)$ from its expectation. We obtain the following theorem (see Section~\ref{thm:compression-accuracy} for a precise statement).

\begin{theorem}[informal]\label{thm:comp_informal}
Fix $\alpha,\delta>0$. 
There exists a computationally efficient mechanism with the following properties. The mechanism obtains a sample (an $n$-tuple) from some unknown underlying distribution $\measure$. Then, for $k$ rounds $i=1,2,\dots,k$, the mechanism obtains a query $q_i$ and responds with an answer $a_i$ such that 
$$
\Pr[\exists i \text{ s.t.\ } |a_i-q_i(\measure)|>\alpha+\gamma(q_i,\measure,\delta)]\leq \delta\cdot k \cdot 2^{k\cdot\log\frac{1}{\alpha}}.
$$
\end{theorem}

In particular, as long as the adversary poses queries $q_i$ such that $\gamma(q_i,\measure,\delta)\leq\alpha$, the mechanism from Theorem~\ref{thm:comp_informal} guarantees that all of its answers are $2\alpha$-accurate, with probability at least $1-\delta\cdot k \cdot 2^{k\cdot\log\frac{1}{\alpha}}$. In order for such a statement to be  meaningful, we want to assert that $\delta\ll\frac{1}{k} \cdot 2^{-k\cdot\log\frac{1}{\alpha}}$. This is easily obtained in many settings of interest by taking the sample size $n$ to be big enough. For example, for sub-Gaussian or sub-exponential queries, we would get that $\delta$ vanishes exponentially with $n$, and hence, for large enough $n$ we would get that $\delta\ll\frac{1}{k} \cdot 2^{-k\cdot\log\frac{1}{\alpha}}$.

We note that in the case of {\em non-adaptive} data analysis, learning from non-i.i.d samples is a well-known problem that has been heavily studied in various directions. 
This includes works on the Markovian criteria  \cite{marton_measure_1996,kontorovich2017concentration,DBLP:conf/alt/WolferK19,juang1991hidden}, 
as well as other criteria such as those researched by \cite{DBLP:conf/stoc/DaskalakisDP19,DBLP:conf/colt/DaganDDJ19}. 
These lines of work do not transfer, at least not in a way that we are aware of, to the adaptive setting.

\citet{DBLP:journals/corr/BassilyF16} also studied the problem of adaptive data analysis with correlated observations; we now elaborate on the differences. 
\begin{enumerate}

\item {\bf Results regarding transcript compression (Section~\ref{sec:compression})}
As we mentioned,  \citet{DBLP:journals/corr/BassilyF16} introduced the beautiful framework where the mechanism is required to provide accurate answers only as long as the analyst poses "concentrated queries". They obtained their results for this setting via a new notion they introduced, called {\em typical stability}. However, their analysis and definitions are quite complex. We show that essentially the same bounds can be obtained {\em in a significantly simpler way, using standard compression tools}. Specifically, our result (Theorem~\ref{thm:comp_informal}) recovers essentially the same bounds for all types of queries considered by  \citet{DBLP:journals/corr/BassilyF16}, including bounded-sensitivity queries, subgaussian queries, and subexponential queries. In addition to being significantly simpler, our result offers the following advantage: Using the results of \citet{DBLP:journals/corr/BassilyF16}, we need to know {\em in advance} the parameter controlling the "concentration level"  of the queries that will be presented in runtime, and this parameter is used by their algorithm. In contrast, our algorithm is oblivious to this parameter, and the guarantee is that our accuracy depends on the "concentration level" of the given queries. Furthermore, with our algorithm, different queries throughout the execution can have different "concentration levels", a feature which is not directly supported by \citet{DBLP:journals/corr/BassilyF16}.


    \item 
    {\bf Results regarding Gibbs-dependence (Section~\ref{sec:learning-with-low}).}
    These results are in a different setting than that of Bassily and Freund, and the results are not directly comparable. In particular, Bassily and Freund do not study any specific dependence notion, such as Gibbs dependence. Our results show that assuming low Gibbs dependence allows for improved bounds. Specifically, \citet{DBLP:journals/corr/BassilyF16} can answer at most $\tildeO{n}$ adaptive queries efficiently, even if the dependencies within the sample are very weak. Using our notion of Gibbs-dependency, we can answer $\tildeO{n^2}$ adaptive queries efficiently, while accommodating small (but non-negligible) dependencies.
\end{enumerate}

%

The work in this chapter is joint with Aryeh Kontorovich and Uri Stemmer (ICML \citeyear{pmlr-v162-kontorovich22a}).

\chapter{Literature Review}
\label{sec:litrature}

\section{Compression Schemes}
\paragraph{Background.}
It appears that
generalization bounds
based on sample compression
were
independently discovered by
\citet{1056018} and \citet{littlestone1986relating} 
(when the former dealt with nearest-neighbor rules and the latter with generic learning algorithms),
%
%
and further elaborated upon by \citet{DBLP:journals/ml/GraepelHS05};
see \citet{DBLP:journals/ml/FloydW95} for background and discussion.
A more general kind of Occam learning was discussed in \citet{blumer1989learnability}.
Computational lower bounds on sample compression were
obtained in \citet{gottlieb2014near},
and some communication-based
lower bounds
were given in
\citet{DBLP:journals/corr/abs-1711-05893}.

\paragraph{Compression and Boosting.}
\label{par:comp-and-boost}
The idea of constructing compression schemes using the boosting technique is known in the literature. The first  mention of this connection was made by \citet{FreundSchapire97}. In their seminal work they proved that boosting is possible by answering an open question, suggested by \citet{kearns94crypto}. Starting with \citet{10.5555/92571.92640} and later on with the construction of the famous \algname{AdaBoost} algorithm by \citet{FreundSchapire97}, the boosting mechanism relied on an intermediate construction, providing  a compression scheme of size $\bigO{\vc{\class} \log n}$ for binary function classes $\class$ with VC-dimension $\vc{\class}$.
Continuing this line of work,
\citet{moran2016sample} discuss the idea of leveraging this connection between boosting and compression, 
and recognize that their main result can in fact be seen as a refinement of this connection.

\paragraph{Real-Valued Functions.}
\begin{sloppypar}
Beginning with \citet{FreundSchapire97}'s \algname{AdaBoost.R}
algorithm, there have been numerous attempts to extend
AdaBoost to the real-valued case
(\citet*{bertoni1997boosting,Drucker:1997:IRU:645526.657132,avnimelech1999boosting};\ \citet*{Karakoulas00,DuffyHelmbold02,kegl2003robust,NOCK200725})
along with various theoretical and heuristic
constructions of particular weak regressors
\citep{Mason1999,MR1873328,DBLP:journals/ml/MannorM02};
see also the survey \citet{Mendes-Moreira2012}.
\end{sloppypar}

An explanation for the challenge
of defining a good weak-learner was pointed and explained by 
\citet[Remark 2.1]{DuffyHelmbold02}
we discuss this issue on \ref{sec:notion-weak-learning}.
The $(\dev,\adv)$-weak learner,
which has appeared, among other works, in
\citet{DBLP:journals/cpc/AnthonyBIS96,DBLP:journals/siamcomp/Simon97,avnimelech1999boosting,kegl2003robust},
gets around this difficulty,
but provable general constructions of such learners have been lacking.
Likewise, the heart of our sample compression engine, \algname{MedBoost},
has been widely in use since \citet{FreundSchapire97} in various guises.
Our Theorem~\ref{thm:gen-weak-learn}
supplies the remaining piece of the puzzle:
{\em any}
sample-consistent regressor applied to some random sample of bounded size
yields
an $(\dev,\adv)$-weak hypothesis.
The closest analogue we were able to find was
\citet[Theorem 3]{DBLP:journals/cpc/AnthonyBIS96},
which is non-trivial only for function classes with finite pseudo-dimension,
and is inapplicable, e.g., to classes of $1$-Lipschitz or bounded variation functions
(see \ref{sec:examples}).

The literature on general sample compression schemes for real-valued functions is quite sparse. 
There are
well-known
narrowly tailored
results on specifying functions or approximate versions of functions 
using a finite number of points, such as the
classical
fact that a polynomial of degree $p$ can be perfectly 
recovered from $p+1$ points.
To our knowledge, the only \emph{general} results on sample compression for real-valued functions
(applicable to \emph{all} learnable function classes) is Theorem 4.3 of \citet*{david2016supervised}.
They propose a general technique to convert any learning algorithm into a compression scheme.  However, their notion of compression scheme is significantly weaker 
than ours: namely, they only guarantee a bound on the average error
rather than  than our \emph{uniform} approximation requirement.
In particular, in the special case of a family of \emph{binary}-valued functions, their notion of 
sample compression does \emph{not} recover the usual notion of sample compression schemes for classification, 
whereas our uniform $\eps$-approximate compression notion \emph{does} recover it as a special case.  We therefore 
consider our notion to be a more
fitting
generalization of the definition of sample compression 
to the real-valued case.

\citet{ashtiani2020near} adopted the notion of a compression scheme
to the distribution learning problem.
They showed
that if a class of distributions admits
robust compressibility
then
it is agnostically learnable.
They used those results in order to provide state-of-the-art
sample-complexity bounds for learning a mixture of Gaussians.

\section{Privacy}
\subsection{Background}
The formal connection between differential privacy and learning theory was made by \citet{KLNRS08}, who proposed the Privately Probably Approximately Correct learning model (PPAC for short). 
One of the fundamental results In the field of machine learning is that the sample complexity of PAC learning is proportional to the Vapnik-Chervonenkis (VC) dimension of the concept class being learned. 
\citeauthor{KLNRS08} demonstrated that for any finite concept class, there exists a privacy-preserving learner with sample complexity that is logarithmic in the size of the class. 
However, \citet{beimel2010bounds} showed that while a non-private learner can properly learn the concept class of point functions (which consists of functions that evaluate to $1$ for a single element and $0$ anywhere else) with a sample complexity of $O(1)$, a pure differential private learner requires a sample complexity of $\Omega(\log(|\domain|))$ for proper learning. 
Later on, \citet{FeldmanX15} demonstrated that this separation holds even for improper learning. It was shown by \citet{BNS13b} that this gap can be made significantly smaller by relaxing the privacy requirement to approximate privacy (i.e. $\delta >0$); nevertheless, there is still a crucial gap for some classes.

Recently, it was proven by \cite{AlonLMM19, BunLM20, DBLP:journals/corr/abs-2106-13513} that private learning and online learning are equivalent. As online learning can be characterized by the Littlestone dimension \citep{littlestone1988learning, ben2009agnostic} thus implying that private learning is possible if and only if for classes with finite \emph{Littlestone dimension}. Specifically, it was shown that the sample complexity of privately learning a class $\class$ is at most $poly(\Ldim{\class})$ 
and at least $\log^*(\Ldim{\class})$, 
when $\Ldim{\class}$ represents the Littlestone dimension of the class. 
Within this large quantity gap, the exact dependency of sample complexity on the Littlestone dimension is unknown.
Exploring the relationship between sample complexity and measures such as Littlestone dimension, VC dimension, and potentially other measures is an essential open question in the field.
%
%

\subsection{Alternative models}
In recent years several variants and modifications have been proposed for the initial definition of differential privacy. 
%
%

\paragraph{Rényi differential privacy (\cite{DBLP:journals/corr/Mironov17}).} 
One main variant of differential privacy is the \emph{Rényi differential privacy} (RDP for short). This definition utilizes the notion of \emph{Rényi divergence}, a measure of the difference between two probability distributions. 

\begin{definition}[Rényi divergence]
    Given two discrete distributions 
    $\mathcal{P} = \{p_1,\ldots,p_n\}$, $\mathcal{Q} = \{q_1,\ldots,q_n\}$
    The $\alpha$-Rényi divergence of $\mathcal{P}$ and $\mathcal{Q}$ is defined as
    \[
        \mathcal{D}_{\alpha}(\mathcal{P}, \mathcal{Q}) := 
        \frac{1}{1-\alpha}\log\left(\sum_{i=1}^{n}\frac{p^\alpha_i}{q^{\alpha-1}_i}\right)
    \]
    When for $\alpha \in \{0,1,\infty\}$ is defined by taking a limit \footnote{The definition can be extended to continuous random variables but for the sake of simplicity we present it on its discrete form.}.
\end{definition}

Recall that pure epsilon-differential privacy can be stated as a bound on the privacy loss function, i.e. 
$\log\frac{\Pr(\mechanism(S)\in \outcomes)}{\mechanism(S')\in \outcomes} \leq \eps$
for any two neighboring data sets $S,S'$. 
In a similar manner, an algorithm will be said to preserve $(\alpha,\eps)$-Rényi differential privacy, requiring instead that the Rényi divergence between $\mechanism(S)$ and $\mechanism(S')$ be bounded by $\eps$, 
$
\mathcal{D}_{\alpha}(\mechanism(S), \mechanism(S')) \leq \eps.
$
It can be shown that if $\alpha$ is taken to infinity we get 
$
        \mathcal{D}_{\infty}(\mathcal{P}, \mathcal{Q}) = 
        \log \sup_{i\in [n]}  \frac{p_i}{q_i}
$ \citep{DBLP:journals/corr/abs-1206-2459}; 
hence, $(\infty, \eps)$-Rényi differential privacy is equivalent to $\eps$-differential privacy.  %

It was shown by \cite{DBLP:journals/corr/Mironov17} that any $(\alpha, \eps)$-Rényi differential private algorithm is also $(\eps', \delta)$-differentially private for 
$\eps' = \eps + \frac{\log(1/\delta)}{1-\alpha}$, 
hence, Rényi differential privacy can be seen as an intermediate notion between pure and approximate differential privacy. 
Moreover, its composition properties are easy to calculate, making it practically appealing. 
In addition to this definition, additional variants to the basic definition of differential privacy have been defined in recent years. 
Among them, we can mention concentrated differential privacy (zCDP) \citep{DBLP:journals/corr/BunS16}, Gaussian differential privacy (f-DP) \citep{DBLP:journals/corr/abs-1905-02383} and fuzzy differential privacy \citep{9730083}.

%
%

\paragraph{Local differential privacy (\citet{KLNRS08}).}
Recall that differential privacy is based on the assumption that a trustworthy curator with access to all private information is responsible for collecting the data. \emph{Local differential privacy} (Local-DP or LDP for short) is a variant of differential privacy that removes this assumption, requiring that the process of data collection itself preserves privacy. 
Historically, The first well-known privacy-preserving statistical mechanism, the “randomized response”, proposed by \citet{warner1965randomized} and \citet{greenberg1969unrelated}, was in fact locally private. 
In recent years, since its first formulation by \citet{10.1145/773153.773174} and \citet{KLNRS08}, the local model has gained great popularity in the world of research. 
Furthermore, it has found many uses in industry and in practical applications (see \citet{pmlr-v119-acharya20a, bureau_2021, erlingsson2014rappor, murakami2019utility}). 
The model allows companies to guarantee their users that the personal information collected while using the various services cannot be exposed even in the event of malicious use of their own servers. 
The main caveat of the local model is that it is a stricter notion of privacy and thus can result in significantly reduced utility.


\paragraph{The shuffle model of differential privacy.}
This problem is addressed by another model, known as the \emph{shuffle model} \citet{DBLP:journals/corr/abs-1808-01394, DBLP:journals/corr/abs-1710-00901, DBLP:journals/corr/abs-1811-12469}. 
% 
%
In this model, each user adds some auxiliary randomness to their information before sending it to an intermediate server. This randomness is combined with the actual data, and the intermediate server shuffles all of the inputs together before transmitting them to the main server. By including random data from each user, the shuffle model helps to obscure any individual user's contribution to the final result.
It was shown by \citet{DBLP:journals/corr/abs-1808-01394} that for some problems this model can achieve significantly better accuracy than the local model (see \citet{DBLP:journals/corr/abs-2107-11839} for a survey of results on such separations). 
The main disadvantage of the model, compared to the above local model, is that users have to trust the auxiliary server, both in terms of its reliability and in terms of the correctness of its implementation. 
Nevertheless, the shuffle model is of main interest in the privacy community (see \citet{Balle2020PrivateSI, Feldman2020HidingAT, Balle2019ThePB}).

\subsection{Basic Differentially Private Tools}

%
%
The inclusion of a privacy requirement necessitates the adaptation or reconstruction of statistical analyses and learning mechanisms, as they can no longer be utilized in their current form. 
As a result, in the past several years there was a tremendous effort of the research community to provide the fundamental building blocks of private statistics and private learning. 

\paragraph{The Laplace mechanism \citet{dwork2006calibrating}.}
In their seminal paper, \citeauthor{dwork2006calibrating} proposed the Laplace mechanism, which provides a way of answering queries as long as their sensitivity to changes in input is not high.  
One useful basic example which can be solved using the Laplace mechanism is answering counting queries, which simply asks for the number of points in the data which satisfies a certain property. 
It can be easily seen that this type of query has low sensitivity, any single change in the data will not change the count by more than 1, implying the need for a relatively small amount of noise added by the Laplace mechanism.
%
%
Although very basic, counting queries are important primitives which can be composed into more complex algorithms, hence the ability to perform them in a privacy-preserving manner is highly important. 
Differential privacy can be composed by running several such primitives, resulting in a complex private mechanism which privacy parameters can be computed using the composition theorem \citep{dwork2006calibrating,5670947}. 
Another, commonly used, set of functions that also enjoys low sensitivity is the histogram query. This defines a partitioning of the space into bins and then asks how many points in the data fall inside a specific bin. 
As in the case of counting queries, histogram queries enjoy low sensitivity to changes in the input data, and therefore each query can be answered with a small amount of noise added. In the case of large or even infinite spaces, the number of bins can be too large to answer all. In such a case, the notion of approximate privacy can be utilized using the Stability-based histogram algorithm \citet{JMLR:v20:18-549}, which essentially ignores almost the entire space and takes into account only the part of the support which has high probability mass.

\paragraph{The Exponential Mechanism \citet{mcsherry2007mechanism}.}
Later on, \citeauthor{mcsherry2007mechanism} proposed the Exponential Mechanism, a generic technique which able to tackle a wide range of tasks. The exponential mechanism, although it is often inefficient, as its run-time depends on the size of the space, can be used in an even broader set of tasks. 
A significant example is the one of private median estimation. The median, as opposed to the average, can be highly sensitive and hence can not be computed privately using the Laplace mechanism, yet it can be computed using the exponential mechanism. 

\paragraph{The Sparse Vector technique \citep{dwork2009complexity}.}
Another primitive technique is the sparse vector technique by \citet{dwork2009complexity}. Its main purpose is to answer only queries whose value is above some given threshold. The sparse vector technique allows the algorithm to “pay”, in privacy, only for the queries which do pass the threshold and not for the entire stream of queries, which may be much larger. 
This technique lies at the core of several important works, including the private multiplicative weights mechanism by \citet{hardt2010multiplicative}. 

\paragraph{Empirical Risk Minimization}
Another highly important task is the one of \emph{empirical risk minimization} (ERM) which lies at the core of learning theory. The main setting involves an instance space and data set and a metric of the closeness of a prediction to the true label value called the loss function (see Section~\ref{sec:background} for precise setting and definitions). 
The problem is to find a hypothesis or a vector, in the case of linear prediction, in which cumulative loss on the data set is minimal. 

For bounded loss functions which are also Lipschitz, this task can be done using the exponential mechanism \citep{DBLP:journals/corr/BassilyST14}. In order to solve the task in a broader setting and with better parameters, \citet{6736861} introduced the \emph{Differentially-Private Stochastic Gradient Descent} (DP-SGD), a private variant of the celebrated gradient descent algorithm used widely for almost every deep-learning framework. 
The DP-SGD mechanism has been thoroughly studied, and various modifications suggested through the years, in order to improve the utility-privacy trade-off in general or for specific settings \citep{Abadi_2016, Wang2019DPLSSGDAS, Jayaraman2019EvaluatingDP, Kairouz2021PracticalAP, Liu2021LowerBF}.
 
\subsection{Current research directions}
In recent years, research and development in the field of privacy have experienced significant growth, both in theoretical and practical aspects. On the theoretical side, several significant results can be mentioned: 

\paragraph{Characterization of Private Learning.}
First, as mentioned above, the line of research attempting to characterize what can be learned in a way that preserves privacy still includes some crucial gaps. 
Despite the recent breakthroughs that have shown that private learning is equivalent to online learning, i.e., is possible if and only if the Littlestone dimension of the class is finite, there is still a gap when it comes to the exact quantification of the connections \citep{alon2022private, DBLP:journals/corr/abs-2012-03893}.

\paragraph{Private Convex Optimization.}
As a continuation of the results concerning empirical risk minimization, which often relies on the DP-SGD algorithm, a fairly significant part of the research deals with the subject of private convex optimization. 
This area of research deals with the different ways to find optimal solutions (maximum or minimum) of convex functions. These problems, which are at the heart of many fields even beyond the ERM problem, are of great importance to the research world. Many natural scenarios can be formulated in this framework, such as the support vector machine algorithm (SVM) \citep{shalev-shwartz_understanding_2014}, network-flow optimization \citep{wei2017optimal}, image denoising \citep{thanh2019review} and much more. 
Therefore, it is important to find solutions that also meet the demand for privacy. To name a few key results from the last few years, we can point to \citet{Kifer2012PrivateCE, Iyengar2019TowardsPD, Bassily2021NonEuclideanDP, Bassily2019PrivateSC, Feldman2020PrivateSC, Kulkarni2021PrivateNE, Wu2016DifferentiallyPS}.

\paragraph{2020 United States census.}
On the practical side, the latest highlight was the transition of the \emph{US Central Bureau} to the use of differential privacy in order to preserve the privacy of those surveyed in the 2020 census. The census, which takes place once a decade, is required by law to prevent harm to the privacy of those who participate \citep{bureau_2021c}. 
In previous surveys various privacy heuristics were used, but after a long discussion it was decided to change the methodology and use differential privacy. 
The project, perhaps the largest routine statistical project in the United States, required the Central Bureau of Statistics to integrate various tools from the literature within an analysis platform that would be accessible to the general public of researchers, most of whom had no prior knowledge of differential privacy \citep{bureau_2021b}. 
The great applied challenge constitutes a significant milestone on the way to the assimilation of the paradigm as an essential tool in additional concrete statistical analyses.

\paragraph{Deep Learning.}
As part of the applied research in the field of privacy, similar to the situation in the machine learning community, the focus of interest is in deep learning. 
As we mentioned above, at the center of the research is the DP-SGD algorithm and attempts to improve it so that it enables increasingly better performance, under the same privacy constraints. 
The fundamental problem of image classification is considered a central test case. Prior work on differentially-private deep learning by \citet{Abadi_2016} demonstrated its use on the standard image classification benchmark data sets - MNIST and CIFAR-10. On the MNIST data set, which is considered an easy task, the authors  achieved $0.9$, $0.95$, and $0.97$ test set accuracy for $(0.5, 10^{-5})$, $(2, 10^{-5})$ and $(8, 10^{-5})$-differential privacy, respectively. 
On the more challenging CIFAR-10, the authors  achieved accuracy of $0.67$, $0.7$, and $0.73$,  for $(2, 10^{-5})$, $(4, 10^{-5})$, and $(8, 10^{-5})$-differential privacy, respectively. 
This is a significant gap from the non-private state-of-the-art which is currently $0.9991$ for MNIST and  $0.995$ for CIFAR-10 \citep{https://doi.org/10.48550/arxiv.2008.10400, DBLP:journals/corr/abs-2010-11929}. 
These results had repeatedly been improved up until the recent work by \citet{jax-privacy2022github}, which introduced a differentially private model based on a 40-layer Wide-ResNet neural network, achieving $0.814$ accuracy under $(8, 10^{-5})$-differential privacy for CIFAR-10. 
Some more recent works propose different techniques in order to even further improve those results, using pre-training on non-private data, hyperparameter tuning, and so on (see \citet{DBLP:journals/corr/abs-2201-12328}).

\paragraph{Attacks.}
Parallel to this direction, many studies deal with presenting the problems of using algorithms that do not preserve privacy. 
The research in this direction revolves around the idea of reconstruction attacks and membership attacks. 
These attacks recover parts of the information from the models and the information that is traditionally released to the network openly, together with the non-private models. Certain attacks succeed in identifying, with high probability, whether information about a certain individual appeared in the database that was used \citep{shokri2017membership, hu2022membership, DBLP:journals/corr/abs-2112-03570, DBLP:journals/corr/abs-2007-14321}. 
Other attacks perform the reconstruction of images that appeared in the database on which models were trained to classify images \citep{DBLP:journals/corr/abs-2201-04845} and some attacks reconstruct words that appeared in the database on which language models were trained \citep{DBLP:journals/corr/abs-2012-07805}.
Recently, studies have also been done that leverage the idea of reconstruction attacks to try and quantify the relationship between privacy parameters and the likelihood or extent of information leakage from algorithms that preserve differential privacy \citep{hannun2021measuring, guo2022bounding}.

\subsection{Open questions on the private learning domain}

Research on differential privacy continues to advance with significant momentum in recent years. At the same time, many questions are still open and serious challenges are still facing the field. 
Fundamental questions are at the center of attention and require in-depth research. Problems exist, such as quantitative characterization of private learning, learning in models other than the classic PAC model, and understanding the meaning and connections between the various models, to name a few central issues.

In the applied direction, there is still a long way to go. A series of open-source libraries for implementing privacy-preserving algorithms are gaining momentum, but they are not yet complete. There is a requirement to understand the appropriate parameters for the various research and statistical needs. In addition, there is a fundamental need to find ways to improve the performance of privacy-preserving deep learning algorithms, so that they can be used in the various fields at the center of technological practice today, such as advanced classification models, large language models, and generative models.

\section{Universal learning}

%


Despite the dominance of the PAC model since its definition in the 1980s, over the years many criticisms have been heard \citep{buntine1990theory, sarrett1989average, 10.1162/neco.1992.4.2.249, NIPS1990_816b112c}. The main argument was that the model is too pessimistic, and that in reality the results are often fundamentally different from those obtained in the theory that derives from this definition. 

For these reasons, \citeauthor{DBLP:conf/stoc/BousquetHMHY21} proposed a relaxed model which they called \emph{Universal Learning}. In this paper, the authors point out that in very natural cases the rate of convergence of different learning algorithms is several orders of magnitude faster than that which results from a pessimistic calculation under the definition of the PAC model. 
Hence, the main idea of the universal learning model is to allow the studied learning rate to depend not only on the class at hand but also on the distribution in the background.
This idea also corresponds to the practical conduct in which the distribution does not change throughout the learning process but is fixed in the background, and only the sample size increases as needed.

This concept of learning rates that depend on the distribution (in contrast to the distribution-free nature of the PAC model) is not new. The classical notion of \emph{bayes-consistency} defines a close model. The main difference is that Bayes consistency sets the class of functions to be learned to be the class of all measurable functions, as opposed to the universal learning model which allows for a more specific choice of classes.

The notion of Bayes consistency is one of the initial definitions of learning theory and can be found in the classical works of \citet{fisher1922mathematical, stone1977consistent, 1053964} and \citet{fix1989discriminatory}.
After the introduction of the PAC model, the consistency definition was pushed aside and its study became secondary and less common. For this reason, the current literature dealing with this subject is quite sparse. 

As for the study of private learning in a distribution-dependent context, prior work in this vein focused on obtaining better utility guarantees under the assumption that the underlying distribution adheres to certain "niceness" assumptions; for example margin assumptions. That is, these works do not aim to learn under {\em any} underlying distribution as we do, only under "nice" distributions. For example, private learning under margin assumptions was considered by \cite{BlumDMN05,ChaudhuriHS14,BunCS20,NguyenUZ20}, and private clustering under data stability assumptions was previously considered by \citet{NissimRS07,NIPS2015_051e4e12,HuangL18};\citet{ShechnerSS20,CohenKMST21,FriendlyCore}. Another related work is by \cite{HaghtalabRS20} who studied smooth analysis in the context of private learning, where the input points are perturbed slightly by nature. This is equivalent to assuming that the underlying distribution is not overly concentrated on any single point, which is similar in spirit to margin assumptions.

Parallel to our research and independently, this definition was investigated in a series of articles by \citet{gyorfi2023multivariate, gyorfi2022rate} and \citet{berrett2019classification}. The results in these papers are very similar to ours, but with several differences. In terms of privacy, while we have focused on analyzing the curator model of differential privacy, \citet{gyorfi2023multivariate, gyorfi2022rate} and \citet{berrett2019classification} have primarily focused on local privacy. Also, they did a minimax analysis of the convergence rates for a specific family of density functions, while our focus was to demonstrate the possibility of private learning under the universal definition.
In addition, they have proven results for regression problems, which is not the case in our research. Furthermore, while we used approximate privacy for density estimation, they focused on pure privacy only.
On the other hand, for the classification case, we have shown results for general, metric (doubling) unbounded spaces, a property from which the construction and results of \citet{gyorfi2023multivariate, gyorfi2022rate} and \citet{berrett2019classification} do not benefit. 
Overall, our results provide a complementary perspective to their findings. Together, they provide a comprehensive perspective on the privacy analysis of machine learning algorithms, using this universal learning model to highlight the importance of considering different privacy metrics, problem types, and analysis techniques.

\section{Adaptive data analysis}
%
%
A classical technique used for multiple hypothesis testing is the Dunn-Bonferroni correction \citep{shaffer1995multiple,dunn1961multiple}, 
whose usage is limited due to the fact that it significantly reduces the number of discoveries. 
A more robust type suite of techniques is the false-discovery-rate (FDR) control method, such as the Benjamini-Hochberg procedure \citep{benjamini1995controlling} and the Bayesian approach by \citet{storey2003positive}. 
Those methods focus mainly on statistical inference approaches such as hypothesis testing, confidence intervals etc., and are still a subject of active research (see also \cite{li2017accumulation,javanmard2015online}). 

The different generic approach, which we also take here, uses the notion of algorithmic stability.
Algorithmic stability is known to be intimately connected (and, in some settings, equivalent) to
learnability \citep{bousquet2002stability,shalev2010learnability}. Most of the existing stability notions, however, are not sufficient for our goal of adaptive learnability. For example, {\em uniform stability}, which has recently been the subject of several interesting results, is not closed under post-processing and does not yield the same type of adaptive generalization bounds as we study in this paper \citep{BousquetE02,Shalev-ShwartzSSS10,pmlr-v48-hardt16,FeldmanV18,FeldmanV19}. 
A notable exception is {\em local statistical stability}, which was shown to be both necessary and sufficient for adaptive generalization \citep{ShenfeldL19}. However, so far, local statistical stability has not yielded new algorithmic insights.


A different line of research employs information-theoretic techniques,
whereby overfitting is prevented by bounding the amount of mutual information
between the input sample and the output hypothesis.
However, these techniques generally only guarantee generalization in expectation, rather than high probability bounds \citep{pmlr-v51-russo16,NIPS2017_6846,RogersRST16,RaginskyRTWX16,russo2019much,SteinkeZ20}.

The formulation of the adaptive data analysis we consider was introduced by \citet{dwork2015reusable} (in the context of i.i.d.\ sampling), and has since then been the subject of many interesting papers \citep{bassily2016algorithmic,bun2018fingerprinting,hardt2014preventing,NIPS2018_7876,ShenfeldL19,JungLN0SS20,abs-2106-10761}. The connection between differential privacy and adaptive generalization also originated from \citet{dwork2015reusable}. Interestingly, this connection has recently been repurposed for different settings, such as adversarial streaming and dynamic algorithms \citep{HassidimKMMS20,abs-2107-14527,KaplanMNS21,abs-2111-03980}, and also in a preprint by the author, Uri Stemmer and Moshe Shechner, named \emph{Streaming with advice} which is currently under review.
We note that while this work by the author is relevant to the broader topic of privacy and compression, it was not included in this thesis due to its focus on a different aspect of the subject. Nonetheless, the findings have potential for further exploration and development and may be a valuable contribution to the field.


\chapter{Background and Preliminaries}
\label{sec:background}
This chapter details some preliminaries needed for a proper presentation of the results and discussion in the main part of the thesis. Some more specific preliminaries will be presented on the dedicated chapter in which they are relevant.
An {\em instance space} is an abstract set $\X$ 
and a classifier is a binary function mapping points from the space to either zero or one $f: \domain \to \{0,1\}$.

\section{Learning}

\subsection{The PAC Model and the VC Dimension}
We use standard definitions from statistical learning theory. 
See, e.g.,\ \cite{shalev-shwartz_understanding_2014}.

The main goal of a classifier is to correctly predict the label of future points. 
Nevertheless, its performance in the current given sample serves as a starting point and a measure which might indicate its true performance.
\begin{definition}[Sample error]
The empirical error of a classifier $h$ 
w.r.t.\ a labeled-sample $S \in (\domain\times\{0,1\})^n$ is defined as  
$
\errS{h}{S} = \frac{1}{n}\sum_{(x,y)\in S} \mathbbm{1}[h(x) \neq y].
$
\end{definition}

\begin{definition}[True error]
Given a data distribution $\measure$ and a hypothesis $h$ we denote $\err_\measure(h) = 
\E_{(x,y)\sim \measure}[1[h(x)\neq y]].$ 
Given a data distribution $\measure$, an algorithm $\mathcal{A}$, and sample size $n$, define
\begin{align*}
\err_\measure(\mathcal{A},n) &= 
\E_{S \sim \measure^{n}}
\E_{h_S\leftarrow \mathcal{A}(S)}
[\err_\measure(h_S)] 
=
\E_{S \sim \measure^{n}}
\E_{h_S\leftarrow \mathcal{A}(S)}
\E_{(x,y)\sim \measure}[1[h_S(x)\neq y]].  
\end{align*}
In words, $\err_\measure(\mathcal{A},n)$ is the expected loss of $\mathcal{A}$ given $n$ labeled examples from $\measure$.
\end{definition}

The main definition at the core of statistical learning literature is PAC learning. The definition aims to capture, in a quantifiable manner, what it essentially means \emph{to learn} from the algorithmic perspective.
\begin{definition}[PAC learnability \cite{valiant1984theory}]
Let $\alpha,\beta\in[0,1]$ and let $\complexity\in\N$.
An algorithm $\alg$ is an $(\alpha,\beta,\complexity)$-PAC-learning algorithm for a class $\class$ if for every distribution $\measure$ over $\domain\times\{0,1\}$ s.t.\ $\exists h^*\in\class$ with $\errD{h^*}=0$, 
it holds that 
$
\Pr_{S\sim\measure^m}[\errD{\alg(S)} > \alpha] < \beta.
$ 
We refer to $\complexity$ as the \emph{the sample complexity} of $\alg$.
\end{definition}

In simple terms, a PAC learner is an algorithm that takes a labeled dataset as input and produces a classifier as output, which is guaranteed to predict the label of new instances with high probability.
Since the seminal work by Vapnik and Chervonenkis, 
one main idea in the research of learnability of function classes is by trying to measure the complexity or the expressability of a function class.
This is usually done using the combinatory idea of \emph{shattering}.
\begin{definition}[Shattering]
Let $\class$ be a class of functions over a domain $\domain$. A set $S = (s_1,\ldots,s_k) \subseteq \domain$
is said to be \emph{shattered} by $\class$
if
$
\abs{ \{(f(s_1),\ldots,f(s_k)) : f\in \class\} } = 2^k.
$
\end{definition}

Using the idea of shattering, Vapnik and Chervonenkis defined their famous notion of dimension for function classes.
\begin{definition}[VC Dimension \cite{vapnik_uniform_1971}]
\label{def:vc-dimension}
The \emph{VC dimension} of a class $\class$,
denoted as $\vc{\class}$, 
is the cardinality of the largest set shattered by $\class$.
If $\class$ shatters sets of arbitrary large cardinality, then it is said that $\vc{\class}=\infty$.
\end{definition}

When the roles of $\X$ and $\class$ are exchanged,
i.e., an $x\in\X$ acts on $f\in\class$
via
$x(f)=f(x)$, 
we refer to $\X=\class^*$ as the {\em dual} class of $\class$.
Its VC-dimension is then
$\dualvc{\class} := \vc{\class^*}$, 
and referred to as the \emph{dual VC dimension}.
\citet{MR723955} showed that 
$\dualvc{\class} \leq 2^{\vc{\class}+1}$.

Intuitively, the VC-dimension of a class captures its complexity,
by measuring the number of points that can be separated by the class' functions.

The main result that emerged from the VC-theory is that, in classes with a finite VC-dimension, it is possible to bound, with high probability,  the gap between the empirical risk and the true risk of the classifier, also known as the generalization gap, meaning that such classes are PAC-learnable.


\begin{theorem}[VC Dimension Generalization Bound \cite{vapnik_uniform_1971,blumer1989learnability}]
\label{thm:vc}
Let $\class$ be a function-class 
and let $\measure$ be a probability measure over  $\domain\times\{0,1\}$. 
There exist a constant $c$ s.t.
for every 
$n \geq
\frac{c}{\alpha}\big(\vc{\class} \log(\frac{1}{\alpha}) + \log(\frac{1}{\beta})\big)$
every $\alpha, \beta > 0$, 
and every $f \in \class$ it holds that
$
  \Pr_{S\sim \measure^n}[\exists f\in\class:
  \errD{f} \geq \alpha \wedge \errS{f}{S} \leq \alpha/10] \leq \beta.
$
\end{theorem}

When dealing with real-valued functions, in contrast to the binary function of VC-theory, the notion found to be best suited as an analog is  \emph{fat-shattering}.
\begin{definition}[Fat-Shattering \citep{alon97scalesensitive}]
For $\F\subset\R^\X$ and $t>0$,
we say that $\F$ $t$-shatters
a set $\set{x_1,\ldots,x_k}\subset\X$
if
there is an $r\in\R^m$ such that for all $y\in\set{-1,1}^m$
there is an $f\in\F$ such that
$ \min_{i\in[k]} y_i(f(x_i)-r_i)\ge t$, 
when 
$[k]:=\set{1,\ldots,k}$.
\end{definition}
That is, fat-shattering looks for expressability of the functions in the class as threshold functions with a given margin or scale.
Again, this shattering notion is the basis for a dimension which characterizes learnability for real-valued function classes, just as the VC-dimension does for binary functions.
\begin{definition}[Fat-Shattering dimension \citep{alon97scalesensitive}]
The $t$-fat-shattering dimension 
$\fat{t}{\F}$
is the size of the largest $t$-shattered set (possibly $\infty$) . 
Again, the roles of $\X$ and $\F$ may be switched, 
in which case $\X=\F^*$ becomes the dual class of $\F$.
Its $t$-fat-shattering dimension is then 
$\dualfat{t}{\F}$.
\footnote{It was proven by \citet{DBLP:journals/corr/abs-2108-10037} that it is impossible to obtain general bounds on the dual-fat-shattering dimension, similar to the one proven by \citet{MR723955} for VC-dimension. 
Nevertheless, bounds do exist for some natural classes. To demonstrate this, in Section~\ref{sec:examples} we prove such bounds for the dual class of two fundamental classes: Lipschitz functions and bounded-variation functions.}
\end{definition}

Understanding the PAC model, the VC dimension and its extensions is essential as they provide theoretical frameworks for assessing the generalization performance of machine learning algorithms. These concepts serve as building blocks for the development of efficient learning algorithms with provable guarantees.

\subsection{Compression Schemes}

The formal notion of \emph{compression scheme}, which was defined by \citet{littlestone1986relating} is the following:
A {\em sample compression scheme} $(\kappa,\rho)$ for a hypothesis
class $\F\subset\Y^\X$
is defined as follows.
A $k$-{\em compression} function $\kappa$
maps sequences $\mathcal{S}=((x_1,y_1),\ldots,(x_m,y_m))\in\bigcup_{\ell\ge1}(\X\times\Y)^\ell$
to elements in
$\K=\bigcup_{\ell\le k'}(\X\times\Y)^\ell
\times
\bigcup_{\ell\le k"}\set{0,1}^\ell$,
where $\kappa(\mathcal{S})\subseteq \mathcal{S}$ and $k'+k"\le k$.
A {\em reconstruction} is a function $\rho:\K\to\Y^\X$.
We say that $(\kappa,\rho)$ is a $k$-size sample compression
scheme for $\F$
if
$\kappa$ is a $k$-compression and
for all
$h^*\in\F$
and all
$S=((x_1,h^{*}(x_1)),\ldots,(x_m,h^{*}(y_m)))$,
we have
$\hat h:=\rho(\kappa(S))$
satisfies $\hat h(x_i)=h^*(x_i)$ for all $i\in[m]$.

For real-valued functions,
there are several notions of compression-schemes.
We say it is a \emph{uniformly $\eps$-approximate} compression scheme if 
\[
\max_{1 \leq i \leq m} | \hat{h}(x_i) - h^*(x_i) | \leq \eps.
\]

We note that a somewhat similar definition was proposed by \cite{david2016supervised}: 
Let $S = (x_1,y_i),\ldots,(x_m,y_m)$ be a tagged sample drawn i.i.d from some unknown distribution,
an let $l : \X \times \R \to \R$ be some loss function.
We say that $(\kappa,\rho)$
is an \emph{agnostic sample compression scheme} 
for $\H$ if,
for every sample $S$,
$f_S := \rho(\kappa(S))$,
achieves $\F$-competitive empirical loss:
\[ 
\uf{m}\sum_{i=1}^{m} l(f_S (x_i), y_i) \leq 
\inf_{h\in\H} \uf{m}\sum_{i=1}^{m} l(f_S (x_i), y_i),
\] 

and we say that it is 
\emph{$\epsilon$-Approximate Agnostic Sample Compression Scheme} for $\H$
if for every sample $S$
\[ 
\uf{m}\sum_{i=1}^{m} l(f_S (x_i), y_i) \leq 
\inf_{h\in\H} \uf{m}\sum_{i=1}^{m} l(f_S (x_i), y_i) + \epsilon.
\] 

\subsection{Bayes optimal Classifier and Plug-in Estimators}

PAC learnability and VC-theory have been the main vanilla settings in the research community in the last decades. Nevertheless, there are other notions and regimes in the learning literature. 
The principal notion of learning, which predated the PAC-learning idea, is \emph{consistency}. The main difference between consistency and PAC is that, while PAC aims for distribution-free and uniform rates for a given class of functions, consistency looks for distribution dependent rates without looking at a specific types of functions.
This direction is most known in the context of analyzing classical learning algorithms such as the Nearest Neighbor rule, for which the arguments and tools from the PAC-model and VC-theory are irrelevant  \citep{devroye1985nonparametric, 1053964, DBLP:journals/corr/GottliebKK13}.

Given a probability measure $\measure$ over $\domain \times \{0,1\}$, we denote by $\reg$ the {\em regression function}, also known as the {\em posteriori probability function}, 
defined as
$\reg(x) = \Pr(y=1\mid x)$.
The Bayes-optimal classifier is, then,
\[
h^*(x) = \begin{cases}
  1  & \reg(x) > 1/2 \\
  0 & \text{otherwise}
\end{cases},
\]
and we denote its error probability by 
$L^* = \err_\measure(h^*)$.
It can be easily shown that $h^*$ achieves the lowest error-rate among all the possible classifiers. 

\begin{definition}
An algorithm $\alg$ is said to be \emph{universally consistent} if for any distribution $\measure$ it holds that
$
\lim_{n\to\infty}\left\{\err_\measure(\alg,n)\right\} = L^*.
$
\end{definition}

As $\reg$ is generally unknown, a possible approach for designing universally consistent algorithms is to create an approximation $\hat{\reg}$.

\begin{definition}
Let $\hat{\reg}:\domain\rightarrow[0,1]$ be any function. 
A \emph{plug-in classification rule} w.r.t.\ $\hat{\reg}$ is defined as 
$
\hat{h}(x) = \begin{cases}
  1  & \hat{\reg}(x) > 1/2 \\
  0 & \text{otherwise}
\end{cases}.
$
\end{definition}

The following theorem provides a bound on the error-rate of such a construction. 
\begin{theorem}[{\citet[Theorem 2.2]{devroye2013probabilistic}}]
Let $\hat{\reg}:\domain\rightarrow[0,1]$ be any function and let $\hat{h}$ be its corresponding plug-in classification rule. Then,
\label{thm:plugin}
  $\err_\measure(\hat{h}) - \err_\measure(h^*) 
  \leq 
  2 \E\left[\abs{\reg(x)-\hat{\reg}(x)}\right],$ 
  where the expectation is over sampling $x$ from the marginal distribution on unlabeled examples from $\measure$.
\end{theorem}

In \cite{devroye2013probabilistic}, the above theorem is stated only for $\R^d$. The extension to arbitrary spaces is immediate; the details are given in Section~\ref{sec:additional} for completeness.

\subsection{Density Estimation}

In the problem of density estimation, given a sample containing $n$ iid (unlabeled) elements from an (unknown) underlying distribution $\measure$, our goal is to output a distribution $\hat{\measure}$ that is close (in $L_1$ distance) to the underlying distribution $\measure$. 

\begin{definition}
An algorithm is said to be {\em universally consistent for density estimation in $L_1$ norm} if for any underlying distribution $\measure$ the following holds.
\begin{align*}
&\lim_{n\to\infty}
\E_{S\sim\measure^n}
\E_{\measure_n\leftarrow\alg(S)}
\| \measure - \measure_n\|_{1} 
=
\lim_{n\to\infty}
\E_{S\sim\measure^n}
\E_{\measure_n\leftarrow\alg(S)}
\int \abs{\measure(x) - \measure_n(x)}dx = 0.
\end{align*}
\end{definition}

Our main metric for similarity between probability measures will be the \emph{total variation distance}.

\begin{definition}[Total Variation Distance]
  Given two measures $\nu$ and $\mu$ on the same space $\Omega$,
  the {\em total variation distance}
  between them is defined as 
  $\tv{\nu}{\mu} := \sup_{A\subseteq \Omega} \abs{\nu(A) - \mu(A)},$
  where the supremum is over the Borel sets of $\Omega$. Equivalently, $\tv{\nu}{\mu} = \frac{1}{2}\sum_{a\in\Omega}\abs{\nu(a) - \mu(a)} = \frac{1}{2}\norm{\mu - \nu}_{\ell_1}.$
\end{definition}

\subsection{Adaptive Data Analysis}
\label{sec:perlim-ada}
The standard formulation of adaptive data analysis is defined as a game
involving some (adversary) analyst and a query-answering mechanism. In the interests of this part of the thesis, queries are {\em statistical queries}, meaning they are functions of the form $q:\X\to [0,1]$.
The goal of the mechanism is to make sure that the answers provided to the analyst are accurate w.r.t.\ the expected value of the corresponding queries over the underlying distribution. The idea is to formalize a utility notion that holds for \emph{any} strategy of the data analyst. As a way of dealing with \emph{worst-case analysts}, the analyst is assumed to be \emph{adversarial} in that it tries to cause the mechanism to fail. If a mechanism can maintain utility against any such \emph{adversarial} analyst, then it maintains utility against any analyst. This game is specified in Algorithm~\ref{alg:adapt-pop-game}.

\begin{algorithm}[tb]
   \caption{$\texttt{Game}(\mechanism,k,\adversary,S)$}
  \label{alg:adapt-pop-game}
\begin{algorithmic}
   \State {\bfseries Inputs:} Mechanism $\mechanism$, interaction length $k$, adversary $\adversary$, dataset $S$.
   \State The dataset $S$ is given to $\mechanism$.
   \For{$i\in [k]$}
   \State $\adversary$ picks a query $q_i$.
   \State The query $q_i$ is given to $\mechanism$.
   \State $\mechanism$ outputs an answer $a_i$.
   \State The answer $a_i$ is given to $\adversary$.
   \EndFor
\end{algorithmic}
\end{algorithm}


\begin{definition}[Adaptive Empirical Accuracy]
  A mechanism $\mechanism$ is {\em $(\alpha,\beta)$-empirically accurate} for $k$ rounds given a dataset of size $n$,
  if for every dataset $S$ of size $n$ and every adversary $\adversary$, it holds that 
  $
    \Pr_{\texttt{Game}(\mechanism,k,\adversary,S)}\left[
      \max_{i\in [k]} \abs{q_i(S) - a_i} > \alpha
    \right] \leq \beta,
  $ 
  where $q_i(S) := \uf{|S|}\sum_{x\in S}q_i(x)$.
\end{definition}


\begin{definition}[Adaptive Statistical Accuracy]\label{def:adaptiveaccuracy}
  A mechanism $\mechanism$ is {\em $(\alpha,\beta)$-statistically accurate} for $k$ rounds given $n$ samples with Gibbs-dependence $\psi$, 
  if for every distribution $\measure$ over $n$-tuples with Gibbs dependency $\mixing$, and every adversary $\adversary$, 
  it holds that 
  \[
    \Pr_{\substack{S\sim\measure\\\texttt{Game}(\mechanism,k,\adversary,S)}}\left[
      \max_{i\in [k]} \abs{q_i(\measure) - a_i} > \alpha
    \right] \leq \beta,
  \]
  where 
  $q_i(\measure) := \mean{T\sim\measure}[q_i(T)]=\mean{T\sim\measure}\left[\uf{|T|}\sum_{x\in T}q_i(x)\right]$.
\end{definition}

\begin{remark}
\label{rem:det-is-suff}
    The above definition is stated in general form, but in fact it is sufficient to show that a mechanism $\mechanism$ exhibits the above guarantee for every \emph{deterministic} adversary $\adversary$. The reason is that for a randomized adversary one can fix the adversary's random coins and use the \emph{total probability law} in order to get the same result. 
\end{remark}

\section{Differential privacy}

Differential privacy \citep{dwork2006calibrating} is a mathematical definition of privacy that aims to enable statistical analyses of datasets, while providing strong guarantees that individual-level information does not leak. Informally, an algorithm that analyzes data satisfies differential privacy if it is robust in the sense that its outcome distribution does not depend "too much" on any single data point. Formally,

\begin{definition}[Differential Privacy \citep{dwork2006calibrating}]
  A randomized algorithm $\alg:\X^n\to \Y$
  is $(\varepsilon,\delta)$-differentially private
  if for every two datasets $S,S'$ which differ on a single element and for any event $F$ we have 
  $\Pr[\alg(S)\in F]\leq e^{\varepsilon}\cdot \Pr[\alg(S')\in F]+\delta.$
\end{definition}

Two datasets $S, S'\in\domain$ are said to be neighboring if they differ exactly on one element,
formally,
$\distH{S}{S'} = 1$.

\begin{definition}[\citet{DMNS06}]
Let $f$ be a function mapping databases to real vectors. 
The {\em global sensitivity} of $f$ is defined as 
$\GS{f} = \max_{\distH{S}{S'}=1}\Norm{f(S)-f(S')}_1$.
\end{definition}

\subsection{Properties and basic tools of Differential privacy}

When dealing with the privacy of released information and statistics, we must account for future unknown attacks.
This implies that every reasonable protection guarantee must not break under post-process of any kind. 
Hence, an important property of differential privacy is the following:
\begin{theorem}[post-processing]
Let $\alg:\domain^n\to\Y$ be an $(\eps, \delta)$-differentially private algorithm, and let $f:\Y\to\Y'$ be any arbitrary mapping. Then $f\circ\alg:\domain^n\to\Y'$ is also $(\eps, \delta)$-differentially private.
\end{theorem}

We will later make use of several differentially private algorithms combined, all applied upon the same data, in parallel or even in an adaptive-sequential manner.
Even so, the combined process will remain privacy preserving (for different parameters) due to differential privacy been preserved under composition. Formally,
\begin{theorem}[Advanced Composition, \citep{5670947}]
Let $0<\delta', \eps\leq1$,
and let $0\leq\delta', \eps\leq1$.
An algorithm which permits $k$ adaptive interactions with (various) mechanisms which preserve $(\eps, \delta)$-differential privacy, 
is then 
$(\eps', k\delta+\delta')$-differentially private by itself, 
when
$\eps' = 
\eps\sqrt{sk\ln(1/\delta')} + 2k\eps^2.
$
\end{theorem}

We write $\Lap(\mu,b)$ to denote the \emph{Laplace distribution} 
with mean $\mu$ and scale $b$. When the mean is zero, we will simply write $\Lap(b)$. The Laplace distribution lies at the core of many algorithms from the differential private literature, and most notably as used in the generic Laplace mechanism.
\begin{definition}[The Laplace mechanism \citep{DMNS06}]
Let $f$ be a function mapping databases to vectors in $\R^k$, and let $\eps$ be a privacy parameter.
Given an input database $S$, the Laplace mechanism outputs 
$\mathbb{M}_\eps(f,S) = f(S) + (a_1,\ldots,a_k)$,
when $a_i$ are sampled i.i.d.\ from $\Lap(\GS{f}/\eps)$.
\end{definition}

\begin{theorem}[\citet{DMNS06}]
\label{thm:lap}
The Laplace mechanism is $\eps$-differentially private.
\end{theorem}

One of the most basic and generic tools in the literature on differential privacy is the exponential mechanism of \citet{mcsherry2007mechanism}, defined as follows. Consider a "quality function" $f$ that, given a dataset $S$, assigns every possible solution $a$ (coming from some predefined solution-set $A$) a real valued number, identified as the "score" of the solution $a$ w.r.t.\ the input dataset $S$. The goal is to privately identify a solution $a\in A$ with a high score $f(S,a)$. The mechanism itself simply picks a solution at random, where the probability for solution $a$ is proportional to $e^{\varepsilon f(S,a)}$. As shown by \citet{mcsherry2007mechanism} the exponential mechanism is $(\varepsilon,0)$-differentially private.

\subsection{Private Learning}


Combining the ideas from privacy and learning research yields the natural idea of private learning.
The vast majority of the research in recent years showing interested in this link 
revolves around the definition of Private-PAC learnability.
\begin{definition}[Private-PAC learnability]
An algorithm $\alg$ is an $(\alpha,\beta,\eps,\delta,\complexity)$-PPAC learner for a class $\class$ 
if: 
\begin{enumerate*}[label=(\roman*)]
    \item $\alg$ is $(\eps,\delta)$-differentially private; and,
    \item $\alg$ is an $(\alpha,\beta,\complexity)$-PAC learning algorithm for $\class$.
\end{enumerate*}
\end{definition}

In this thesis, we will also investigate the other notion of learnability in the light of privacy aspects. Namely, we define the natural definition combining privacy and consistency as follows:
\begin{definition}
An algorithm $\alg$ is said to be $(\eps,\delta)$-\emph{Privately universally consistent}, or PUC for short, 
if it is $(\eps,\delta)$-differentially private and universally consistent.
\end{definition}

\begin{remark}
Note that the utility requirement and the privacy requirement in the above definition are fundamentally different: Utility is only required to hold in the limiting regime when the sample size goes to infinity. In contrast, the privacy requirement is a worst-case kind of requirement that must hold for {\em any} two neighboring inputs, no matter how they were generated, even if they were not sampled from any distribution. 
\end{remark}

\section{Additional Notation}

Given a number $\ell\in\N$ and a dataset $S$ containing points from an ordered domain,
we use $\min(S,\ell)$ (or $\max(S,\ell)$) 
to indicate the subset of $\ell$ minimal (or maximal) values within $S$.
When $S$ contains points from a $d$-dimentional domain, we write $\min_i(S,\ell)$ (or $\max_i(S,\ell)$) to denote the subset of $\ell$ minimal (or maximal) values within $S$ w.r.t.\ the $i^{th}$ axis.

\chapter{Real Valued Compression}

In the study of machine learning theory,
the standard definitions of learning,
as PAC-learning for the binary case,
require the learner to achieve arbitrary small accuracy.
It is often difficult to be able to supply such a strong requirement,
but nevertheless it may be much simpler,
for a large set of problems,
to construct learners which are somewhat
better than a random labeling.
Those learners are called \emph{weak-learners} as opposed
to the standard \emph{strong-learners}.
The idea of combining weak-learners in a way that produces a strong-learner is called Boosting. 

As mentioned above (\ref{par:comp-and-boost}), boosting has been shown to be a powerful technique for constructing compression schemes. In this chapter, we will first explore theoretical concepts regarding boosting and the notion of weak learning. We will then show how these ideas can be applied to the construction of compression schemes, and present our main results in this area. 

\section{Boosting Real-Valued Functions}
\label{subsec:real-boosting}


The idea of leveraging or boosting weak-learners in order to achieve
stronger learning guarantees started as a question
proposed by \citeauthor{k-thb-88},
and reached a positive result in the seminal works
by \cite{DBLP:journals/ml/Schapire90}
and \cite{FreundSchapire97}.
The latter contained the well-known Adaboost algorithm,
which is widely used in practice.





\subsection{The MedBoost Algorithm}

In the context of boosting for real-valued functions, 
the notion of an $(\dev,\adv)$-weak hypothesis, defined above at Definition~\ref{def:weak}, plays a role analogous to the usual notion of a weak hypothesis in boosting for classification.
Using this notion, the following boosting algorithm was proposed by \citet{kegl2003robust}
as an extension to the classic Adaboost algorithm.

\begin{algorithm}[ht]
\label{alg:medboost}
\caption{\algname{MedBoost}($\{(x_i,y_i)\}_{i\in[m]}$,$T$,$\adv$,$\dev$)}
\begin{algorithmic}[1]
\State Define $P_{0}$ as the uniform distribution over $\{1,\ldots,n\}$
\For{$t=0,\ldots,T$} 
    \State Call weak learner to get $h_{t}$ and $(\dev/2,\adv)$-weak hypothesis \\ 
    \hspace{0.625cm} w.r.t. $(x_{i},y_{i})\! :\! i \!\sim\! P_{t}$ (repeat until it succeeds)
    \For{$i = 1,\ldots,m$}
        \State $\theta_{i}^{(t)} \gets 1 - 2 \ind[ |h_{t}(x_{i}) - y_{i}| > \dev/2 ]$
    \EndFor
    \State $\alpha_{t} \gets \frac{1}{2} \ln\!\left( \frac{ (1-\adv) \sum_{i=1}^{m} P_{t}(i) \ind[ \theta_{i}^{(t)} = 1 ] }{ (1+\adv) \sum_{i=1}^{m} P_{t}(i) \ind[ \theta_{i}^{(t)} = -1 ] } \right)$
    \If{$\alpha_{t} = \infty$} 
        \State Return $T$ copies of $h_{t}$, and $(1,\ldots,1)$
    \EndIf
    \For {$i = 1,\ldots,m$}
        \State $P_{t+1}(i) \gets P_{t}(i) \frac{\exp\{-\alpha_{t}\theta_{i}^{(t)}\}}{\sum_{j=1}^{m} P_{t}(j) \exp\{-\alpha_{t}\theta_{j}^{(t)}\}}$
    \EndFor
\EndFor
\State Return $(h_{1},\ldots,h_{T})$ and $(\alpha_{1},\ldots,\alpha_{T})$
\end{algorithmic}
\end{algorithm}

As it will be convenient for our later results, we expressed
the algorithm output as a sequence
of functions and weights; 
the boosting guarantee from \citet{kegl2003robust} applies to the weighted quantiles (and in particular, the weighted median) of these function values.

\begin{sloppypar}
Here we define the weighted median as 
\begin{equation*}
\text{Median}(y_{1},\ldots,y_{T};\alpha_{1},\ldots,\alpha_{T}) = \min\!\left\{ y_{j} : \frac{\sum_{t=1}^{T} \alpha_{t} \ind[ y_{j} < y_{t} ] }{\sum_{t=1}^{T} \alpha_{t}} < \frac{1}{2} \right\}.
\end{equation*}
Also define the weighted \emph{quantiles}, for $\adv \in [0,1/2]$, as 
\begin{align*}
Q_{\adv}^{+}(y_{1},\ldots,y_{T};\alpha_{1},\ldots,\alpha_{T}) & = \min\!\left\{ y_{j} : \frac{\sum_{t=1}^{T} \alpha_{t} \ind[ y_{j} < y_{t} ] }{\sum_{t=1}^{T} \alpha_{t}} < \frac{1}{2} - \adv \right\}
\\ Q_{\adv}^{-}(y_{1},\ldots,y_{T};\alpha_{1},\ldots,\alpha_{T}) & = \max\!\left\{ y_{j} : \frac{\sum_{t=1}^{T} \alpha_{t} \ind[ y_{j} > y_{t} ] }{\sum_{t=1}^{T} \alpha_{t}} < \frac{1}{2} - \adv \right\},
\end{align*}
and abbreviate $Q_{\adv}^{+}(x) = Q_{\adv}^{+}(h_{1}(x),\ldots,h_{T}(x);\alpha_{1},\ldots,\alpha_{T})$ and $Q_{\adv}^{-}(x) = Q_{\adv}^{-}(h_{1}(x),\ldots,h_{T}(x);\alpha_{1},\ldots,\alpha_{T})$ 
for $h_{1},\ldots,h_{T}$ and $\alpha_{1},\ldots,\alpha_{T}$ the values returned by \algname{MedBoost}.
\end{sloppypar}




After proposing the algorithm, \citet{kegl2003robust} proves the following:

\begin{lemma}
\label{lem:kegl}
(\citet{kegl2003robust}) 
For a training set $Z = \{(x_{1},y_{1}),\ldots,(x_{m},y_{m})\}$ of size $m$, 
the return values of \algname{MedBoost} satisfy 
\begin{equation*}
\frac{1}{m} \sum_{i=1}^{m} \ind\!\left[ \max\!\left\{ \left| Q_{\adv/2}^{+}(x_{i}) - y_{i} \right|, \left| Q_{\adv/2}^{-}(x_{i}) - y_{i} \right| \right\} > \dev/2 \right] 
\leq \prod_{t=1}^{T} e^{\adv \alpha_{t}} \sum_{i=1}^{m} P_{t}(i) e^{-\alpha_{t} \theta_{i}^{(t)}}.
\end{equation*}
\end{lemma}

We note that, in the special case of binary classification, \algname{MedBoost}
is closely related to the well-known AdaBoost algorithm
\citep{FreundSchapire97}, 
and the above results correspond to a
standard
margin-based analysis of
\citet{MR1673273}.

    

For our purposes, we will need the following corollary,
which we prove below.

\begin{corollary}
\label{cor:kegl-T-size}
For $T = \Theta\!\left(\frac{1}{\adv^{2}} \ln(m)\right)$, 
every $i \in \{1,\ldots,m\}$ has 
\begin{equation*}
\max\!\left\{ \left| Q_{\adv/2}^{+}(x_{i}) - y_{i} \right|, \left| Q_{\adv/2}^{-}(x_{i}) - y_{i} \right| \right\} \leq \dev/2.
\end{equation*}
\end{corollary}

In the proof, we use the following technical lemma

\begin{lemma}
  \label{lem:cor-tech-kegl}
  For $x \geq \uf{2} + \gamma$ it holds that
  \[
    x^{1+\gamma} (1-x)^{1-\gamma}
    \leq
    \left(\frac{1}{2} + \gamma\right)^{1-\gamma}
    \left(\frac{1}{2} - \gamma\right)^{1+\gamma}
    .
  \]
\end{lemma}

\begin{proof}
  Denote the left side as a function $f$
  and take log of $f$ 
  \[
    \log(f(x)) = (1+\gamma)\log(x) + (1-\gamma)\log(1-x).
  \]
  Observe that the derivative with respect to $x$ which is
  $(\log(f(x)))' = (1+\gamma)/x - (1-\gamma)/(1-x)$
  is negative for
  $x \geq (1+\gamma)/2$.
  Since
  $x \geq \uf{2} + \gamma > (1+\gamma)/2$
  this condition holds.
  So the function
  $\log(f(a)) := \log\left(a^{1+\gamma} (1-a)^{1-\gamma}\right)$
  is monotonically decreasing
  and by that also $f$ itself is monotonically decreasing.
  Hence
  \begin{align*}
    x^{1+\gamma} (1-x)^{1-\gamma}
    \leq
    (\uf{2} + \gamma)^{1+\gamma}
    (1 - \uf{2} + \gamma)^{1-\gamma}
    .
  \end{align*}

\end{proof}

\begin{proof}[Proof of Corollary~\ref{cor:kegl-T-size}]
  By the definition of $\alpha_t$ we know that
  \[
    e^{\alpha_t} = 
    \left(
      \frac{(1-\gamma)\sum_{\theta_i(t)=1}P_t(i)}
      {(1+\gamma)\sum_{\theta_i(t)=-1}P_t(i)}
    \right)^{1/2}.
  \]

  Split the sum within the RHS into
  $\set{i \mid \theta_i(t) = 1}$
  and
  $\set{i \mid \theta_i(t) = -1}$
  to get that
  \begin{align*}
    &\prod_{t=1}^T e^{\gamma \alpha_t} 
    \sum_{i=1}^m P_t(i) e^{-\alpha_t \theta_i(t)}a \\
    &= \prod_{t=1}^T e^{\gamma \alpha_t} 
    \Biggl[
    \sum_{\theta_i(t)=1} P_t(i) e^{-\alpha_t}
    +
    \sum_{\theta_i(t)=-1} P_t(i) e^{\alpha_t}
    \Biggl] \\
    &= \prod_{t=1}^T e^{\gamma \alpha_t} 
    \Biggl[
    e^{-\alpha_t} \sum_{\theta_i(t)=1} P_t(i) 
    +
    e^{\alpha_t} \sum_{\theta_i(t)=-1} P_t(i)
    \Biggl] \\
    &= \prod_{t=1}^T
    \Biggl[
    e^{-\alpha_t(1-\gamma)} \sum_{\theta_i(t)=1} P_t(i) 
    +
    e^{\alpha_t(1+\gamma)} \sum_{\theta_i(t)=-1} P_t(i)
    \Biggl].
  \end{align*}

  Plug-in $e^{\alpha_t}$ 
  \begin{align*}
    &= \prod_{t=1}^T 
    \Biggl[
    \left(
    \frac{(1+\gamma)\sum_{\theta_i(t)=-1}P_t(i)}
    {(1-\gamma)\sum_{\theta_i(t)=1}P_t(i)}
    \right)^{\frac{1-\gamma}{2}} 
    \sum_{\theta_i(t)=1} P_t(i) \\
    &\qquad+
    \left(
    \frac{(1-\gamma)\sum_{\theta_i(t)=1}P_t(i)}
    {(1+\gamma)\sum_{\theta_i(t)=-1}P_t(i)}
    \right)^{\frac{1+\gamma}{2}}
    \sum_{\theta_i(t)=-1} P_t(i)
    \Biggr]\\
    &=
    \prod_{t=1}^T 
    \Biggl[
    \left(\sum_{\theta_i(t)=1}P_t(i)\right)^{\frac{1+\gamma}{2}}
    \left(\sum_{\theta_i(t)=-1}P_t(i)\right)^{\frac{1-\gamma}{2}}
    \left(\frac{1+\gamma}{1-\gamma}\right)^{\frac{1-\gamma}{2}} \\
    &\qquad+
    \left(\sum_{\theta_i(t)=1}P_t(i)\right)^{\frac{1+\gamma}{2}}
    \left(\sum_{\theta_i(t)=-1}P_t(i)\right)^{\frac{1-\gamma}{2}}
    \left(\frac{1-\gamma}{1+\gamma}\right)^{\frac{1+\gamma}{2}}
    \Biggr].
  \end{align*}
  By the $(\varepsilon,\gamma)$-weak-learning guarantee
  we know that
  $$\sum_{\theta_i(t)=1}P_t(i) \geq \uf{2} + \gamma$$
  and
  $$\sum_{\theta_i(t)=-1}P_t(i) < \uf{2} - \gamma$$  
  and by Lemma~\ref{lem:cor-tech-kegl}
  \begin{align*}
    &\leq \prod_{t=1}^T 
    \Biggl[
    \left(\frac{1+\gamma}{1-\gamma}\right)^{\frac{1-\gamma}{2}}
    +
    \left(\frac{1-\gamma}{1+\gamma}\right)^{\frac{1+\gamma}{2}}
    \Biggr]
    \left(\frac{1}{2} + \gamma\right)^{\frac{1-\gamma}{2}}
    \left(\frac{1}{2} - \gamma\right)^{\frac{1+\gamma}{2}}\\
    &=
    \prod_{t=1}^T 
    \uf{2}
    \left(\frac{1-\gamma}{1+\gamma}\right)^{\frac{\gamma}{2}}
    \left(\frac{1+2\gamma}{1-2\gamma}\right)^{\frac{\gamma}{2}}
    (1 - 4\gamma^2)^{1/2}
    \left(
    \left(\frac{1+\gamma}{1-\gamma}\right)^{1/2}
    +
    \left(\frac{1-\gamma}{1+\gamma}\right)^{1/2}
    \right),
  \end{align*}
  noting that for every
  $\gamma \in (0,1/3)$.
  \[
    \uf{2}
    \left(\frac{1-\gamma}{1+\gamma}\right)^{\frac{\gamma}{2}}
    \left(\frac{1+2\gamma}{1-2\gamma}\right)^{\frac{\gamma}{2}}
    (1 - 4\gamma^2)^{1/2}
    \left(
      \left(\frac{1+\gamma}{1-\gamma}\right)^{1/2}
      +
      \left(\frac{1-\gamma}{1+\gamma}\right)^{1/2}
    \right)
    <
    e^{-\gamma^2/4},
  \]
  we get that
  \begin{align*}
    \frac{1}{m} \sum_{i=1}^{m} \ind\!\left[ \max\!\left\{ \left| Q_{\adv/2}^{+}(x_{i}) - y_{i} \right|, \left| Q_{\adv/2}^{-}(x_{i}) - y_{i} \right| \right\} > \dev/2 \right] \\
    \leq \prod_{t=1}^{T} e^{\adv \alpha_{t}} \sum_{i=1}^{m} P_{t}(i) e^{-\alpha_{t} \theta_{i}^{(t)}}
    <
    e^{-T\gamma^2/4}    .
  \end{align*}
  Finally for
  $T = \frac{4}{\gamma^2}\ln(m)$
  the last bound is equal to
  $\uf{m}$
  and hence the corollary holds.
\end{proof}
    


\subsection{The Sample Complexity of Weak Learning}
\label{subsec:weak-learning}

This section reveals our intent in choosing this notion of weak hypothesis, 
rather than using, for example, an $\eps$-good strong learner under absolute loss.
In addition to being a strong enough notion for boosting to work, 
we show here that it is also a weak enough notion for the sample complexity 
of weak learning to be of reasonable size: namely, a size quantified by the fat-shattering dimension.
This result is also relevant to an open question posed by
\citet{DBLP:journals/siamcomp/Simon97},
which we discuss on Subsection~\ref{sec:simon-lower-bound}.

    


\subsubsection{The Notion of "Weak Learning"}
\label{sec:notion-weak-learning}

As mentioned above, the notion of a \emph{weak learner} for learning real-valued functions must be formulated carefully.
The na\"{i}ve thought that we could take any learner guaranteeing, for example, absolute loss at most $\frac{1}{2}-\adv$, 
is known to be not strong enough to enable boosting to $\eps$ loss.  However, if we make the requirement too strong, 
such as in
\citet{FreundSchapire97}
for
{\tt AdaBoost.R},
then the sample complexity of weak learning will be so high that weak learners cannot 
be expected to exist for large classes of functions. 

Starting with \citeauthor{k-thb-88} and \citeauthor{DBLP:journals/ml/Schapire90}, 
the notion of weak learning was tied to the notion of
PAC learnability.
Weak learning is, as one may expect,
the weak version of PAC learning.
This relation meant that weak-learning also
was defined using a loss-function
and a (weak) upper-bound on
the loss of the resulting hypothesis,
namely a fixed, yet bounded away from $1/2$,
bound on the expected loss.

Normally when extending the PAC paradigm
to the real-valued/continuous case,
we just replace the loss-function.
Thus, we get the following
\begin{definition}["Standard"-Weak-Hypothesis]
  For $\adv \in [0,1/2]$,
  we say that $f:\X\to\R$ is an  
  an \emph{$\adv$-weak hypothesis}
  (with respect to distribution $D$ and target $f^*\in\F$)
  if \[\E_{X\sim D} \left[ l(f_S (x), f^*(x)) \right] \leq \uf{2} - \adv.\]
\end{definition}

Unfortunately,
this extension
for the problem of boosting essentially fails.
\citet[Remark 2.1]{DuffyHelmbold02}
point out that,
using this notion of
weak learning,
one can not guarantee
that using the method of
modifying the distribution over the sample
will force the learner
to establish a good hypothesis.
This is due to the fact that,
unlike the binary-case,
the error can be spread evenly
over all the sample,
meaning that the error remains the same
regardless of the distribution on the sample.
This might result in the learner
outputting the same hypothesis on each iteration,
and hence not improving the error
of the final output regressor.
Some lines of work, including
\citeauthor{FreundSchapire97}'s
{\tt AdaBoost.R},
used more complex boosting ideas in order to bypass this problem.
Those algorithms are either problematic in their runtime,
or, as in the {\tt AdaBoost.R} case, based on weak learners
whose sample complexity depends on the Pseudo-dimension of the class
\footnote{A different combinatorial dimension for real-valued function classes,
first defined by \citeauthor{pollard84}.},
which tends to be so high that weak learners cannot 
be expected to exist for large classes of functions.

For this reason, we use a different notion.
Recall the definition
\begin{definition}[$(\dev,\adv)$-Weak-Hypothesis]
  For $\dev \in [0,1]$ and $\adv \in [0,1/2]$,
  we say that $f:\X\to\R$ is an  
  an \emph{$(\dev,\adv)$-weak hypothesis}
  (with respect to distribution $D$ and target $f^*\in\F$)
  if $$\Pr_{X\sim D}(|f(X)-f^*(X)|>\dev)\le \frac12-\adv.$$
\end{definition}
The $(\dev,\adv)$-weak-learner,
which has appeared, among other works, in
\citet{DBLP:journals/cpc/AnthonyBIS96,DBLP:journals/siamcomp/Simon97,avnimelech1999boosting,kegl2003robust},
gets around this difficulty
by demanding a bound on the measure of the
points  in which the hypothesis has "big" local error.
Furthermore, this notion was in fact proven useful
in various, quite simple, boosting mechanisms,
but, to our knowledge,
provable general constructions of such learners have been lacking.
Note that, as in other definitions of weak-learning,
this definition also uses a "strong" definition of learning,
which was proposed by \citeauthor{DBLP:journals/siamcomp/Simon97}.
\begin{definition}[$(\varepsilon,\adv)$-good-model]
  For $\varepsilon, \dev \in [0,1]$ and $\adv \in [0,1/2]$,
  we say that $f:\X\to\R$ is an  
  an \emph{$(\varepsilon,\adv)$-good model}
  (with respect to distribution $D$ and target $f^*\in\F$)
  if $$\Pr_{X\sim D}(|f(X)-f^*(X)|>\dev)\le \varepsilon.$$
\end{definition}
and a $\mathcal{A}$ is $\adv$-learner
if \emph{for every} $\varepsilon, \delta$ and
sample $S$ of size $m=m(\varepsilon,\delta)$,
with probability at least $1-\delta$,
$f = \mathcal{A}(S)$ is a $(\varepsilon,\adv)$-good-model.
So $(\dev,\adv)$-weak-learner is simply
a $\adv$-learner
with the error parameter $\varepsilon$ fixed,
and bounded away from $1/2$.

Although there exist several uses of this type of "weak-learning",
to our knowledge there exist no provable constructions of such algorithms.
We now present a provable and very natural, namely ERM based, $(\dev,\adv)$-learner.
From this result, we are also able to construct our $(\dev,\adv)$-weak-learner,
which was used by our compression-boosting mechanism.

    



\subsubsection{Upper Bound on The Sample Complexity of $(\varepsilon,\adv)$-Good-Learning}

The following result is stated in the notion of the more general case of
$(\varepsilon,\adv)$-good-model.
in order to apply it to our boosting mechanism,
we later fix the error parameter $\varepsilon$ as
previously discussed,
which then yields an Upper Bound
on the sample complexity of $(\varepsilon,\adv)$-weak-learner.

Define $\rho_{\dev}(f,g) = P_{2m}( x : |f(x) - g(x)| > \dev )$, where $P_{2m}$ is the empirical measure induced by $X_{1},\ldots,X_{2m}$ iid $P$-distributed random variables
(the $m$ data points and $m$ ghost points).
Define $N_{\dev}(\beta)$ as the $\beta$-covering numbers of $\F$
under the $\rho_{\dev}$ pseudo-metric.

\begin{theorem}
\label{thm:weak-learning-bound}
Fix any $\dev,\beta \in (0,1)$, $\alpha \in [0,1)$, and $m \in \mathbb{N}$. 
For $X_{1},\ldots,X_{m}$ iid $P$-distributed, 
with probability at least $1 - \E\!\left[ N_{\dev (1-\alpha)/2}(\beta/8) \right] 2 e^{-m \beta / 96}$, 
every $f \in \F$ with $\max_{1 \leq i \leq m} |f(X_{i}) - f^{*}(X_{i})| \leq \alpha \dev$ 
satisfies $P( x : | f(x) - f^{*}(x) | > \dev ) \leq \beta$.
\end{theorem}
\begin{proof} 
This proof roughly follows the usual symmetrization argument for uniform convergence \citep{MR0288823,DBLP:journals/iandc/Haussler92}, 
with a few important modifications to account for this $(\dev,\beta)$-based criterion. 
If $\E\!\left[ N_{\dev (1-\alpha)/2}(\beta/8) \right]$ is infinite, then the result is trivial, so let us suppose it is finite for the remainder of the proof.
Similarly, if $m < 8/\beta$, then $2 e^{-m \beta/96} > 1$ and hence the claim trivially holds, so let us suppose $m \geq 8/\beta$ for the remainder of the proof.
Without loss of generality, suppose $f^{*}(x) = 0$ everywhere and every $f \in \F$ is non-negative 
(otherwise subtract $f^{*}$ from every $f \in \F$ and redefine $\F$ as the absolute values of the differences; 
note that this transformation does not increase the value of $N_{\dev (1-\alpha)/2}(\beta/8)$ since applying this 
transformation to the original $N_{\dev (1-\alpha)/2}(\beta/8)$ functions remains a cover).

Let $X_{1},\ldots,X_{2m}$ be iid $P$-distributed.  Denote by $P_{m}$ the empirical measure induced by $X_{1},\ldots,X_{m}$, 
and by $P_{m}^{\prime}$ the empirical measure induced by $X_{m+1},\ldots,X_{2m}$.
We have 
\begin{align*}
& \Pr\!\left( \exists f \in \F : P_{m}^{\prime}( x : f(x) > \dev ) > \beta/2 \text{ and } P_{m}( x : f(x) \leq \alpha \dev ) = 1 \right)
\\ & \geq \Pr\Big( \exists f \in \F : P( x : f(x) > \dev ) > \beta \text{ and } P_{m}( x : f(x) \leq \alpha \dev ) = 1 \\
&\quad\quad\quad\text{ and }  P_{m}^{\prime}( x : f(x) > \dev ) > \beta/2 \Big).
\end{align*}
Denote by $A_{m}$ the event that there exists $f \in \F$ 
satisfying $P( x : f(x) > \dev ) > \beta$ and $P_{m}( x : f(x) \leq \alpha \dev ) = 1$, 
and on this event let $\tilde{f}$ denote such an $f \in \F$ (chosen solely based on $X_{1},\ldots,X_{m}$); 
when $A_{m}$ fails to hold, take $\tilde{f}$ to be some arbitrary-fixed element of $\F$.
Then the expression on the right-hand side above is at least as large as 
\begin{equation*}
\Pr\left( A_{m} \text{ and } P_{m}^{\prime}( x : \tilde{f}(x) > \dev ) > \beta/2 \right),
\end{equation*}
and noting that the event $A_{m}$ is independent of $X_{m+1},\ldots,X_{2m}$, this equals
\begin{equation}
\label{eqn:uc-factored-expression}
\E\!\left[ \ind_{A_{m}} \cdot \Pr\!\left( P_{m}^{\prime}( x : \tilde{f}(x) > \dev ) > \beta/2 \middle| X_{1},\ldots,X_{m} \right) \right].
\end{equation}
Then note that for any $f \in \F$ with $P( x : f(x) > \dev ) > \beta$, a Chernoff bound implies 
\begin{align*}
  &
  \Pr\Big(  P_{m}^{\prime}( x : f(x) > \dev ) > \beta/2 \Big) 
  \\ &
  = 1 - \Pr\Big( P_{m}^{\prime}( x : f(x) > \dev ) \leq \beta/2 \Big) 
\geq 1 - \exp\!\left\{ - m \beta / 8 \right\} 
\geq \frac{1}{2},
\end{align*}
where we have used the assumption that $m \geq \frac{8}{\beta}$ here.
In particular, this implies that the expression in \eqref{eqn:uc-factored-expression} is no smaller than 
$\frac{1}{2} \Pr(A_{m})$.
Altogether, we have established that 
\begin{align}
& \Pr\!\left( \exists f \in \F : P( x : f(x) > \dev ) > \beta \text{ and } P_{m}( x : f(x) \leq \alpha \dev ) =1 \right) \notag 
\\ & \leq 2 \Pr\!\left( \exists f \in \F : P_{m}^{\prime}( x : f(x) > \dev ) > \beta/2 \text{ and } P_{m}( x : f(x) \leq \alpha \dev ) = 1 \right). \label{eqn:uc-ghost-ub}
\end{align}

Now let $\sigma(1),\ldots,\sigma(m)$ be independent random variables (also independent of the data), with $\sigma(i) \sim \emph{Uniform}(\{i,m+i\})$,
and denote $\sigma(m+i)$ as the sole element of $\{i,m+i\} \setminus \{\sigma(i)\}$ for each $i \leq m$.
Also denote by $P_{m,\sigma}$ the empirical measure induced by $X_{\sigma(1)},\ldots,X_{\sigma(m)}$, 
and by $P_{m,\sigma}^{\prime}$ the empirical measure induced by $X_{\sigma(m+1)},\ldots,X_{\sigma(2m)}$.
By exchangeability of $(X_{1},\ldots,X_{2m})$, the right-hand side of \eqref{eqn:uc-ghost-ub} is equal 
\begin{equation*}
\Pr\!\left( \exists f \in \F : P_{m,\sigma}^{\prime}( x : f(x) > \dev ) > \beta/2 \text{ and } P_{m,\sigma}( x : f(x) \leq \alpha \dev ) = 1 \right).
\end{equation*}
Now let $\hat{\F} \subseteq \F$ be a minimal subset of $\F$ such that 
$\max\limits_{f \in \F} \min\limits_{\hat{f} \in \hat{\F}} \rho_{\dev (1-\alpha)/2}(\hat{f},f) \leq \beta/8$.
The size of $\hat{\F}$ is at most $N_{\dev(1-\alpha)/2}(\beta/8)$, which is finite almost surely (since we have assumed above that its expectation is finite). 
Then note that (denoting by $X_{[2m]} = (X_{1},\ldots,X_{2m})$) the above expression is at most \small{
\begin{align}
& \Pr\!\left( \exists f \in \hat{\F} : P_{m,\sigma}^{\prime}( x : f(x) > \dev (1+\alpha)/2 ) > (3/8)\beta \text{ and } P_{m,\sigma}( x : f(x) > \dev (1+\alpha)/2 ) \leq \beta/8 \right) \notag 
\\ & \leq \E\!\left[ N_{\dev (1-\alpha)/2}(\beta/8) \max_{f \in \hat{\F}} \Pr\!\left( P_{m,\sigma}^{\prime}( x \!:\! f(x) \!>\! \dev (1+\alpha)/2 ) > (3/8)\beta \right.\right. \notag
\\ & {\hskip 4.5cm} \left. \phantom{\max_{f \in \hat{\F}}} \left. \text{ and } 
P_{m,\sigma}( x \!:\! f(x) \!>\! \dev (1+\alpha)/2 ) \leq \beta/8 \middle| X_{[2m]} \right) \right]\!. \label{eqn:uc-condition-on-data}
\end{align}
}
Then note that for any $f \in \F$, we have almost surely 
\begin{align*}
& \Pr\!\left( P_{m,\sigma}^{\prime}( x : f(x) > \dev (1+\alpha)/2 ) > (3/8)\beta \text{ and } P_{m,\sigma}( x : f(x) > \dev (1+\alpha)/2 ) \leq \beta/8 \middle| X_{[2m]} \right)
\\ & \leq \Pr\!\left( P_{2m}( x : f(x) > \dev (1+\alpha)/2 ) > (3/16) \beta \text{ and } P_{m,\sigma}( x : f(x) > \dev (1+\alpha)/2 ) \leq \beta/8 \middle| X_{[2m]} \right)
\\ & \leq \exp\!\left\{ - m \beta / 96 \right\},
\end{align*}
where the last inequality is by a Chernoff bound, which (as noted by \citet*{hoeffding}) remains valid even when sampling without replacement.
Together with \eqref{eqn:uc-ghost-ub} and \eqref{eqn:uc-condition-on-data}, we have that
\begin{align*}
& \Pr\!\left( \exists f \in \F : P( x : f(x) > \dev ) > \beta \text{ and } P_{m}( x : f(x) \leq \alpha \dev ) =1 \right) 
\\ & \leq 2 \E\!\left[ N_{\dev (1-\alpha)/2}(\beta/8) \right] e^{- m \beta / 96}.
\end{align*}
\end{proof}


\begin{lemma}
\label{lem:eps-gam-covering-numbers}
There exist universal numerical constants $c,c^{\prime} \in (0,\infty)$ such that $\forall \dev,\beta \in (0,1)$,  
\begin{equation*}
N_{\dev}(\beta) \leq \left(\frac{2}{\dev\beta}\right)^{c \fat{c^{\prime} \dev \beta}{\F}},
\end{equation*}
where $\fat{\cdot}{\cdot}$ is the fat-shattering dimension.
\end{lemma}
\begin{proof}
   \citet[Theorem 1]{MR1965359} establishes that 
the $\dev\beta$-covering number of $\F$ under the $L_{2}(P_{2m})$ pseudo-metric 
is at most 
\begin{equation}
\label{eqn:mendelson-vershynin-bound}
\left(\frac{2}{\dev\beta}\right)^{c \fat{c^{\prime} \dev \beta}{\F}}
\end{equation}
for some universal numerical constants $c,c^{\prime} \in (0,\infty)$.
Then note that for any $f,g \in \F$, Markov's
and Jensen's inequalities imply
$\rho_{\dev}(f,g) \leq \frac{1}{\dev} \| f - g \|_{L_{1}(P_{2m})} \leq \frac{1}{\dev} \| f - g \|_{L_{2}(P_{2m})}$.
Thus, any $\dev\beta$-cover of $\F$ under $L_{2}(P_{2m})$ is also a $\beta$-cover of $\F$ under $\rho_{\dev}$,
and therefore \eqref{eqn:mendelson-vershynin-bound} is also a bound on $N_{\dev}(\beta)$.
\end{proof}

Combining the above two results yields the following theorem.

\begin{theorem}
\label{thm:eps-gam-weak-sample-complexity}
For some universal numerical constants $c_{1},c_{2},c_{3} \in (0,\infty)$, 
for any $\dev,\delta,\beta \in (0,1)$ and $\alpha \in [0,1)$,
letting $X_{1},\ldots,X_{m}$ be iid $P$-distributed, where 
\begin{equation*}
m = \left\lceil \frac{c_{1}}{\beta} \left(  \fat{c_{2} \dev \beta (1-\alpha)}{\F} \ln\!\left( \frac{c_{3}}{\dev \beta (1-\alpha)} \right) + \ln\!\left(\frac{1}{\delta}\right) \right) \right\rceil, 
\end{equation*}
with probability at least $1-\delta$, every $f \in \F$ with $\max_{i \in [m]} |f(X_{i}) - f^{*}(X_{i})| \leq \alpha \dev$
satisfies $P( x : | f(x) - f^{*}(x) | > \dev ) \leq \beta$.
\end{theorem}
\begin{proof}
The result follows immediately from combining Theorem~\ref{thm:weak-learning-bound} and Lemma~\ref{lem:eps-gam-covering-numbers}.
\end{proof}

In particular, the specific case of weak-learners,
as stated in Theorem~\ref{thm:gen-weak-learn},
follows immediately from this result by taking $\beta = 1/2 - \adv$ and $\alpha = \adv/2$.


    

\subsubsection{Tightness of The Upper Bound}
\label{sec:simon-lower-bound}

To discuss tightness of Theorem~\ref{thm:eps-gam-weak-sample-complexity}, we note that
in addition to the definition of a \emph{$(\beta,\dev)$-good model}
\citet{DBLP:journals/siamcomp/Simon97} also proved the following lower bound
\begin{theorem}[\cite{DBLP:journals/siamcomp/Simon97}]
  Let $A$ be an algorithm which learns function class $F$ with an
  $(\beta, \dev)$-good model
  \begin{enumerate}
  \item If $F$ is nontrivial
    \footnote{Meaning: there exist $f,g \in F$
      which are not pairwise disjoin, namely
      $\exists x\in X: f(x) = g(x)$.
    }
    , $\beta < 1/2$ and $\dev < \Delta(F)/2$.
    then $A$ needs $\Omega(\ln(1/\delta)/\beta)$ examples.
  \item If $\beta \leq 1/8$, $0 < \delta \leq 1/100$. then $A$ needs
    $\Omega((d_F^N(\dev) - 1)/\beta)$ examples.
  \end{enumerate}  
\end{theorem}

When
$ \Delta(F) = sup\{\Norm{g - f}_\infty \mid \exists x \in X : f(x) = g(x)\}$.

Combining the two, we get that
a sample complexity lower bound 
for the same criterion of 
\begin{equation*}
  \Omega \left( \frac{d_F^N(c \dev)}{\beta} + \uf{\beta}\log\uf{\delta} \right),
\end{equation*}
where $d_F^N(\cdot)$ is a quantity somewhat smaller than the fat-shattering dimension, 
essentially representing a fat Natarajan dimension.

Simon showed that this lower bound is tight and
placed an open question
\paragraph{Open Problem:}
For every function class $F$
there exists an algorithm $A$
which learns $F$ with an
$(\beta, \dev)$-good model,
using
\[\bigO{ \frac{d_F^N(\dev)}{\beta} + \uf{\beta}\ln(1/\delta) }\]
examples.

Thus, aside from the differences in the complexity measure (and a logarithmic factor), 
we establish an upper bound of a similar form to Simon's lower bound, hence making significant progress towards solving Simon's open question.

    

\section{From Boosting to Compression}
    
\label{subsec:boosting-to-compression}

Generally, our strategy for converting the boosting algorithm \algname{MedBoost} into a sample compression scheme of smaller size 
follows a strategy of Moran and Yehudayoff for binary classification, based on arguing that because the ensemble makes its predictions 
with a \emph{margin} (corresponding to the results on \emph{quantiles} in Corollary~\ref{cor:kegl-T-size}), 
it is possible to recover the same proximity guarantees for the predictions while using only a smaller \emph{subset} of the 
functions from the original ensemble.  Specifically, we use the following general \emph{sparsification} strategy.

For $\alpha_{1},\ldots,\alpha_{T} \in [0,1]$ with $\sum_{t=1}^{T} \alpha_{t} = 1$, 
denote by $Cat( \alpha_{1},\ldots,\alpha_{T} )$ the \emph{categorical distribution}: 
i.e., the discrete probability distribution on $\{1,\ldots,T\}$ with probability mass $\alpha_{t}$ on $t$.

\begin{algorithm}[ht]
  \caption{\algname{Sparsify}($\{(x_i,y_i)\}_{i\in[m]},\adv,T,n$)}
  \begin{algorithmic}[1]
    \State Run \algname{MedBoost}($\{(x_i,y_i)\}_{i\in[m]},T,\adv,\dev$)
    \State Let $h_{1},\ldots,h_{T}$ and $\alpha_{1},\ldots,\alpha_{T}$ be its return values 
    \State Denote $\alpha_{t}^{\prime} = \alpha_{t} / \sum_{t^{\prime}=1}^{T} \alpha_{t^{\prime}}$ for each $t\in[T]$
    \Repeat
      \State Sample $(J_{1},\ldots,J_{n}) \sim Cat( \alpha_{1}^{\prime},\ldots,\alpha_{T}^{\prime} )^{n}$ 
      \State Let $F = \{ h_{J_{1}},\ldots, h_{J_{n}} \}$
      \Until {$\max_{1 \leq i \leq m} |\{f \in F: |f(x_{i}) - y_{i}| > \dev\}| < n/2$} 
      \State Return $F$
  \end{algorithmic}
\end{algorithm}

For any values $a_{1},\ldots,a_{n}$, denote the (unweighted) median 
\begin{equation*}
\Med(a_{1},\ldots,a_{n}) = \Median(a_{1},\ldots,a_{n}; 1,\ldots,1).
\end{equation*}
Our intention in discussing the above algorithm is to argue that, for a sufficiently large choice of $n$, 
the above procedure returns a set $\{f_{1},\ldots,f_{n}\}$ such that 
\[
\forall i\in[m], | \Med(f_{1}(x_{i}),\ldots,f_{n}(x_{i})) - y_{i} | \leq \dev.
\]

We analyze this strategy separately for binary classification and real-valued functions, 
since the argument in the binary case is much simpler (and
demonstrates more directly
the connection to the 
original argument of Moran and Yehudayoff), and also because we arrive at a tighter result for binary functions 
than for real-valued functions.

    
    


\subsection{Binary Classification}

\label{subsubsec:binary-classification-compression}

We begin with the simple observation about binary classification (i.e., where the functions in $\F$ all map into $\{0,1\}$).
The technique here is quite simple, and follows a similar line of reasoning to the 
original argument of Moran and Yehudayoff.  The argument for real-valued functions 
below will diverge from this argument in several important ways, but the high level 
ideas remain the same.

The compression function is essentially the one introduced by Moran and Yehudayoff, 
except applied to the classifiers produced by the above \algname{Sparsify} procedure, 
rather than a set of functions selected by a minimax distribution over all classifiers produced by $O(\vc{\F})$ samples each.
The weak hypotheses in \algname{MedBoost} for binary classification can be obtained using samples of size $O(\vc{\F})$.
Thus, if the \algname{Sparsify} procedure is successful in finding $n$ such classifiers whose median predictions are 
within $\dev$ of the target $y_{i}$ values for all $i$, 
then we may encode these $n$ classifiers as a compression set, 
consisting of the set of $k = O(n\vc{\F})$ samples used to train these classifiers, 
together with $k \log k$ extra bits to encode the order of the samples.\footnote{In fact, 
$k \log n$ bits would suffice if the weak learner is permutation-invariant in its data set.}
To obtain Theorem~\ref{thm:classification}, it then suffices to argue that $n=\Theta(\dualvc{\F})$ is a sufficient value.
The proof follows.

\begin{proof}[Proof of Theorem~\ref{thm:classification}]
Recall that $\dualvc{\F}$ bounds the VC dimension of the class of sets $\{ \{ h_{t} : t \leq T, h_{t}(x_i) = 1 \} : 1 \leq i \leq m \}$.
Thus for the iid samples $h_{J_{1}},\ldots,h_{J_{n}}$ obtained in \algname{Sparsify}, 
for $n = 64(2309 + 16\dualvc{\F}) > \frac{2304 + 16\dualvc{\F}+\log(2)}{1/8}$, 
by the
VC
uniform convergence inequality of 
\citet{MR0288823}, 
with probability at least $1/2$ 
we get that
\begin{equation*}
  \max_{1 \leq i \leq m} \left| \left( \frac{1}{n} \sum_{j=1}^{n} h_{J_{j}}(x_i) \right)  - \left( \sum_{t=1}^{T} \alpha^{\prime} h_{t}(x_i) \right) \right| < 1/8.
\end{equation*}
In particular, if we choose $\adv = 1/8$, $\dev = 1$, and $T = \Theta(\log(m))$ appropriately, 
then Corollary~\ref{cor:kegl-T-size} implies that 
every $y_i = \ind\!\left[ \sum_{t=1}^{T} \alpha^{\prime} h_{t}(x_i) \geq 1/2 \right]$ 
and $\left| \frac{1}{2} - \sum_{t=1}^{T} \alpha^{\prime} h_{t}(x_i) \right| \geq 1/8$ 
so that the above event would imply 
every 
$$y_i = \ind\!\left[ \frac{1}{n} \sum_{j=1}^{n} h_{J_{j}}(x_i) \geq 1/2 \right] = \Med(h_{J_{1}}(x_i),\ldots,h_{J_{n}}(x_i)).$$
Note that the \algname{Sparsify} algorithm need only try this sampling $\log_{2}(1/\delta)$ times to find such a set of $n$ functions.
Combined with the description above (from \citealp{moran2016sample}) 
of how to encode this collection of $h_{J_{i}}$ functions as a sample compression set plus side information, 
this completes the construction of the sample compression scheme.
\end{proof}


    


\subsection{Real-Valued Functions}


\label{subsubsec:real-valued-compression}


Next, we turn to the general case of real-valued functions (where the functions in $\F$ may generally map into $[0,1]$).
We have the following result, which says that the \algname{Sparsify} procedure can reduce the ensemble of functions 
from one with $T=O(\log(m)/\adv^2)$ functions in it, down to one with a number of functions \emph{independent of $m$}.

\begin{theorem}
\label{thm:real-sparsification}
Choosing $$n = \Theta\!\left( \frac{1}{\adv^{2}} \dualfat{c\dev}{\F} \log^{2}( \dualfat{c\dev}{\F}/\dev ) \right)$$ 
suffices for the \algname{Sparsify} procedure to return $\{f_{1},\ldots,f_{n}\}$ 
with $$\max_{1 \leq i \leq m} | \Med(f_{1}(x_{i}),\ldots,f_{n}(x_{i})) - y_{i} | \leq \dev.$$
\end{theorem}

\begin{proof}

Recall from Corollary~\ref{cor:kegl-T-size} that 
\algname{MedBoost} returns functions $h_{1},\ldots,h_{T} \in \F$ and $\alpha_{1},\ldots,\alpha_{T} \geq 0$ 
such that $\forall i \in \{1,\ldots,m\}$, 
\begin{equation*}
\max\!\left\{ \left| Q_{\adv/2}^{+}(x_{i}) - y_{i} \right|, \left| Q_{\adv/2}^{-}(x_{i}) - y_{i} \right| \right\} \leq \dev/2,
\end{equation*}
where $\{(x_{i},y_{i})\}_{i=1}^{m}$ is the training data set.
We use this %
property to sparsify $h_{1},\ldots,h_{T}$ from $T=O(\log(m)/\adv^2)$
down to $k$
elements, where $k$ will depend on $\dev,\adv$, and
the dual fat-shattering
dimension of $\F$
(actually, just of $H = \{h_{1},\ldots,h_{T}\}\subseteq\F$)
but {\bf not} sample size $m$.

Letting $\alpha_{j}^{\prime} = \alpha_{j} / \sum_{t=1}^{T} \alpha_{t}$ for each $j \leq T$, 
we will sample $k$ hypotheses $\set{\tilde h_1,\ldots,\tilde h_k}=:\tilde H\subseteq H$ 
with each $\tilde{h}_{i} = h_{J_{i}}$, where
$(J_{1},\ldots,J_{k}) \sim \Cat(\alpha_{1}^{\prime},\ldots,\alpha_{T}^{\prime})^{k}$
as in \algname{Sparsify}.
Define a function $\hat{h}(x) = \Med(\tilde{h}_{1}(x),\ldots,\tilde{h}_{k}(x))$.
We claim that for any fixed $i\in[m]$,
with high probability
\begin{equation}
  \label{eq:Htil-med}
|\hat{h}(x_i)-f^*(x_i)|\le\dev/2.
\end{equation}
\begin{sloppypar}
Indeed,
partition the indices $[T]$ into the disjoint sets
\small{
\begin{align*}
L(x) &= \set{ j \in[ T] : h_{j}(x) < Q_{\adv}^{-}(h_{1}(x),\ldots,h_{T}(x);\alpha_{1},\ldots,\alpha_{T}) },\\
M(x) &= \set{ j \in[ T] : Q_{\adv}^{-}(h_{1}(x),...,h_{T}(x);\alpha_{1},...,\alpha_{T}) \leq\! h_{j}(x) \!\leq Q_{\adv}^{+}(h_{1}(x),...,h_{T}(x);\alpha_{1},...,\alpha_{T}) },\\
R(x) &= \set{ j \in[ T] : h_{j}(x) > Q_{\adv}^{+}(h_{1}(x),\ldots,h_{T}(x);\alpha_{1},\ldots,\alpha_{T}) }.
\end{align*}
}
Then 
the only way
\eqref{eq:Htil-med}
can fail is if half or more indices
$J_{1},\ldots,J_{k}$ sampled fall into $R(x_i)$ --- or if half or more fall into $L(x_i)$.
Since the sampling distribution puts mass less than $1/2-\adv$ 
on each of $R(x_i)$ and $L(x_i)$, 
Chernoff's bound puts an upper estimate of
$\exp(-2k\adv^2)$ on either event.
Hence,
\begin{equation}
  \label{eq:Htil-med-prob}
  \mathbb{P}\paren{|\hat{h}(x_i)-f^*(x_i)|>\dev/2}
  \le2\exp(-2k\adv^2).
\end{equation}
\end{sloppypar}

Next, our goal is to ensure that
with high probability,
\eqref{eq:Htil-med} holds simultaneously for all
${i\in[m]}$.
Define the map $\bxi:[m]\to\mathbb{R}^k$
by $\bxi(i)=(\tilde{h}_{1}(x_i),\ldots,\tilde{h}_{k}(x_i))$.
Let
$G \subseteq[m]$
be a minimal subset of $[m]$ such that 
$$\max\limits_{i \in [m]} \min\limits_{j \in {G}}
\nrm{\bxi(i)-\bxi(j)}_\infty\le\dev/2
.
$$
This is just a minimal $\ell_\infty$ covering of $[m]$.
Then
\begin{align*}
  &\Pr\paren{
    \exists
    i
    \in[m]
    :
    |\Med(\bxi(i))-f^*(x_i)|>\dev
  }\le\\
  &\sum_{j\in G}
  \Pr\paren{
    \exists i
    :
    |\Med(\bxi(i))-f^*(x_i)|>\dev , \nrm{\bxi(i)-\bxi(j)}_\infty\le\dev/2
  }
  \le\\
  &
  \sum_{j\in G}
  \Pr\paren{
    |\Med(\bxi(j))-f^*(x_j)|>\dev /2
  }
  \le 2N_\infty([m],\dev/2)\exp(-2k\adv^2),
\end{align*}
where $N_\infty([m],\dev/2)$ is the $\dev /2$-covering number (under $\ell_\infty$)
of $[m]$,
and we used the fact that
$$|\Med(\bxi(i))-\Med(\bxi(j))|\le\nrm{\bxi(i)-\bxi(j)}_\infty.$$
Finally, to bound $N_\infty([m],\dev/2)$, note that
$\bxi$
embeds
$[m]$
into the dual
class
$\F^*$.
Thus, we may apply
the bound in \cite[Display (1.4)]{MR2247969}:
$$
\log N_\infty([m],\dev/2)\le C 
\dualfat{c\dev}{\F}\log^2(k/\dev),
$$
where $C,c$ are universal constants and $\dualfat{\cdot}{\F}$ 
is the dual fat-shattering dimension of $\F$.
It now only remains to choose a $k$
that makes
$\exp\paren{C \dualfat{c\dev}{\F}\log^2(k/\dev)-2k\adv^2}$
as small as desired.
\end{proof}

To establish Theorem~\ref{thm:regression}, 
we use the weak learner from above, with the booster \algname{MedBoost} from K\'{e}gl, and then apply the \algname{Sparsify} procedure. 
Combining the corresponding theorems, together with the same technique for converting to a compression scheme 
discussed above for classification (i.e. encoding the functions with the set of training examples from which they were obtained, plus a string of bits to denote from which examples, and in what order, each weak hypothesis was obtained), 
this immediately yields the result claimed in Theorem~\ref{thm:regression}, 
which represents our main new result for sample compression of general families of real-valued functions.




\subsection{Examples}
\label{sec:examples}

As an example of the generality and usefulness of the above schemes, we present two interesting and efficient compression schemes that can then be derived. The main technical result needed in order to apply our method to those cases was to find and prove the dual Fat-Shattering dimension of the function-classes at hand, a problem which is not trivial most of the time, requiring using tools from various domains. Leveraging novel and relatively new algorithmic results from learning theory yields the final desired compression-schemes.
    



\subsubsection{Sample compression for BV functions}

\label{sec:BV}
\newcommand{\gammat}{t}
The function class $\mathrm{BV}(v)$ consists of all $f:[0,1]\to\R$ for which
\begin{eqnarray*}
V(f):=\sup_{n\in\N}\sup_{0=x_0<x_1<\ldots<x_n=1}\sum_{i=1}^{n-1}|f(x_{i+1})-f(x_i)| \le v.
\end{eqnarray*}
It is known
\citep[Theorem 11.12]{MR1741038} that 
$\fat{t}{\mathrm{BV}(v)}=1+\floor{v/(2t)}$
.
In Theorem~\ref{thm:bv-dual} below, we show that the dual class has
$\dualfat{t}{\mathrm{BV}(v)}=\Theta\left(\log(v/t)\right)$
.
\citet{DBLP:journals/iandc/Long04} presented an efficient, proper, consistent learner
for the class $\F=\mathrm{BV}(1)$ with range restricted to $[0,1]$,
with sample complexity
$m_\F(\eps,\delta)=O(\frac1\eps\log\frac1\delta)$.
Combined with Theorem~\ref{thm:regression}, this yields
\begin{corollary}
  Let $\F=\mathrm{BV}(1)\cap[0,1]^{[0,1]}$
  be the class
  $f:[0,1] \to [0,1]$ with $V(f)\le1$.
  Then the proper, consistent learner $\calL$
  of \citet{DBLP:journals/iandc/Long04},
  with target generalization error $\eps$,
  admits
  a sample compression scheme of size $O(k\log k)$, where
  \[k = \bigO{ \uf{\eps} \log^2 \uf{\eps} \cdot \log \left(\uf{\eps} \log \uf{\eps}\right)  }.\]
The compression set is computable in expected runtime
  \[
    \bigO{  n \uf{\eps^{3.38}} \log^{3.38} \uf{\eps}
      \left(
        \log n + \log \uf{\eps} \log
        \left(
          \uf{\eps} \log \uf{\eps}
        \right)
        \right) }
    .
  \]
\end{corollary}

The remainder of this section is devoted to proving
\begin{theorem}
  \label{thm:bv-dual}
  For $\F=\mathrm{BV}(v)$
  and $t<v$,
  we have
  $\dualfat{t}{\F}=\Theta\left(\log(v/t)\right)$.
\end{theorem}

First, we define some preliminary notions:

\begin{definition}
  For a binary $m \times n$ matrix $M$, define
  \begin{eqnarray*}
    V(M, i) &:=& \sum_{j=1}^m \ind[M_{j,i} \neq M_{j+1,i}],\\
    G(M) &:=& \sum_{i=1}^n V(M,i),\\
    V(M) &:=& \max_{i \in [n]}V(M,i).
\end{eqnarray*}
\end{definition}

\begin{lemma} \label{lemma:1}
  Let $M$ be a binary $2^n \times n$ matrix.
  If
for each
$b \in \{0,1\}^n$
there is a row $j$ in $M$
equal to $b$,
then
\[V(M) \geq \frac{2^n}{n}.\]
In particular,
for at least one row $i$,
we have
$V(M,i) \geq 2^n/n$.
\end{lemma}

\begin{proof}
  Let $M$ be a $2^n \times n$
  binary
  such that
for each
$b \in \{0,1\}^n$
there is a row $j$ in $M$
equal to $b$.  
Given $M$'s dimensions,
every
$b \in \{0,1\}^n$ appears exactly in one row of $M$,
and hence
the minimal Hamming distance between two rows is 1.
Summing over the $2^n-1$ adjacent row pairs, we have
\[G(M) = \sum_{i=1}^n V(M,i) = \sum_{i=1}^n\sum_{j=1}^m \ind[M_{j,i} \neq M_{j+1,i}] \geq 2^n-1,\]
which averages to
\[\frac{1}{n} \sum_{i=1}^n V(M,i) = \frac{G(M)}{n} \geq \frac{2^n-1}{n} .\]
By
the pigeon-hole principle,
there must be
a row $j \in [n]$ for which
$V(M,i) \geq \frac{2^n-1}{n}$,
which implies
$V(M)  \geq \frac{2^n-1}{n}$.
\end{proof}
We split the proof of Theorem~\ref{thm:bv-dual} into two estimates:

\begin{lemma} \label{lem:bv-ub}
  For $\F=\mathrm{BV}(v)$ and $t<v$,
  $\dualfat{t}{\F}\le2\log_2(v/\gammat)$.
\end{lemma}

\begin{lemma} \label{lem:bv-lb}
  For $\F=\mathrm{BV}(v)$ and $4t<v$,
  $\dualfat{t}{\F}\ge\floor{\log_2(v/\gammat)}$.
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lem:bv-ub}]
  Let $\set{f_1,\ldots,f_n}\subset \F$ be a set of functions that are $\gammat$-shattered by $\F^*$.
  In other words, there is an $r\in\R^n$
  such that for each
  $b \in \{0,1\}^n$
there is an $x_b
\in\F^*$
such that
\[\forall i \in [n] ,
  x_b(f_i)
  \begin{cases}
    \geq r_i + \gammat, & b_i = 1 \\
    \leq r_i - \gammat, & b_i = 0
  \end{cases}
  .
\]

Let us order the $x_b$s by
magnitude
$x_1<x_2<\ldots<x_{2^n}$,
denoting this sequence by
$(x_i)_{i=1}^{2^n}$.
Let $M \in \{0,1\}^{2^n \times n}$ be a matrix whose $i$th row is $b_j$,
the latter ordered arbitrarily.

By Lemma~\ref{lemma:1}, there is $i \in [n]$ s.t. 
\[\sum_{j=1}^{2^n} \ind[M(j,i) \neq M(j+1,i)] \geq \frac{2^n}{n}.\]
Note that if
$M(j,i) \neq M(j+1,i)$ shattering implies that
\[x_j(f_i) \geq r_i + \gammat \text{ and } x_{j+1}(f_i) \leq r_i - \gammat\]
or
\[x_j(f_i) \leq r_i - \gammat \text{ and } x_{j+1}(f_i) \geq r_i + \gammat;\]
either way,
\[\abs*{f_i(x_j) - f_i(x_{j+1})} = \abs*{x_j(f_i) - x_{j+1}(f_i)} \geq 2\gammat.\]
So
for the function
$f_i$,
we have
\begin{eqnarray*}
  \sum_{j=1}^{2^n} \abs*{f_i(x_j) - f_i(x_{j+1})} = \sum_{j=1}^{2^n} \abs*{x_j(f_i) - x_{j+1}(f_i)} 
  \geq \sum_{j=1}^{2^n} \ind[b_{j_i} \neq b_{{j+1}_i} \cdot 2\gammat 
  \geq \frac{2^n}{n} \cdot 2\gammat.
\end{eqnarray*}
As $\{x_j\}_{j=1}^{2^n}$ is a partition of $[0,1]$ we get
\[v \geq \sum_{j=1}^{2^n} \abs*{f_i(x_j) - f_i(x_{j+1})} \geq  \frac{\gammat 2^{n+1}}{n} \geq \gammat 2^{n/2}\]
and hence
\[v/\gammat \geq 2^{n/2}\]
\[\Rightarrow 2\log_2(v/\gammat) \geq n.\]
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lem:bv-lb}]
  We construct a set of $n = \floor{\log_2(v/\gammat)}$ functions that are $t$-shattered by $\F^*$.
  First, we build a balanced Gray code \citep{flahive2007balancing} with $n$ bits,
which we arrange into the rows of $M$.
Divide the unit interval into $2^n
$ segments and define, for each $j\in[2^n]$,
\[
x_j := \frac{j}{2^n}
.
\]
  Define the functions $f_1,\ldots,,f_{\floor{\log_2(v/\gammat)}}$ as follows:
  \[f_i(x_j) =
    \begin{cases}
      \gammat, &
      M(j,i)
      = 1
      \\
      -\gammat, &
      M(j,i)
      = 0
    \end{cases}.
  \]
We claim that each
$
f_i \in \F$.
Since
$M$ is balanced Gray code,
\[V(M) = \frac{2^n}{n} \le \frac{v}{\gammat \log_2(v/\gammat)} \leq \frac{v}{2\gammat}.\]
Hence, for each
$f_i$,
we have
  \[V(f_i) \leq 2 \gammat V(M,i) \leq 2 \gammat \frac{v}{2\gammat} = v.\]
Next, we show
that this set is shattered by $\F^*$.
Fix the trivial offest $r_1=...=r_n = 0$
For every $b \in \{0,1\}^n$ there is a $j \in [2^n]$ s.t. $b = b_i$.
By construction,
for every
$i \in [n]$,
we have
  \[x_j(f_i) = f_i(x_j) =
    \begin{cases}
      \gammat \geq r_i + \gammat, & M(j,i)
              = 1 \\
              -\gammat \leq r_i - \gammat, & M(j,i)
              = 0
    \end{cases}.
  \]
\end{proof}



    



\subsubsection{Sample compression for nearest-neighbor regression}
\label{sec:NN}
Let $(\X,\rho)$ be a metric space
and define, for $L\ge0$, the collection $\F_L$ of all $f:\X\to[0,1]$ satisfying
$$ |f(x)-f(x')|\le L\rho(x,x');$$
these are the $L$-Lipschitz functions.
\citet{GottliebKK17_IEEE} showed that
\begin{eqnarray*}
\fat{t}{\F_L} = O\paren{ \ceil{L\diam{\X}/t}^{\ddim(\X)}},
\end{eqnarray*}
where $\diam{\X}$ is the diameter and $\ddim$ is the {\em doubling dimension} (see Definition~\ref{def:doubling}.
The proof is achieved via a packing argument, which also shows that the estimate is tight.
Below we show that
$  \dualfat{t}{\F} =
\Theta(\log\left({M}(\X,2\gammat/L)\right))$,
where $M(\X,\cdot)$ is the packing number of $(\X,\rho)$.
Applying this to the efficient
nearest-neighbor regressor\footnote{
  In fact, the technical machinery in
  \citet{DBLP:journals/tit/GottliebKK17}
  was aimed at achieving {\em approximate} Lipschitz-extension,
  so as to gain a considerable runtime speedup. An {\em exact} Lipschitz extension
  is much simpler to achieve. It is more computationally costly but still polynomial-time in sample size.
  }
of
\citet{DBLP:journals/tit/GottliebKK17},
we obtain
\begin{corollary}
  Let $(\X,\rho)$ be a metric space with hypothesis class $\F_L$,
  and let $\calL$ be a consistent, proper learner for $\F_L$ with target generalization error $\eps$.
Then $\calL$ admits a compression scheme of size $O(k\log k)$, where
  \[k = \bigO{ D(\eps) \log \uf{\eps} \cdot \log D(\eps)
      \log \left(\uf{\eps} \log D(\eps) \right) }\]
and
\[D(\eps) = \ceil{\frac{L\diam{\X}}{\eps}}^{\ddim(\X)}.\]
\end{corollary}


We now prove our estimate on the dual fat-shattering dimension of $\F$:
\begin{lemma}
  For $\F=\F_L$,
  $\dualfat{t}{\F}
  \leq \log_2\left(\mathcal{M}(\xathcal{X},2\gammat/L)\right)$.
\end{lemma}
\begin{proof}
  Let $\set{f_1,\ldots,f_n}\subset \F_L$ a set that is $\gammat$-shattered by $\F^*_L$.
  For $b \neq b' \in \{0,1\}^n$,
  let $i$ be the first index
  for which $b_i \neq b'_i$,
  say, $b_i = 1\neq0= b'$.
  By shattering,
  there are points $x_b,x_{b'} \in \F^*_L$ such that
  $x_b(f_i) \geq r_i + \gammat$ and
  $x_{b'}(f_i) \leq r_i - \gammat$,
  whence
  \[f_i(x_b) - f_i(x_{b'}) \geq 2\gammat\]
  and
  \[L  \rho(x_b,x_{b'}) \geq  f_i(x_b) - f_i(x_{b'}) \geq 2\gammat.\]
  It follows that for
  $b \neq b' \in \{0,1\}^n$,
we have
$ \rho(x_b,x_{b'}) \geq 2\gammat / L$.
  Denoting by ${M}(\X, \eps)$ the $\eps$-packing number of $\X$,
 we get
  \[2^n = |\{x_b \mid b \in \{0,1\}^n\}| \leq \mathcal{M}(\mathcal{X}, 2\gammat / L). \]
\end{proof}


\begin{lemma}
  For $\F=\F_L$ and $t<L$,
  $\dualfat{t}{\F}  \geq \log_2\left(\mathcal{M}(\mathcal{X},2\gammat/L)\right)$.
\end{lemma}
\begin{proof}
  Let $S=\{x_1,...,x_m\} \subseteq \X$
  be a maximal
  $2\gammat/L$-packing
of $\X$.
Suppose that
$c: S \to \{0,1\}^{\floor{\log_2m}}$
is one-to-one.
Define the set of function $F = \{f_1,\ldots,f_{\floor{\log_2(m)}}\} \subseteq \F_L$ by
  \[
    f_i(x_j) =
    \begin{cases}
      \gammat, & c(x_j)_i = 1 \\
      -\gammat, & c(x_j)_i = 0
    \end{cases}.
  \]
  For every $f \in F$ and every two points $x,x' \in S$ it holds that
  \[\abs{f(x) - f(x')} \leq 2\gammat = L \cdot 2\gammat / L \leq L  \rho(x,x').\]
  This set of functions is $\gammat$-shattered by $S$
  and is of size
  $\floor{\log_2m} = \floor{\log_2\left(\mathcal{M}(\mathcal{X},2\gammat/L)\right)}$.
\end{proof}


    

\chapter{Privately Leading Axis Aligned Rectangles}

We now investigate the problem of privately learning the class of axis-aligned rectangles, defined as follows.

\begin{definition}[Axis Aligned Rectangles]
Let $\domain=\{0,\ldots,X\}^d$ be a finite discrete d-dimensional domain.
Every $p = (p_1,\ldots,p_d) \in \domain$,
induces a classifier $h_p:\domain\to\{0,1\}$ s.t 
for a given input $x \in \domain$ we have 
$$h_p (x) = 
\begin{cases}
1,\quad \forall i\in [d]: x_i \leq p_i\\
0,\quad \text{otherwise}
\end{cases}
$$ 
Define the class of all \emph{axis-aligned and origin-placed rectangles} as $REC^X_d = \{h_p : p\in\domain\}$.
\end{definition}
We focus on the \emph{realizable} setting in which 
for a class $\class$ of potential classifiers,
there exist some $h^* \in REC^X_d$,
s.t $\errD{h^*} = 0$.

Without the privacy requirement, learning axis-aligned rectangles is a simple task. As it is described in classical books such as \citet{Kearns97} and \citet{shalev-shwartz_understanding_2014} we can consider the simple algorithm which removes all the negative labeled points and picks the tightest rectangle containing the remaining points. 
It can also be seen from a compression scheme perspective, by applying the following simple algorithm:
\begin{enumerate}[leftmargin=15pt]
    \item For every axis $i\in [d]$
    \begin{enumerate}
        \item Remove all the negative-labeled points.
        \item Project all the remaining points onto the $i^{th}$ axis.
        \item Pick a point $a_i$ from the lowest valued projected point.
        \item Pick a point $b_i$ from the highest valued projected point.
    \end{enumerate}
    \item Return the axis-aligned rectangle defined by the intervals $[a_i,b_i]$ at the different axes.
\end{enumerate}
This is a valid compression scheme which yields a simple and efficient PAC-learner for the class.

As for privately learning, this idea is no longer valid due to its high sensitivity. Moreover, as mentioned in \ref{sec:intro-aar}, it is not a matter of any specific algorithm or technique, but under privacy constraints this learning task is inherently harder, and the sample complexity must depend on the domain size at least by a $\log^*$ factor.

\section{Baseline}
\label{sec:litrature-plaar}

The best prior result for the problem, which we use as our baseline, obtains sample complexity $\tilde{O}\big( d^{1.5} \cdot \left( \log^*|\domain|\right)^{1.5} \big)$.
This baseline algorithm is based on a reduction to (privately) solving the following problem, called the {\em interior point problem}.

\begin{definition}[\citealt{BNSV15}]
An algorithm $\alg$ is said to solve the 
\emph{Interior Point Problem} for domain $\domain$
with failure probability $\beta$ and sample complexity $n$,
if for every $m\geq n$ and every database $S$ containing $m$ elements from $\domain$ it holds that:
$
\Pr[\min(S) \leq \alg(S) \leq \max(S)] \geq 1-\beta.
$
\end{definition}
That is, given a database $S$ containing (unlabeled) elements from a (one dimensional) grid $\domain$, the interior point problem asks for an element of $\domain$ between the smallest and largest elements in $S$. 
The baseline we consider for privately learning axis-aligned rectangles is as follows: Suppose we have a differentially private algorithm $\IPalg$ for the interior point problem over domain $\domain$ with sample complexity $n$ (let us ignore the failure probability for simplicity). We now use $\IPalg$ to construct the following algorithm $\alg$ that takes a database $S$ containing labeled elements from $\domain^d$. For simplicity, we assume that $S$ contains ``enough'' positive elements, as otherwise we could simply return the all-zero hypothesis.

\begin{enumerate}[leftmargin=15pt]
    \item For every axis $i\in[d]$:
        \begin{enumerate}
        \item Project the positive points in $S$ onto the $i$th axis.
        \item Let $A_i$ and $B_i$ denote the smallest $n$ and the largest $n$ (projected) points, without their labels.
        \item Let $a_i\leftarrow\IPalg(A_i)$ and $b_i\leftarrow\IPalg(B_i)$.
    \end{enumerate}
    \item\label{step:Bend} Return the axis-aligned rectangle defined by the intervals $[a_i,b_i]$ at the different axes.
\end{enumerate}

Now, recall that each application of algorithm $\IPalg$ returns an {\em interior point} of its input points. Hence, for every axis $i$, it holds that the interval $[a_i,b_i]$ contains (the projection) of all, but at most $2n$, of the positive examples in the $i$th axis. Therefore, the rectangle returned in Step~\ref{step:Bend} contains all, but at most $2nd$, of the positive points (and it does not contain any of the negative points, because this rectangle is {\em contained} inside the target rectangle). So algorithm $\alg$ errs on at most $2nd$ of its input points. 

Assuming that $|S|\gg 2nd$, we therefore get that algorithm $\alg$ has small empirical error. As the VC dimension of the class of axis-aligned rectangles is $O(d)$, this means that algorithm $\alg$ is a PAC learner for this class with sample complexity $O(nd)$. The issue here is that algorithm $\alg$ executes algorithm $\IPalg$ many times (specifically, $2d$ times). Hence, in order to argue that $\alg$ is $(\eps,\delta)$-differentially private, standard composition theorems for differential privacy require each execution of algorithm $\IPalg$ to be done with a privacy parameter of $\approx\eps/\sqrt{2d}$. This, in turn, would mean that $n$ (the sample complexity of algorithm $\IPalg$) needs to be at least $\sqrt{2d}$, which means that algorithm $\alg$ errs on $2nd\approx d^{1.5}$ input points, which translates to sample complexity of $|S|\gg d^{1.5}$.

The takeaway from this baseline learner is that in order to reduce the sample complexity to be linear in $d$, we want to bypass the costs incurred from composition. That is, we still want to follow the same strategy (apply algorithm $\IPalg$ twice on every axis), but we want to do it without appealing to composition arguments in the privacy analysis. This was the starting point of our thought process.

\section{The Algorithm}
\label{sec:algorithm}

We now briefly survey two intuitive attempts that fail to achieve this, but are useful for the presentation of our final algorithm.

\paragraph{Failed Attempt \#1.}
As before, let $\IPalg$ denote an algorithm for the interior point problem over domain $\domain$ with sample complexity $n$. Consider the following modification to algorithm $\alg$ (marked in red). As before, algorithm $\alg$ takes a database $S$ containing labeled elements from $\domain^d$, where we assume for simplicity that $S$ contains "enough" positive elements.
\begin{enumerate}[leftmargin=15pt]
    \item For every axis $i\in[d]$:
        \begin{enumerate}
        \item Project the positive points in $S$ onto the $i$th axis.
        \item Let $A_i$ and $B_i$ denote the smallest $n$ and the largest $n$ (projected) points, without their labels.
        \item Let $a_i\leftarrow\IPalg(A_i)$ and $b_i\leftarrow\IPalg(B_i)$.
        \item[\textcolor{red}{(d)}] \textcolor{red}{Delete from $S$ all points (with their labels) that correspond to $A_i$ and $B_i$.}
    \end{enumerate}
    \item Return the axis-aligned rectangle defined by the intervals $[a_i,b_i]$ at the different axes.
\end{enumerate}

The (incorrect) idea here is that by adding Step~1d we make sure that each datapoint from $S$ is "used only once", and hence we do not need to pay in composition. In other words, the hope is that if every execution of algorithm $\IPalg$ is done with a privacy parameter $\eps$, then the whole construction would satisfy differential privacy with parameter $O(\eps)$.

The failure point of this idea is that by deleting {\em one} point from the data, we can create a "domino effect" that affects (one by one) many of the sets $A_i,B_i$ throughout the execution. 
Specifically, consider two neighboring datasets $S$ and $S'=S\cup\{(x',y')\}$ for some labeled point $(x',y')\in X^d\times\{0,1\}$. Suppose that during the execution on $S'$ it holds that $x'\in A_1$. So the additional point $x'$ participates "only" in the first iteration of the algorithm, and gets deleted afterwards. However, since the size of the sets $A_i,B_i$ is fixed, during the execution on $S$ (without the point $x'$) it holds that {\em a different point $z$} gets included in $A_1$ instead of $x'$, and this point $z$ is then deleted from $S$ (but it is not deleted from $S'$ during the execution on $S'$). Therefore, also during the second iteration we have that $S$ and $S'$ are not identical (they still differ on one point) and this domino effect can continue throughout the execution. That is, a single data point can affect many of the executions of $\IPalg$, and we would still need to pay in composition to argue privacy.

\paragraph{Failed Attempt \#2.}
In order to overcome the previous issue, one might try the following variant of algorithm $\alg$.

\begin{enumerate}[leftmargin=15pt]
    \item For every axis $i\in[d]$:
        \begin{enumerate}
        \item Project the positive points in $S$ onto the $i$th axis.
        \item[\textcolor{red}{(b)}] \textcolor{red}{Let ${\rm size}_{A_i}=2n+{\rm Noise}$ and let ${\rm size}_{B_i}=2n+{\rm Noise}$.}
        \item[(c)] Let $A_i$ and $B_i$ denote the smallest \textcolor{red}{${\rm size}_{A_i}$} and the largest \textcolor{red}{${\rm size}_{B_i}$} (projected) points, respectively, without their labels.
        \item[(d)] Let $a_i\leftarrow\IPalg(A_i)$ and $b_i\leftarrow\IPalg(B_i)$.
        \item[(e)] Delete from $S$ all points (with their labels) that correspond to $A_i$ and $B_i$.
    \end{enumerate}
    \item Return the axis-aligned rectangle defined by the intersection of the intervals $[a_i,b_i]$ at the different axes.
\end{enumerate}

The idea now is that the noises we add to the sizes of the $A_i$'s and the $B_i$'s would "mask" the domino effect mentioned above. Specifically, the hope is as follows: Consider the execution of (the modified) algorithm $\alg$ on $S$ and on $S'=S\cup\{(x',y')\}$, and let $i$ be the first axis such that $x'\in A_i\cup B_i$ during the execution on $S'$. Suppose w.l.o.g.\ that $x'\in B_i$. Now, the hope is that if during the execution on $S$ we have that the noisy ${\rm size}_{B_i}$ is smaller by 1 than its value during the execution on $S'$, then this eliminates the domino effect we mentioned, because we would not need to add another point instead of $x'$.  Specifically, during time $i$, the point $x'$ gets deleted from $S'$, and every other point is either deleted from both $S,S'$ or not deleted from any of them. So after time $i$ the two executions continue identically. Thus, the hope is that by correctly "synchronizing" the noises between the two executions (such that only the size of the "correct" set gets modified by 1) we can make sure that only one application of $\IPalg$ is affected (in the last example -- only the execution of $\IPalg(B_i)$ is affected), and so we would not need to apply composition arguments.

Although very convincing, this idea fails.
The (very subtle) issue here is that it is not clear how to synchronize the noises between the two executions. To see the problem, let us try to formalize the above argument.

Fix two neighboring databases $S$ and $S'=S\cup\{(x',y')\}$. Let us write $A_i,B_i$ and $A'_i,B'_i$ to denote these sets during the executions on $S$ and on $S'$, respectively.
Aiming to synchronize the two executions, let us define a mapping $\pi:\R^{2d}\rightarrow\R^{2d}$ from noise vectors during the execution on $S'$ to noise vectors during the execution on $S$ (determining the values of ${\rm size}_{A_1},{\rm size}_{B_1},\dots,{\rm size}_{A_d},{\rm size}_{B_d}$), such that throughout the execution we have that $A_i=A'_i$ and $B_i=B'_i$ for all $i$ except for a single pair, say $B_j\neq B'_j$, of neighboring sets.

The straightforward way for defining such a mapping is as follows: Let $j$ be the first time step in which the additional point $x'$ gets included in a set $A'_j$ or $B'_j$, and say that it is included in $B'_j$. Then the mapping would be to reduce (by 1) the value of ${\rm size}_{B_j}$ (the noisy size of $B_j$ during the execution on $S$). This would indeed make sure that, conditioned on the noise vectors $v'$ and $v=\pi(v')$, the two executions differ only in a single application of the interior point algorithm $\IPalg$, and hence the outcome distribution of these two (conditioned) executions is very similar (in the sense of differential privacy). That is, for any noise vector $v$ and any event $F$,
$$
\Pr[\alg(S')\in F | v] \leq e^{\eps}\cdot \Pr[\alg(S)\in F | \pi(v)] +\delta.
$$

Furthermore, (assuming an appropriate noise distribution) we can make sure that the probability of obtaining the noise vectors $v$ and $\pi(v)$ is similar, with densities differing by at most an $e^\eps$ factor (as is standard in the literature of differential privacy). Therefore, {\em had the mapping $\pi$ we defined was a bijection}, for any event $F$ we would have that

\begin{align*}
    \Pr[\alg(S')\in F] &= \sum_{v} \Pr[v]\cdot\Pr[\alg(S')\in F | v]\\
    &\leq \sum_{v} e^{\eps}\cdot\Pr[\pi(v)]\cdot\left(e^{\eps}\cdot\Pr[\alg(S)\in F | \pi(v)]+\delta\right)\\
    &= \sum_{\pi(v)} e^{\eps}\cdot\Pr[\pi(v)]\cdot\left(e^{\eps}\cdot\Pr[\alg(S)\in F | \pi(v)]+\delta\right)\\
    &=e^{2\eps}\cdot\Pr[\alg(S)\in F]+e^{\eps}\cdot\delta,
\end{align*}
which would be great. Unfortunately, the mapping $\pi$ we defined is {\em not} a bijection, and hence the second-to-last equality above is incorrect. To see that it is not a bijection, suppose that $d=2$ and consider a database $S$ containing the following positively labeled points: Many copies of the point $(0,0)$, as well as 10 copies of the point $(1,0)$ and 10 copies of the point $(0,1)$. The neighboring database $S'$ contains, in addition to all these points, also the point $\left(\frac{1}{2},\frac{1}{2} \right)$. Now suppose that during the execution on $S'$ we have that $|B'_1|=5$ and $|B'_2|=4$. That is, the additional point is included in $B'_1$. During the execution on $S$ we therefore reduce (by 1) the size of $B_1$ and so $|B_1|=|B_2|=4$. Now suppose that during the execution on $S'$ we have that $|B'_1|=4$ and $|B'_2|=5$. Here, during the execution on $S$ we reduce the size of $B_2$ and so, again,  $|B_1|=|B_2|=4$. This shows that the mapping $\pi$ we defined is {\em not} a bijection. In general, in $d$ dimensions, it is only a $d$-to-$1$ mapping, which would would break our analysis completely (it will not allow us to avoid the extra factor in $d$).

\subsection{Our Solution - A Technical Overview}

We now present a simplified version of our construction that overcomes the challenges mentioned above. We stress that the actual construction is somewhat different. Consider the following (simplified) algorithm.


\begin{enumerate}[leftmargin=15pt]
    \item For every axis $i\in[d]$:
        \begin{enumerate}
        \item Project the positive points in $S$ onto the $i$th axis.
        \item Let ${\rm size}_{A_i}=100n+{\rm Noise}$ and let ${\rm size}_{B_i}=100n+{\rm Noise}$, where the standard deviation of these noises is, say, $10n$.
        \item Let $A_i$ and $B_i$ denote the smallest ${\rm size}_{A_i}$ and the largest ${\rm size}_{B_i}$ (projected) points, respectively, without their labels.
        \item Let $A_i^{\rm inner}\subseteq A_i$ be the $n$ {\em largest} points in $A_i$. Similarly, let $B_i^{\rm inner}\subseteq B_i$ be the $n$ {\em smallest points in $B_i$}.
        \item Let $a_i\leftarrow\IPalg(A_i^{\rm inner})$ and $b_i\leftarrow\IPalg(B_i^{\rm inner})$.
        \item Delete from $S$ all points (with their labels) whose projection onto the $i$th is {\em not} in the interval $[a_i,b_i]$.
    \end{enumerate}
    \item Return the axis-aligned rectangle defined by the intersection of the intervals $[a_i,b_i]$ at the different axes.
\end{enumerate}

There are {\em two} important modifications here. First, we still add noise to the size of the sets $A_i,B_i$, but we only use the $n$ "inner" points from these sets. Second, we delete elements from $S$ not based on them being inside $A_i$ or $B_i$, but only based on the (privately computed) interval $[a_i,b_i]$. 
We now elaborate on these ideas, and present a (simplified) overview for the privacy analysis. Any informalities made herein are removed in the sections that follow.

Let $S$ and $S'=S\cup\{(x',y')\}$ be neighboring databases, differing on the labeled point $(x',y')$. Consider the execution on $S$ and on $S'$. The privacy analysis is based on the following two lemmas.

\begin{lemma}[informal]
With probability at least $1-\delta$, throughout the execution it holds that $x'$ participates in at most $O(\log(1/\delta))$ sets $A_i,B_i$.
\end{lemma}

This lemma holds because of our choice for the noise magnitude. In more detail, given that $x'\in A_i$, there is a constant probability that $x'\in A_i\setminus A_i^{\rm inner}$. Since the interior point $a_i$ is computed from $A_i^{\rm inner}$, in such a case we will have that $x'<a_i$, and hence, $x'$ is deleted from the data during this iteration. This means that every time $x'$ is included in $A_i$, there is a constant probability that $x'$ will be deleted from the data. Thus, one can show (using concentration bounds) that the number of times $i$ such that $x'\in A_i$ is bounded (w.h.p.). A similar argument also holds for $B_i$.

\begin{lemma}[informal]
In iterations $i$ in which $x'$ is {\em not} included in $A_i$ or $B_i$, we have that $a_i$ and $b_i$ are distributed {\em exactly} the same during the execution on $S$ and on $S'$.
\end{lemma}

Indeed, in such an iteration, the point $x'$ has no effect on the outcome distribution of $\IPalg$ (who computes $a_i,b_i$). 
Overall, w.h.p.,\ there are at most $O(\log\frac{1}{\delta})$ axes the point $x'$ effects. We pay in composition only for those axes, while in all other axes we get privacy "for free". This allows us to save a factor of $\sqrt{d}$ in the sample complexity, and obtain an algorithm with sample complexity linear in $d$.


Note that the definition of privacy we work with is that of $(\varepsilon,\delta)$-differential privacy.
In contrast to the case of $(\varepsilon,0)$-differential privacy,
where it suffices to analyze the privacy loss w.r.t.\ every {\em single} possible outcome,
with $(\varepsilon,\delta)$-differential privacy we must account for arbitrary events. 
To tackle this, we had to perform a more explicit and meticulous analysis than that outlined above.
Our analysis draws its structure from the proof of the advanced-composition theorem \citep{dwork_boosting_2010},
but instead of composing everything we aim to preform \emph{effective composition},
meaning that we incur a privacy loss only on a small fraction of the iterations.
To achieve this, as we mentioned, we partition the iterations into several types -- iteration on which we "pay" in privacy and iterations on which we do not. However, this partition must be done carefully, as the partition itself is random and needs to be different for different possible outcomes.


We believe that ideas from our work can be used more broadly, and hope that they find new applications in avoiding (or reducing) composition costs in other settings.


\begin{myremark}{\em
To simplify the presentation, in the technical sections of this paper we assume that the target rectangle is placed at the origin. Our results easily extend to arbitrary axis-aligned rectangles.
}\end{myremark}


\subsection{Formal Construction}

Let $\IP$ be an $(\eps, \delta)$-differentially private algorithm for solving the interior point problem over domain $\{0,\ldots,X\}$ with failure probability $\beta$ and sample complexity $\ASample{\IP}{\eps,\delta,\beta}$. 
We propose Algorithm~\ref{alg:main}, which we call \algname{RandMargins}, and prove the following theorem.

\begin{algorithm}[ht]
   \caption{\algname{RandMargins}}
   \label{alg:main}
\begin{algorithmic}
   \State {\bfseries Input:} Data $S\subseteq \mathbb{R}^d$ of size $n$, and parameters $\beta < \frac{1}{4}$ and $\delta < 1/e^2,\eps$ %
   \State {\bfseries Tool used:} An $(\eps, \delta)$-private algorithm $\IP$ for solving the interior point problem with failure probability $\beta$ and sample complexity  $\ASample{\IP}{\eps,\delta,\beta}$.\vspace{5pt}
   \State Denote $\Delta = \ASample{\IP}{\eps,\delta,\beta}$
   \State Denote $\mu = 4 \Delta \log(1/\beta)$
    \State Initialize $\bar{S} \leftarrow S$
   \For{$i=1$ {\bfseries to} $d$}
   \State $w_i \sim Lap(\var)$
   \State $B_i = \max_i(\bar{S},\lceil \mu + w_i \rceil)$
   \State $\inner{i} = \min_i(B_i, \Delta)$
   \State $p_i \leftarrow \IP\left(\inner{i},\varepsilon,\delta,\beta\right)$
   \State $R_i = \{y\in \bar{S} : y[i] \geq p_i\}$
   \State $\bar{S} \leftarrow \bar{S}\setminus R_i$
   \EndFor
   \State \textbf{Return} $(p_1,\ldots,p_d)$
\end{algorithmic}
\end{algorithm}

\begin{theorem}\label{thm:main}
Let $\varepsilon < 1 , \delta < \frac{1}{e^2}, \alpha, \beta$. 
Algorithm~\ref{alg:main} is 
$(\alpha,\beta,\bigeps,\bigdelta)$-PPAC learner, for the $REC_d$ class,
given a labeled sample of size
$\bigO{ \ASample{\IP}{\eps,\delta,\beta}\cdot  \frac{d}{\alpha}\log\left(\frac{1}{\alpha}\right)
\log\left(\frac{1}{\beta}\right)}$, 

for
$
\bigdelta = (d+2)\delta,$
and 
$
\bigeps = \bigO{\varepsilon\log(1/\delta)}.
$
\end{theorem}

\begin{myremark}{\em
\citet{KaplanLMNS20} introduced an algorithm $\IP$ for the interior point problem with sample complexity 
$
\ASample{\IP}{\eps,\delta,\beta}=
\tildeO{\frac{1}{\eps} \log^{1.5}\left(\frac{1}{\delta}\right)\left(\logstar{|\domain|}\right)^{1.5}}.
$
Hence, using their algorithm within Algorithm~\ref{alg:main} provides the result of Theorem~\ref{thm:informal}.
}\end{myremark}

We analyze the privacy guarantees of Algorithm~\ref{alg:main} in Section~\ref{sec:privacy-analysis}, and show the following lemma.

\begin{lemma}
\label{lem:privacy}
For every $\varepsilon$ and every $\delta < \frac{1}{e^2}$.
Then, given a labeled sample of size
$\bigO{ \ASample{\IP}{\eps,\delta,\beta}\cdot   \frac{d}{\alpha}\log\left(\frac{1}{\alpha}\right)
  \log\left(\frac{1}{\beta}\right)}$,
Algorithm~\ref{alg:main} is $(\bigeps,\bigdelta)$-differentially private,
for
$
\bigdelta = (d+2)\delta,$
and
$\bigeps = \bigO{\varepsilon\log(1/\delta)}.
$
\end{lemma}

We analyze the utility guarantees of Algorithm~\ref{alg:main} in Section~\ref{sec:accuracy},
and show the following lemma.

\begin{lemma}
\label{lem:accuracy}
For any choice of $\alpha, \beta, \eps, \delta$,
given a labeled sample of size

$\mathcal{O}\Big( \ASample{\IP}{\eps,\delta,\beta}\cdot  
\frac{d}{\alpha}\log\left(\frac{1}{\alpha}\right)
  \log\left(\frac{1}{\beta}\right)\Big)$

then, with probability at least $1-\beta$
Algorithm~\ref{alg:main} is $\alpha$-accurate.
\end{lemma}

\section{Privacy Analysis}
\label{sec:privacy-analysis}

\begin{proof}[Proof of Lemma~\ref{lem:privacy}]

  Let $S$ and $S'=S\cup\{(x',y')\}$ be neighboring databases, differing on the labeled point $(x',y')$. Consider the execution on $S$ and on $S'$.
  
We denote by $\num{i}{x}$ the position of the point $x$ in the remaining data $\bar{S}$,
when the data is sorted by the $i^{th}$ coordinate. 

Denote by $i^*$ the first iteration on which
$x'[i] > p_i$, note that $i^*$ is a random variable.
For an input set $S$, denote by $\bar{S}_i$
the remaining set at the beginning of the $i^{th}$ iteration
and 
its size by $\bar{n}$.

Partition the iterations in the following way 
\begin{itemize}[itemjoin={,\quad}]
\item $\event{in} = \{i \leq i^*\mid x' \in B'_i\}$
\item $\event{out} = \{i < i^* \mid x' \notin B'_i\}$  
\item $\event{after} = \{i \mid i > i^*\}$
\end{itemize}



  

We first argue that $|\event{in}|$ is small (with high probability). Intuitively, this follows from the fact that conditioned on $x'\in B'_i$, with constant probability, we get that $x'\in B'_i\setminus D_i$. Note that in such a case, projecting on the $i^{th}$ axis, $x'$ is bigger (or equal) than any point in $D_i$. Furthermore, as the interior point $p_i$ is computed from $D_i$, w.h.p.\ we get that $x'[i]\geq p_i$, and hence $x'$ is removed from the data. To summarize, conditioned on $x'\in B'_i$ there is a constant probability that $x'$ is removed from the data, and hence the number of times such that $x'\in B'_i$ must be small (w.h.p.). We make this argument formal in the appendix, obtaining the following claim.

\begin{claim}\label{claim:concentrate}
\[
\Pr[|\event{in}| > 35 \log(1/\delta)]\leq \delta.
\]
\end{claim}



Next,
we will denote by $\blackbox$ the inner steps of the loop in the algorithm.
Meaning, the input is $\bar{S}_i$,
which $\blackbox$ uses, along with the random noise and the mechanism $\IP$,
in order to output $p_i$.
Note that $\blackbox$ can be seen as a stand-alone $(\varepsilon, \delta)$-differentially private algorithm (essentially amounts to a single execution of algorithm $\IP$). 
For convenience, we will assume that the $\blackbox$'s output includes the noise value $w_i$, and that the final output of \algname{RandMargins} includes the noise vector $w=(w_1,\ldots,w_d)$. As will be proven below, algorithm \algname{RandMargins} remains differentially private even when releasing this noise vector (in addition to the output $(p_1,\dots,p_d)$).

\begin{lemma}[\cite{lindell_complexity_2017}]
  \label{lem:eps-close-events}
  For every $(\varepsilon, \delta)$-private  algorithm $M$ and every two neighboring datasets $S,S'$, there exists an event $G=G(M,S,S')$ such that 
  \begin{enumerate}[label=\roman*),itemjoin={,\quad}]
  \item $\Pr[M(S)\in G] > 1-\delta$
  \item  $\Pr[M(S')\in G] > 1-\delta$
  \item   $\forall x\in G :
    \left|\ln\left(\frac{\Pr(M(S)=x)}{\Pr(M(S')=x)}\right)\right| \leq \varepsilon$.
  \end{enumerate}
\end{lemma}
Define the event 
$
  G = \{(p,w)
  \mid \forall j\in [d]: (p_j,w_j) \in G(\blackbox,\bar{S}_j,\bar{S}_j')\}
$, 
where $G(\blackbox,\bar{S}_j,\bar{S}_j')$ is the event guaranteed to exist
by applying Lemma~\ref{lem:eps-close-events} to $\blackbox,\bar{S}_j,\bar{S}_j'$.


Note that by Lemma~\ref{lem:eps-close-events} and the union bound
$\Pr[G] \geq 1 - d\delta$.

We wish to prove that for any possible output set $P$, it holds that
$$\Pr[\algname{RandMargins}(S)\in P] \leq e^{\bigeps}\cdot\Pr[\algname{RandMargins}(S')\in P] + \bigdelta.$$ 
Define the set
\[
\epsset = \set{(p,w) \left|     
    \ln\left(
    \frac{\Pr[\RandMargin(S)=(p,w)]}{\Pr[\RandMargin(S')=(p,w)]}\right) 
    > \bigeps \right.},
\]
    where $\RandMargin$ is an abbreviation for \algname{RandMargins}.
    
Now note that for every event $P$,
\begin{align*}
    \Pr&[\RandMargin(S) \in P] \\
    &\leq \Pr[\RandMargin(S) \in \epsset] + \Pr[\RandMargin(S) \in P\setminus \epsset]\\ 
    &\leq \Pr[\RandMargin(S) \in \epsset] + e^{\bigeps} \Pr[\RandMargin(S') \in P\setminus \epsset] \\
    &\leq \Pr[\RandMargin(S) \in \epsset| + e^{\bigeps} \Pr[\RandMargin(S') \in P]
    \end{align*}
    
So it is down to show that 
$
\Pr[\RandMargin(S) \in \epsset] \leq \bigdelta.
$ 
That is, we need to prove that
$$
  \Pr_{p,w \leftarrow \RandMargin(S)}\left[\ln\left(\frac{\Pr(\RandMargin(S)=p,w)}{\Pr(\RandMargin(S')=p,w)}\right)
    > \bigeps \right]\leq \bigdelta.
$$ 
We calculate,




\begin{align*}
    \Pr&_{p,w \leftarrow \RandMargin(S)}\left[\ln\left(\frac{\Pr(\RandMargin(S)=p,w)}{\Pr(\RandMargin(S')=p,w)}\right)
       > \bigeps  \right] \\
   =&
     \Pr_{p,w \leftarrow \RandMargin(S)}\Biggr[ \left(
     \ln\left(\frac{\Pr(\RandMargin(S)=p,w)}{\Pr(\RandMargin(S')=p,w)}\right)
     \cdot
     \indicator{p,w \in G}
     > \bigeps \right)
     \text{ OR } \\
     &\qquad \left(
     \ln\left(\frac{\Pr(\RandMargin(S)=p,w)}{\Pr(\RandMargin(S')=p,w)}\right)
     \cdot
     \indicator{p,w \not\in G}
     > \bigeps \right) \Biggr]\\
   \leq& 
     \Pr_{p,w \leftarrow \RandMargin(S)}\left[\ln\left(\frac{\Pr(\RandMargin(S)=p,w)}{\Pr(\RandMargin(S')=p,w)}\right)
     \cdot \indicator{p,w \in G}
     > \bigeps \right] \\
     &+
     \Pr_{p,w \leftarrow \RandMargin(S)}\left[\ln\left(\frac{\Pr(\RandMargin(S)=p,w)}{\Pr(\RandMargin(S')=p,w)}\right)
     \cdot \indicator{p,w \not\in G}
     > \bigeps \right] \\  
   \leq& 
     \Pr_{p,w \leftarrow \RandMargin(S)}\left[\ln\left(\frac{\Pr(\RandMargin(S)=p,w)}{\Pr(\RandMargin(S')=p,w)}\right)
     \cdot \indicator{p,w \in G}
     > \bigeps \right]
     +
     (1 - \Pr[G]) \\
   \leq&
     \Pr_{p,w \leftarrow \RandMargin(S)}\left[\ln\left(\frac{\Pr(\RandMargin(S)=p,w)}{\Pr(\RandMargin(S')=p,w)}\right)
     \cdot \indicator{p,w \in G}
     > \bigeps \right]
     +
     d\delta.
\end{align*}

It remains to prove that
$\Pr_{p,w \leftarrow \RandMargin(S)}
\left[\ln\left(\frac{\Pr(\RandMargin(S)=p,w)}{\Pr(\RandMargin(S')=p,w)}\right)
  \cdot \indicator{p,w \in G}
  > \bigeps \right] \leq 2\delta.$ 
  
  We calculate,



\begin{align*}
  \Pr&_{p,w \leftarrow \RandMargin(S)}\left[\ln\left(\frac{\Pr(\RandMargin(S)=p,w)}{\Pr(\RandMargin(S')=p,w)}\right)
       \cdot \indicator{p \in G}
       > \bigeps \right] \\
  =&
     \Pr_{p,w \leftarrow \RandMargin(S)}\left[\ln\left(
     \prod_{i=1}^{d}
     \frac{\Pr(\RandMargin(S)_i=p_i,w_i
     \mid p_{<i}, w_{<i})}
     {\Pr(\RandMargin(S')_i=p_i,w_i
     \mid p_{<i}, w_{<i})}\right)
     \cdot \indicator{p,w \in G}
     > \bigeps \right] \\
  =&
     \Pr_{p,w \leftarrow \RandMargin(S)}\left[\sum_{i=1}^{d}\ln
     \left(\frac{\Pr(\RandMargin(S)_i=p_i,w_i\mid p_{<i}, w_{<i})}
     {\Pr(\RandMargin(S')_i=p_i,w_i\mid =p_{<i}, w_{<i})}\right)
     \cdot \indicator{p,w\in G}
     > \bigeps  \right]  \\
  \leq&
        \Pr_{p,w \leftarrow \RandMargin(S)}\left[\sum_{i=1}^{d}\left(\ln
        \left(
        \frac{\Pr(\RandMargin(S)_i=p_i,w_i\mid p_{<i}, w_{<i})}
        {\Pr(\RandMargin(S')_i=p_i,w_i\mid p_{<i}, w_{<i})}        
        \right)
        \cdot \indicator{p_i,w_i \in G_{i}(\blackbox,\bar{S}_i,\bar{S}_i')}\right)    
        > \bigeps \right]
  \\
  =&
     \Pr_{p,w \leftarrow \RandMargin(S)}\Bigg[
     \sum_{i\in \event{in}}\left(\ln
     \left(
     \frac{\Pr(\RandMargin(S)_i=p_i,w_i\mid p_{<i}, w_{<i})}
     {\Pr(\RandMargin(S')_i=p_i,w_i\mid p_{<i}, w_{<i})}
     \right)
     \cdot \indicator{p_i,w_i \in G_{i}(\blackbox,\bar{S}_i,\bar{S}_i')}\right)
  \\
     &+
       \sum_{i\in \event{out}}\left(\ln
       \left(
       \frac{\Pr(\RandMargin(S)_i=p_i,w_i\mid p_{<i}, w_{<i})}
       {\Pr(\RandMargin(S')_i=p_i,w_i\mid p_{<i}, w_{<i})}
       \right)
       \cdot \indicator{p_i,w_i \in G_{i}(\blackbox,\bar{S}_i,\bar{S}_i')}\right)
  \\
     &+
       \sum_{i\in \event{after}}\left(\ln
       \left(
       \frac{\Pr(\RandMargin(S)_i=p_i,w_i\mid p_{<i}, w_{<i})}
       {\Pr(\RandMargin(S')_i=p_i,w_i\mid p_{<i}, w_{<i})}
       \right)
       \cdot \indicator{p_i,w_i \in G_{i}(\blackbox,\bar{S}_i,\bar{S}_i')}\right)
       > \bigeps \Bigg].  
       \footnotemark 
       \numberthis \label{eq:final}
\end{align*}
\footnotetext{Note that the outer probability is over $p$ and $w$. This allows the partition of the iterations into $\event{in},\event{out},\event{after}$ to be well-defined, as this partition depends on $p,w$.}



We will prove the following
\begin{enumerate}[label=(\roman*)]
\item $\Pr\left[\sum_{i\in \event{after}}\ln
    \left(
      \frac{\Pr(\RandMargin(S)_i=p_i,w_i\mid p_{<i},w_{<i})}
      {\Pr(\RandMargin(S')_i=p_i,w_i\mid p_{<i},w_{<i})}
    \right)
    \cdot \indicator{p_i,w_i \in G_{i}(\blackbox,\bar{S}_i,\bar{S}_i')}
    = 0 \right] = 1$
\item $ \Pr\left[\sum_{i\in \event{out}}\ln
    \left(
      \frac{\Pr(\RandMargin(S)_i=p_i,w_i\mid p_{<i},w_{<i})}
      {\Pr(\RandMargin(S')_i=p_i,w_i\mid p_{<i},w_{<i})}
    \right)
    \cdot \indicator{p_i,w_i \in G_{i}(\blackbox,\bar{S}_i,\bar{S}_i')}
    = 0 \right] = 1$
 \item $ \Pr\left[\sum_{i\in \event{in}}\ln
     \left(
       \frac{\Pr(\RandMargin(S)_i=p_i,w_i\mid p_{<i},w_{<i})}
       {\Pr(\RandMargin(S')_i=p_i,w_i\mid p_{<i},w_{<i})}
     \right) 
     \cdot \indicator{p_i,w_i \in G_{i}(\blackbox,\bar{S}_i,\bar{S}_i')}
     \leq \bigeps \right] \geq 1-\delta$
\end{enumerate}

Combining the above three claims implies a bound on \eqref{eq:final} and finishes the proof.

\begin{innerproof}[Proof of (i)]
  After $i^*$,
  by the algorithm definition,
  $x'$ gets removed from $S'$.
  Hence, for every $i > i^*$,
  conditioning on $\RandMargin(S)_{<i}=p_{<i}$,
  it holds that $B'_i = B_i$.
  This implies that, for every 
  $i \in \event{after}$,
  
  \[
    \Pr(\RandMargin(S)_i=p_i,w_i\mid p_{<i},w_{<i})
    =
    \Pr(\RandMargin(S')_i=p_i,w_i\mid p_{<i},w_{<i})
  \]
  
  which yields
  
  \begin{align*}
    &\frac{\Pr(\RandMargin(S)_i=p_i,w_i\mid p_{<i},w_{<i})}
      {\Pr(\RandMargin(S')_i=p_i,w_i\mid p_{<i},w_{<i})} = 1 \\
    &\Rightarrow
      \Pr\left[\sum_{i\in \event{after}}\ln
      \left(
      \frac{\Pr(\RandMargin(S)_i=p_i,w_i\mid p_{<i},w_{<i})}
      {\Pr(\RandMargin(S')_i=p_i,w_i\mid p_{<i},w_{<i})}
      \right) 
      \cdot \indicator{p_i,w_i \in G_{i}(\blackbox,\bar{S}_i,\bar{S}_i')}
      = 0 \right] = 1
  \end{align*}  
\end{innerproof}

\begin{innerproof}[Proof of (ii)]
  Recall that by the definition of $\event{out}$
  for every $i \in \event{out}$ it holds that
  $x' \notin B'_i$, and hence,
  conditioning on the previous outputs,
  $B'_i = B_i$. 
  We therefore get that the distribution of the $i^{th}$  output is also the same. 
  Formally, 
  $$\Pr(\RandMargin(S)_i=p_i,w_i\mid p_{<i},w_{<i})
  =
  \Pr(\RandMargin(S')_i=p_i,w_i\mid p_{<i},w_{<i}).$$
  
  This results in
  \begin{align*}
    \label{eq:2}
    \ln&
    \left(
      \frac{\Pr(\RandMargin(S)_i=p_i,w_i\mid p_{<i},w_{<i})}
      {\Pr(\RandMargin(S')_i=p_i,w_i\mid p_{<i},w_{<i})}
    \right)
    \cdot \indicator{p_i,w_i \in G_{i}(\blackbox,\bar{S}_i,\bar{S}_i')}
    = 0 \\
    &\Rightarrow
    \Pr\left[\sum_{i\in \event{out}}\ln
    \left(
      \frac{\Pr(\RandMargin(S)_i=p_i,w_i\mid p_{<i},w_{<i})}
      {\Pr(\RandMargin(S')_i=p_i,w_i\mid p_{<i},w_{<i})}
    \right)
    \cdot \indicator{p_i,w_i \in G_{i}(\blackbox,\bar{S}_i,\bar{S}_i')}
    = 0 \right] = 1
    .
  \end{align*}
\end{innerproof}

\begin{innerproof}[Proof of (iii)]
Note that, as we assume that the output of $\RandMargin$ includes the random Laplasian noise,
  then by fixing the past output-point $p_{<i},w_{<i}$
  we also fix $\bar{S}_i,\bar{S}_i'$.
  So,
  \begin{align*}
    \ln&
    \left(
      \frac{\Pr(\RandMargin(S)_i=p_i,w_i\mid p_{<i},w_{<i})}
      {\Pr(\RandMargin(S')_i=p_i,w_i\mid p_{<i},w_{<i})}
    \right)    
    \cdot \indicator{p_i,w_i  \in G_{i}(\blackbox,\bar{S}_i,\bar{S}_i')} \\
    & =
     \ln
    \left(
      \frac{\Pr(\blackbox(\bar{S}_i)=p_i,w_i)}{\Pr(\blackbox(\bar{S}'_i)=p_i,w_i)}
    \right)
    \cdot \indicator{p_i,w_i  \in G_{i}(\blackbox,\bar{S}_i,\bar{S}_i')}.
  \end{align*}
    
    Moreover, by the definition of the events $G_i$ it holds that
  \[
    \Pr\left[ \abs{\ln
    \left(
      \frac{\Pr(\blackbox(\bar{S}_i)=p_i,w_i)}{\Pr(\blackbox(\bar{S}'_i)=p_i,w_i)}
    \right)
    \cdot \indicator{p_i,w_i  \in G_{i}(\blackbox,\bar{S}_i,\bar{S}_i')}} \leq 2\varepsilon \right] = 1.
  \]
  which yields
  \begin{align*}
  \Pr&\left[
   \sum_{i\in\event{in}}
  \ln
    \left(
      \frac{\Pr(\blackbox(\bar{S}_i)=p_i,w_i)}{\Pr(\blackbox(\bar{S}'_i)=p_i,w_i)}
    \right)
    \cdot \indicator{p_i,w_i  \in G_{i}(\blackbox,\bar{S}_i,\bar{S}_i')} 
    > 
    \bigeps
    \right] \\
    &\leq
    \Pr\left[
    \sum_{i\in\event{in}} 2\varepsilon
    >
    \bigeps
    \right]    
    \leq
    \Pr\left[
    \abs{\event{in}}
    >
    \frac{\bigeps}{2\varepsilon}
    \right]
    \leq \delta,
  \end{align*}
 
 where the last inequality follows from 
 Claim~\ref{claim:concentrate} and from our choice of 
$$
 \bigeps = \bigO{\varepsilon\log(1/\delta)}.
$$
 \end{innerproof}

\end{proof}

\section{Utility}
\label{sec:accuracy}

\begin{proof}[Proof of Lemma~\ref{lem:accuracy}]
  First, we must ensure that at every iteration, with high probability,
  we have enough points left in $\bar{S}$.
  At the same time, we must ensure
  that the auxiliary algorithm $\IP$ will output an inner point of the given subset.
  Denote $a_j = w_j + \mu$.
  By the definition of the noise $w$ and the mean $\mu$,
  we get that for every iteration $i$: \quad
  $
    \Pr[a_i > 6 \Delta \log(1/\beta)] < \beta
  $.
  Hence, with probability $\geq 1- d\beta$, it holds that for every $i$
  $a_i \leq 6 \Delta \log(1/\beta)$.
  This means that the total number of removed points is at most
  $6d\Delta \log(1/\beta)$.
  Therefore, for a sample of size $6d\Delta \log(1/\beta)$
  with high probability $\bar{S}$ will contain enough points.

  Regarding the algorithm's accuracy, we notice that
  at every  iteration $j$, $\IP$ outputs a point
  which is at least the $a_j$-th largest point from
  \emph{the points left in the set}.
  This means that, in the worst case,
  we delete $a_j$ points from the data set at this iteration.
  Hence, again in worst case, we will output the $\sum_{j=1}^{i} a_j$-th largest point
  in the $j^{th}$ axis.

  By the above reasoning, 
  with high probability we can say that for every $i$ it holds that
  $a_j \leq 6 \Delta \log(1/\beta)$.
  Meaning that  every $p_j$ is at least the
  $\sum_{j=1}^d a_j \leq 6 d \Delta \log(1/\beta)$
  largest point in the axis.
  This implies that, 
  for sample of size
  $\bigO{\frac{d \Delta}{\alpha} \log(1/\alpha)\log(1/\beta)}$,
  denoting the by $h_p$ the hypothesis induces by the output of Algorithm~\ref{alg:main} 
  $
    \Pr_{S\sim \measure^n}[\errS{h_p}{S} \geq \alpha/2] \leq \beta/2.
  $
  Since the VC-dimension of the class $REC_d$ is $2d$, by Theorem~\ref{thm:vc} and the fact that the sample size is at least as the sample complexity bound
  $\bigO{\frac{1}{\alpha}\left(d\log\left(\frac{1}{\alpha}\right) + \log\left(\frac{1}{\beta}\right)\right)}$
  it holds that: \quad
  $
    \Pr_{S\sim \measure^n}[\errD{h_p} \geq \errS{h_p}{S} + \alpha/2] \leq \beta/2.
  $
  Combining the two bounds concludes the proof.
\end{proof}

\chapter{Universal Private Learning}
\label{chp:universal-privacy}

We prove the existence of universal private learners. 
As mentioned above, the existence of such algorithm is in sharp contrast to the impossibility results for PAC-learning.
Before introducing the algorithms and the results, we detail one tool which we will be using for doing private \emph{histogram count}, which is one of the most fundamental statistical tasks.
The task is, given a dataset, to count how many times each unique datum appears in the data.
The most common private solution is the Laplace mechanism, which guarantees $(\eps,0)$-differential privacy. 
The main caveat of this approach is that the error, in some cases, might over-accumulate, since we add noise to every possible domain point.
A different technique, specified in Algorithm~\ref{alg:sbh}, is to ignore zero-counts and also zero-out counts which do not exceed a certain (noisy) threshold. This allows us to avoid the above accumulation of error, at the cost of guaranteeing privacy with $\delta>0$.
Formally, 


\begin{algorithm}
  \caption{Stability based Histogram \cite{JMLR:v20:18-549}}\label{alg:sbh}
  \begin{algorithmic}[1]
    \State Input: Dataset $\sample\in \domain^n$
    \For{$x \in \domain$}
        \If{ $\realcount{\sample}(x) = 0$ }
            \State $\dpcount(x) \leftarrow 0$
        \Else %
            \State $\dpcount(x) \leftarrow \realcount{\sample}(x) + \Lap(2/\eps)$
            \If{ $\dpcount(x) < \frac{2}{\eps}\log\left(\frac{2}{\delta}\right)+1$ }
                \State $\dpcount(x) \leftarrow 0$
            \EndIf
        \EndIf
    \EndFor    
    \State Return $\dpcount$
  \end{algorithmic}
\end{algorithm}



\begin{theorem}[\cite{JMLR:v20:18-549}]
\label{thm:sbh}
The  \algname{Stability based Histogram}  algorithm is $(\eps,\delta)$-differentially private.
Moreover, for every domain point $x\in\domain$,
the resulting count $\dpcount(x)$ is such that if $\realcount{\sample}(x) = 0$ then 
$\dpcount(x) = 0$, and otherwise $\E\abs{\dpcount(x) - \realcount{\sample}(x)} \leq O\left(\frac{1}{\eps}\cdot\min\left\{\log\frac{1}{\delta},\,\realcount{\sample}(x)\right\}\right)$.
\end{theorem}

\section{Classification}
\label{sec:classification}


\begin{algorithm}
  \caption{PCL}\label{alg:pcl}
  \begin{algorithmic}[1]
    \State Input: Sample $S_n = \{(x_i,y_i)\}_{i=1}^n$ %
    \State Set $\cubesize=\frac{1}{n^{1/(2d)}}$ %
    \State Partition the space into equally sized cubes $\partition = C_1,C_2,\dots$ with side length $\cubesize$
    \State For any $x$ denote $C(x)$ the cube s.t. $x \in C(x)$
    \State Define the hypothesis $h_{\partition}$ s.t.
    $h_{\partition}(x) = \indicator{\sum_{x_i\in C(x)} y_i + Lap(1/\eps) > \frac{|C(x)|}{2}}$ \label{alg:PCL_define_h}
    \State Return $h_{\partition}$
  \end{algorithmic}
\end{algorithm}

We begin by studying UC learning over the bounded euclidean space $[0,1]^d$. Our classification algorithm is presented in Algorithm~\ref{alg:pcl}. In words: we partition the space into  equally sized cubes with side length $\cubesize$.
To classify a new point, take the bucket into which it falls 
and compute a noisy majority vote within this bucket.



\begin{theorem}
Algorithm~\ref{alg:pcl} is $\eps$-differentially private.
\end{theorem}

\begin{proof}
Histogram counts as used in Algorithm~\ref{alg:pcl} have global sensitivity $1$ (see \citep{dwork2014algorithmic}). 
Hence, adding Laplace noise of scale $1/\eps$ results in $\eps$-differential privacy.
Note that although Step~\ref{alg:PCL_define_h} in the algorithm seems to access the data twice, which might require the scale of the noise to be larger, this is not the case. To see this, notice that a different way of calculating the same majority-vote is by looking at the following sum
$\sum_{x\in C_j}(y_i - 1/2) + w_j$, where $w_j$ is the noise added to the cube $C_j$, and outputting 1 if it is greater than 0 and output 0 otherwise.
As such, this amounts to a single calculation with global sensitivity 1. 
Hence, by Theorem~\ref{thm:lap} the addition of Laplace noise of scale $1/\eps$ ensures that the noisy counts are private. As the final output is merely a post-processing of these counts, it is also $\eps$-private.
\end{proof}









\begin{theorem}
  \label{thm:const}
  Algorithm~\ref{alg:pcl} is universally-consistent. 
  
\end{theorem}

\begin{proof}[Proof of Theorem~\ref{thm:const}]

Given a test point $x\in \ourdomain^d$, denote by $A(x) =  \{X_i \in S \cap C(x) \}$ the set of points from $S$ in the same bucket with $x$,
and denote the size of that bucket as $N(x)=\Sigma_{i=1}^{n}{\indicator{X_i \in A(x)}}$. 
Also define
\begin{itemize}
    \item $\hat\reg_n(x) := \frac{1}{N(x)} \Sigma_{i:x_i \in A(x) }y_i$
    \item $\hat\reg^{\eps}_n(x) := \hat\reg_n(x) + w_j$,  
where $w_j$ is the noise added to $C(x)$.
\end{itemize}

\remove{\begin{enumerate}
\item $\hat\reg_n(x) := \frac{1}{N(x)} \Sigma_{i:x_i \in A(x) }y_i$
\item $\hat\reg^{\eps}_n(x) := \hat\reg_n(x) + w_j$ when $j$ is the index of $A(x)$
\end{enumerate}
}%

Note, that algorithm \algname{PCL} is a {\em plug-in classifier} w.r.t.\ $\hat\reg^{\eps}_n$. Hence, by Theorem~\ref{thm:plugin}, in order to prove that it is consistent it suffices to show that
\[
  \lim_{n\to\infty}\E\left[|\hat\reg_n^\eps(x)-\reg(x) |\right] = 0.
\]

By the triangle inequality, 
$
\label{eq:classif}
  \E\big[|\hat\reg^{\eps}_n(x)-\reg(x) |\big]
  \leq 
         \E\big[|\hat\reg^{\eps}_n(x)-\hat\reg_n(x) |\big]
  + \E\big[|\hat\reg_n(x) - \reg(x) |\big].    
$
In order to show that the first term goes to zero we  use  the following lemma.
\begin{lemma}[\cite{devroye2013probabilistic}]
  \label{lem:cubes}
  For any $k \in \N$ we have 
  $\Pr[N(x)\leq k] \xrightarrow[n\to \infty]{} 0,$ 
  where the probability is over sampling $S_n\sim\measure^n$ and sampling $x\sim\measure$.
\end{lemma}

Using the above lemma, we can bound the expected gap caused by the noise as follows.
\begin{align*}
    &\E_{S,x}\E_{\alg}\big[|\hat\eta^{\eps}_n(x)-\hat\eta_n(x) |\big] \leq 
    \E_{S,x}\E_{\alg}\big[|\hat\eta^{\eps}_n(x)-\hat\eta_n(x) |\cdot\indicator{N(x)>0}\big] + \Pr[N(x)=0]
    \\
    &=\E_{S,x}\;\E_{w_j \sim\Lap}\left[ \frac{|w_j|}{N(x)}\cdot\indicator{N(x)>0} \right] + \Pr[N(x)=0] \\
    &= \E_{S,x}\left[ \frac{1}{\eps N(x)} \cdot\indicator{N(x)>0} \right] + \Pr[N(x)=0] \\
    &= 
    \frac{1}{\eps} \Bigg(\mathbb{E}\left[ \frac{1}{N(x)} \mid 0<N(x) < M \right]\cdot \Pr(0<N(x) < M) \\ 
    &\quad\qquad + \mathbb{E}\left[ \frac{1}{N(x)} \mid N(x) \geq M\right ]\cdot \Pr(N(x) \geq M)\Bigg) \\ 
    &\quad + \Pr[N(x)=0] \leq  
    \frac{1}{\eps} \left(\Pr(N(x) < M) 
    + \frac{1}{M}
    \right). \numberthis\label{eq:noise_bound}
\end{align*}

Since this is true for every choice of $M$
and by using  Lemma~\ref{lem:cubes} again,
this also can be made arbitrarily small using a sufficiently large sample size.
Hence, 
\begin{equation}
    \label{eq:noise-decay}
    \mathbb{E}\big[|\hat\eta^{\eps}_n(x)-\hat\eta_n(x) |\big] \xrightarrow[]{n\to\infty} 0.
\end{equation}

Furthermore, we recall the following result by \cite{devroye2013probabilistic}

\begin{theorem}[\cite{devroye2013probabilistic}]
  For any $\cubesize$ and $n$ s.t. $\lim_{n\to\infty} \cubesize = 0$ 
  and
  $\lim_{n\to\infty}n\cubesize^d = \infty$
  we get that 
  $
  \lim_{n\to\infty}\E\left[|\hat\reg_n(x)-\reg(x) |\right] = 0 .
  $
\end{theorem}
Hence, the choice of $r=\frac{1}{n^{1/(2d)}}$, together with  \eqref{eq:noise-decay} completes the proof.
\end{proof}

As we mentioned, in the supplementary material we extend this construction to metric spaces with finite doubling dimension. 
 

\section{Density Estimation}
\label{sec:density-estimation}
We now turn to the problem of density estimation over $\R^d$. In particular, this implies private UC learning over $\R^d$ (rather than over $[0,1]^d$ as in the previous section). We present the following histogram-based approximation algorithm for density function.

\begin{algorithm}
  \caption{PCDE}\label{alg:pcde}
  \begin{algorithmic}[1]
    \State Input: Sample $S_n = \{(x_i)\}_{i=1}^n$.
    \State Set $\cubesize=\frac{1}{n^{1/(2d)}}$ 
    \State Partition the space into equally sized cubes $\partition := C_1,C_2,\dots$ with side length $\cubesize$
    \State Apply \algname{Stability based Histogram} with input $S_n$ to obtain estimates $\hat{c}_1,\hat{c}_2,\dots$ for $c_1,c_2,\dots$, where $c_j:=|\{x\in S_n : x\in C_j\}|$ denotes the number of input points in the cube $C_j$.
    \State For $x\in C_j$ denote $c(x)=c_j$ and $\dpcount(x)=\dpcount_j$.
    \State Return the function $\hat{f}_\sample$ defined as $\hat{f}_\sample(x):= \frac{1}{n \cubesize^d}\dpcount(x)$.
  \end{algorithmic}
\end{algorithm}

\begin{remark}\label{rem:zeroout}
Due to the noises in the counts, the output $\hat{f_\sample}$ of algorithm \algname{PCDE} might not be a density function: one needs to zero out negative terms it might contain and then to normalize it. This has a negligible effect on the distance from the underlying distribution, and we ignore it for simplicity.\footnote{In more detail, let $f$ denote the target distribution, let $\hat{f}$ denote the outcome of the algorithm, and suppose that the $L_1$ distance between $f$ and $\hat{f}$ is $w$. Now let $g$ denote $\hat{f}$ after zeroing out negative terms and after normalizing it (as in Remark~\ref{rem:zeroout}). An easy calculation (follows from the triangle inequality) shows that the $L_1$ distance between $f$ and $g$ is at most $O(w)$. This means that if the $L_1$ distance between $f$ and $\hat{f}$ goes to zero, then so does the distance between $f$ and $g$.}
\end{remark}



\label{sec:privacy}
\begin{theorem}
Algorithm~\ref{alg:pcde} is $(\eps,\delta)$-differentially private.
\end{theorem}

\begin{proof}
As \algname{Stability based Histogram} is $(\eps,\delta)$-differentially private, 
and since differential privacy is closed under post-processing, the output of \algname{PCDE} is also $(\eps,\delta)$-differentially private.
\end{proof}



\begin{theorem}
\label{thm:pcde-is-const}
The output of Algorithm~\ref{alg:pcde}, 
denoted by $\hat{f}_\sample$, is 
universally consistent for density estimation in $L_1$ norm.
Namely, for every distribution $\measure$ over $\R^d$ with density function $f$ we have
\[
\lim_{n\to\infty}
\E_{\sample\sim\measure^n}
\E_{\hat{f_\sample}\leftarrow\alg(S)}
\int \abs{f(x) - \hat{f}_\sample(x)}dx  = 0.
\]
\end{theorem}


\begin{proof}
For sample $\sample$ and the corresponding partition $\partition$, 
define the classic histogram-density estimation
\begin{equation}
    \label{def:hist}
  f_\sample(x)
  := \frac{1}{n \cubesize^d} 
  \sum_{i=1}^{n}\indicator{x_i\in C(x)}.  
\end{equation}
We will be using the following theorem 
\begin{theorem}[\citet{devroye2013probabilistic}, \citet{devroye1985nonparametric}]
\label{thm:dent_hist_const}
Let $f_\sample$ denote the standard histogram estimator (defined as in \eqref{def:hist}). Then,
\[
\lim_{n\to\infty}
\E_{S\sim P^n}
\int \abs{f(x) - f_\sample(x)}dx = 0.\]
\end{theorem}

Now, by the triangle inequality,
\begin{align*}
&\E_{S\sim\measure^n}
\E_{\hat{f_\sample}\leftarrow\alg(S)}
\int \abs{f(x) - \hat{f_\sample}(x)}dx \\
&\leq
\E_{S\sim\measure^n}
\E_{\hat{f_\sample}\leftarrow\alg(S)}
\int \abs{f_\sample(x) - \hat{f_\sample}(x)}dx 
+
\E_{S\sim\measure^n}
\int \abs{f(x) - f_\sample(x)}dx 
\numberthis\label{eq:triangle}.
\end{align*}

Hence, by Theorem~\ref{thm:dent_hist_const}, it suffices to show that 
$
  \lim_{n\to\infty}
    \E_{S,\hat{f_\sample}}
    \int \abs{f_\sample(x) - \hat{f_\sample}(x)}dx = 0.
$  
To this end, let $\arbitrarilysmall > 0$ be some parameter,
let $\support_0$ be such that there exist a cube $\supportcube_0$ of side-length $\support_0$ satisfying  
$\measure(\supportcube_0) > 1-\arbitrarilysmall.$ Now let $\supportcube$ denote the cube $\supportcube_0$ after extending it by 1 in each direction (so $\supportcube$ is a cube of side length $\support:=\support_0+2$).

\begin{remark}
Recall that the cubes $C_j$ defined by Algorithm~\ref{alg:pcde} are of side length $r\leq1$. Thus, any cube $C_j$ that intersects $\supportcube_0$ is contained in $\supportcube$.
\end{remark}


The interior of $\supportcube$ will be partitioned into $\frac{\support^d}{r^d}$ cubes of volume $r^d$.
If we restrict our calculation to $\supportcube$, 
we get that

\begin{align*}
\E_{S\sim\measure^n}&
\E_{\hat{f_\sample}\leftarrow\alg(S)}
\int_{\supportcube} \abs{f_\sample(x) - \hat{f_\sample}(x)}dx 
=
\int_{\supportcube} 
\E_{S}
\E_{\hat{f_\sample}}
\abs{f_\sample(x) - \hat{f_\sample}(x)}dx\\
\lesssim&\int_{\supportcube} 
\frac{1}{n\cubesize^d} \cdot
\frac{1}{\eps}\log\left(\frac{1}{\delta}\right)
dx
=
\frac{1}{n\cubesize^d} \cdot
\frac{1}{\eps}\log\left(\frac{1}{\delta}\right)\int_{\supportcube} 
dx\\
=& 
\support^d
\frac{1}{n\cubesize^d} \cdot
\frac{1}{\eps}\log\left(\frac{1}{\delta}\right)
= 
\frac{\support^d}{\eps\sqrt{n}}
\log\left(\frac{1}{\delta}\right),
\numberthis\label{eq:inside}
\end{align*}
where the inequality is by Theorem~\ref{thm:sbh} (after neglecting the constant hiding in the $O$-notation)
and the last equality is by the choice of 
$\cubesize = \frac{1}{n^{1/(2d)}}.$

Outside $\supportcube$, by its definition,
we have $\measure(\bar{\supportcube}) \leq \measure(\bar{\supportcube_0}) < \arbitrarilysmall$ 
and therefore %
\begin{equation}
    \label{eq:outside-vol}
\E\left[|S\cap \bar{\supportcube}|\right]
\leq
\E\left[|S\cap \bar{\supportcube_0}|\right]
< n\arbitrarilysmall.
\end{equation}

We can calculate that
\begin{align*}
\E_{S\sim\measure^n}
&\E_{\hat{f_\sample}\leftarrow\alg(S)}
\int_{\bar{\supportcube}} 
\abs{f_\sample(x) - \hat{f_\sample}(x)}dx 
=
\E_{S}
\int_{\bar{\supportcube}}
\E_{\hat{f_\sample}}
\abs{f_\sample(x) - \hat{f_\sample}(x)}dx\\
&\leq
\E_{S}
\int_{\bar{\supportcube}} 
\frac{1}{\eps n\cubesize^d} \cdot
c(x)\;
dx 
=
\frac{1}{\eps n\cubesize^d} \cdot
\E_{S}
\int_{\bar{\supportcube}} 
c(x)\;
dx\\
&\leq
\frac{1}{\eps n\cubesize^d} \cdot
\E_{S}
\sum_{C_j : C_j\cap \bar{\supportcube}\neq\emptyset} 
|S\cap C_j|\cdot r^d
\leq
\frac{1}{\eps n\cubesize^d} \cdot
\E_{S}
\sum_{C_j : C_j\subseteq \bar{\supportcube_0}} 
|S\cap C_j|\cdot r^d
\\
&\leq
\frac{1}{\eps n\cubesize^d} \cdot
\E_{S}
|S\cap \bar{\supportcube_0}|\cdot r^d
\leq
\frac{\arbitrarilysmall}{\eps}
,
\numberthis\label{eq:outside}
\end{align*}
where the first inequality follows from the properties of  \algname{Stability based Histogram}, and the last inequality follows from \eqref{eq:outside-vol}.

Finally, combining \eqref{eq:inside} and \eqref{eq:outside} yields
\begin{align*}
&\E_{S\sim\measure^n}
\E_{\hat{f_\sample}\leftarrow\alg(S)}
\int \abs{f_\sample(x) - \hat{f_\sample}(x)}dx \\
&= 
\E_{S\sim\measure^n}
\E_{\hat{f_\sample}\leftarrow\alg(S)}
\int_{\support} \abs{f_\sample(x) - \hat{f_\sample}(x)}dx 
+
\E_{S\sim\measure^n}
\E_{\hat{f_\sample}\leftarrow\alg(S)}
\int_{\bar{\support}} \abs{f_\sample(x) - \hat{f_\sample}(x)}dx \\
&\lesssim
\frac{\support^d}{\eps\sqrt{n}}
\log\left(\frac{1}{\delta}\right)
+
\frac{\arbitrarilysmall}{\eps}.
\end{align*}
As $\frac{\support^d}{\sqrt{n}} \xrightarrow[]{n\to\infty} 0$ 
and $\arbitrarilysmall$ can be arbitrarily small we get that
$$
  \lim_{n\to\infty}
    \E_{S,\hat{f_\sample}}
    \int \abs{f_\sample(x) - \hat{f_\sample}(x)}dx = 0.
$$
This completes the proof.
\end{proof}


\subsection{Consistent and Private Semi-Supervised Learning}

We next show that the above result yields an application to the setting of semi-supervised private learning. 
Let $\class$ be a class of concepts. 
Recall that in the semi-supervised setting, we are given two samples 
$\sample \in (\domain \times \{0,1\})^m$ and 
$\unlabeled \in (\domain \times \{\perp\})^{n}$. For simplicity, we will restrict our discussion in this subsection to the realizable setting. Let us first recall the definition of semi-supervised learning (SSL) in the distribution free PAC model.

\remove{Given a labeled sample, $\sample$, the empirical error of a hypothesis is defined as 
$\err_{\sample}(h) = \frac{1}{m}\abs{\{i \mid h(x_i) = y_i\}}$.
Given an unlabeled sample, $\unlabeled$, the empirical error of a hypothesis, with respect to a given concept $c$ is defined as 
$\err_{\unlabeled}(h,c) = \frac{1}{m}\abs{\{i \mid h(x_i) = c(x_i)\}}$.
}%









\remove{
\begin{definition}[Shattering]
Let $\class$ be a class of functions over a domain $\domain$. A set $S = (s_1,\ldots,s_k) \subseteq \domain$
is said to be \emph{shattered} by $\class$
if
$
\abs{ \{(f(s_1),\ldots,f(s_k)) : f\in \class\} } = 2^k.
$
\end{definition}

\begin{definition}[VC Dimension \cite{vapnik_uniform_1971}]
The \emph{VC dimension} of a class $\class$,
denoted as $\vc(\class)$, 
is the cardinality of the largest set shattered by $\class$.
If $\class$ shatters sets of arbitrary large cardinality, then it is said that $\vc(\class)=\infty$.
\end{definition}
}



\begin{definition}
An algorithm $\alg$ is said to be an \emph{SSL learning algorithm} for a class $\class$ if for every $\alpha,\beta$ 
there exist 
$m=m(\alpha,\beta,\class)$
and
$n=n(\alpha,\beta,\class)$ 
such that for every distribution $\measure$ 
it holds that 
$
\Pr_{\sample\sim\measure^m,\unlabeled\sim\bar{\measure}^n, h\sim\alg(\sample,\unlabeled)}\left[
\err_\measure(h) > \alpha
\right] < \beta, 
$
where $\bar{\measure}$ is the marginal distribution of the unlabeled samples.
\end{definition}



\begin{definition}[Private SSL]
An algorithm is said to be a \emph{PSSL-learning algorithm} for a class $\class$ if it is an SSL-learner for $\class$ and also it is $(\eps, \delta)$-differentially private.
\end{definition}



As in the standard learning model (where all examples are labeled), semi-supervised learning can be defined in the distribution-dependent setting, or consistent setting, as follows.



\begin{definition}
An algorithm $\alg$ is said to be a \emph{consistent semi-supervised learner} (CSSL for short) for a class $\class$ if for every $\alpha,\beta$
there exist 
$m=m(\alpha,\beta,\class)$
such that for every distribution $\measure$ there is some $n=n(\alpha,\beta,\class,\measure)$ for which 
$$
\Pr_{\sample\sim\measure^m,\unlabeled\sim\bar{\measure}^n, h\sim\alg(\sample,\unlabeled)}\left[
\err_\measure(h) > \alpha
\right] < \beta,
$$
where $\bar{\measure}$ is the marginal distribution of the unlabeled samples.
\end{definition}

Note that in the above definition, we required the labeled sample complexity to be {\em uniform} over all possible underlying distributions, while allowing the unlabeled sample complexity to depend on the underlying distribution. This is interesting because with differential privacy there are cases where semi-supervised learning cannot be done in the distribution-free setting. We show that it suffices for the unlabeled sample complexity to depend on the underlying distribution, while keeping the labeled sample complexity independent of it.


\begin{definition}
An algorithm is an
$(\eps,\delta)$-\emph{private consistent semi-supervised learner} (private-CSSL for short)
if it is a consistent semi-supervised learner and \small{$(\eps,\delta)$-differentially private}.
\end{definition}


For the following result, we will be using the notion of \emph{semi-private learning}.
The notion captures a scenario in which the data is sensitive, but the underlying distribution is not. This is modeled by defining a semi-supervised learning task in which the learner is required to preserve privacy only for the labeled part of the sample. Formally, a {\em semi-private} SSL algorithm is an SSL algorithm that satisfies differential privacy w.r.t.\ its labeled database (for every fixture of its unlabeled database).
\begin{theorem}[\cite{DBLP:journals/toc/BeimelNS16,BassilyMA19}]
\label{thm:semi-private}
for any concept class $\class$,
there exists a semi-private SSL algorithm 
which have a labeled sample complexity of
$m = \bigO{\frac{1}{\eps\alpha}VC(\class)\log\left(\frac{1}{\alpha\beta}\right)}$ 
and unlabeled sample complexity 
$n = \bigO{\frac{1}{\alpha}VC(\class)\log\left(\frac{1}{\alpha\beta}\right)}$.
\end{theorem}


As an application of our results for density estimation, we get the following corollary.
\begin{theorem}
\label{cor:ssl}
    For every class $\class$ over $\R^d$ with $VC(\class)<\infty$ 
    and for every $\eps,\delta$, 
    there exists a proper $(\eps,\delta)$-private-CSSL for $\class$ whose (labeled) sample complexity is 
    $
    m = \bigO{\frac{1}{\eps\alpha}VC(\class)\log\left(\frac{1}{\alpha\beta}\right)}.
    $
\end{theorem}

\begin{remark}
Notice that the labeled sample complexity is optimal, as a sample of size $\bigO{\vc(\class)}$ is necessary in order to learn a concept class $\class$ even 
without the privacy requirement.
\end{remark}

\begin{proof}[Proof of Theorem~\ref{cor:ssl}]
Let $\class$ be some class with 
$\vc(\class)<\infty$. Let $\alg$ be a semi-private SSL algorithm for $\class$, as guaranteed by Theorem~\ref{thm:semi-private}, and let $m_{\rm semi}$ and $n_{\rm semi}$ denote its labeled and unlabeled sample complexities, respectively.

Now fix an underlying distribution $\measure$ and let $f$ denote its marginal distribution over unlabeled examples. 
By Theorem~\ref{thm:pcde-is-const}
there is some $n = n\left(\frac{\beta}{n_{\rm semi}},\beta,f\right)$
s.t.\ we can privately generate a function $\hat{f}$, 
which is $\frac{\beta}{n_{\rm semi}}$ close (in \emph{total variation distance}) to the density function $f$ w.p.\ $1-\beta$. We proceed with the analysis assuming that this is the case.

Let $U\sim f^{n_{\rm semi}}$ denote a sample containing $n_{\rm semi}$ samples from $f$ and let $\hat{U}\sim\hat{f}^{n_{\rm semi}}$ denote a sample containing $n_{\rm semi}$ samples from $\hat{f}$. As $f,\hat{f}$ are $\frac{\beta}{n_{\rm semi}}$ close in total variation distance, we get that $f^{n_{\rm semi}}$ and $\hat{f}^{n_{\rm semi}}$ are $\beta$ close in total variation distance. 
By Theorem~\ref{thm:semi-private} we know that
$$
\Pr_{\substack{S\sim\measure^{m_{\rm semi}},\\U\sim f^{n_{\rm semi}}\\h\leftarrow\alg(S,U)}}[\err_{\measure}(h)>\alpha]<\beta,
$$
and so,
$$
\Pr_{\substack{S\sim\measure^{m_{\rm semi}},\\\hat{U}\sim \hat{f}^{n_{\rm semi}}\\h\leftarrow\alg(S,\hat{U})}}[\err_{\measure}(h)>\alpha]<2\beta.
$$

The unlabeled sample is accessed only via the private-density estimation algorithm,
and the labeled sample is accessed only via the semi-private learning method. The algorithm is therefore differentially private by composition and post-processing.
\end{proof}












\section{Metric Spaces with Finite Doubling Dimension}
\label{sec:doubling}
In this section, we extend our results to the more general setting of metric spaces with bounded doubling dimension. We first present some additional preliminaries.


\begin{definition}[Doubling dimension]
\label{def:doubling}
For a metric space $(\domain,\metric)$, let $\expddim > 0$ be the smallest integer such that every ball in $\domain$ can be covered by $\expddim$ balls of half the radius.
The \emph{doubling dimension} of $(\domain,\metric)$ is 
$\ddim(\domain) = \log_2(\expddim)$.
\end{definition}

\begin{definition}
For a metric space $(\domain,\metric)$, a set of points
$\cover$ %
in $\domain$  is said to be $\radius$-cover of $\domain$ if 
for every $x\in\domain$ there exist some $x'\in \cover$ s.t.
$\dist{x}{x'} \leq \radius.$
\end{definition}

\begin{definition}
For a metric space $(\domain,\metric)$, a set of points
$\packing$ %
in $\domain$  is said to be $\radius$-packing of $\domain$ if 
for every $x,x'\in\packing$
$\dist{x}{x'} \geq \radius.$

An $\radius$-packing $\packing$ is said to be {\em maximal} if for any $x\in\domain\setminus\packing$ it holds that $\packing\cup\{x\}$ is not an $\radius$-packing of $\domain$. 
Namely, it means that there is some $x'\in\packing$ s.t. $\dist{x}{x'} < \radius.$
\end{definition}

We will be leveraging the following classical connection between packing and covering.

\begin{theorem}[\citet{vershynin2018high}]\label{thm:coverPacking}
\phantom{blah}
\begin{enumerate}
    \item Let $\packing$ be a {\em maximal $\radius$-packing} of $\domain$,
    then $\packing$ is also an {\em $\radius$-cover} of $\domain.$
    \item If there exists an {\em $\radius$-cover} of $\domain$ of size $m$, then any {\em $2\radius$-packing} of $\domain$ is of  size at most $m$.
\end{enumerate}
\end{theorem}

\begin{definition}
A metric space $(\domain,\metric)$ is {\em separable} if it has a countable dense set. That is, there exists a {\em countable} set $Q\subseteq\domain$ such that every nonempty open subset of $\domain$ contains at least one element from $Q$.
\end{definition}

\subsection{Bounded Doubling Metric Spaces}

We begin by proving the following theorem.

\begin{theorem}\label{thm:bounded-doubling}
Let $\eps\leq1$ be a constant.
There is an $(\eps,0)$-differentially private universal consistent learner for every {\em bounded} and {\em separable} metric space with finite doubling dimension.
\end{theorem}

\begin{remark}
The separability requirement is in fact necessary. 
It has been shown by \citet{10.1214/20-AOS2029} that metric spaces which are not essentially separable have no consistent learning rules, even non-private ones.
\end{remark}

Let $(\domain,\metric)$ be a bounded and separable metric space with doubling dimension $d$.
Note that as $\domain$ has finite doubling dimension and is bounded, it has a finite covering for every $\radius$. Therefore, a {\em maximal} packing of $\domain$ will also be of finite size. 

Consider Algorithm~\ref{alg:pcl2}. The privacy properties of this algorithm are straightforward; we now proceed with its utility analysis.

\begin{algorithm}
  \caption{PCL2}\label{alg:pcl2}
  \begin{algorithmic}[1]
    \State Input: Sample $S_n = \{(x_i,y_i)\}_{i=1}^n$ 
    \State Set $\radius=\frac{1}{n^{1/(4d)}}$
    \State Let $\packing$ be an $\radius$ maximal packing of $\domain$.
    \State Partition the space into Voronoi cells centered in the elements of $\packing$: $\partition = \voronoi_1,\voronoi_2,\dots$.
    \State For any $x$ denote $\voronoi(x)$ the cell s.t. $x \in \voronoi(x)$   
    \State Define 
    $h_{\partition}(x) = \indicator{\sum_{x_i\in \voronoi(x)} y_i + Lap(1/\eps) > \frac{|\voronoi(x)|}{2}}$ \label{alg:PCL2_define_h}
    \State Return $h_{\partition}$
  \end{algorithmic}
\end{algorithm}

Given a test point $x\in \domain$, denote by $\vorsample(x) =  \{X_i \in S \cap \voronoi(x) \}$ the set of points from $S$ in the same bucket with $x$,
denote the size of that bucket as $\vorsize(x)=|\vorsample(x)|$, and lastly, $\vorsize(\voronoi):=\frac{1}{n}\Sigma_{i=1}^{n}{\indicator{X_i \in \voronoi}}$ which is the relative size of the sample points in $\voronoi$ from the entire sample. 

\begin{lemma}
\label{lem:vordiam}
    For every $\voronoi\in \partition$ it holds that $\diam{\voronoi} \leq 2\radius = \frac{2}{n^{1/(4d)}}$
\end{lemma}
\begin{proof}
Given a center point $\point_i$ from $\packing$,
denote by $\coverset_i$ the ball of radius $\radius$ around it, and by $\voronoi_i$ the Voronoi cell induced by it.
Let $a,b \in \voronoi_i$ be two points on $\voronoi_i$.
By the definition of Voronoi cells for any other center point 
$\point_j$,
it holds that
$\dist{a}{\point_j} \geq \dist{a}{\point_i}$ and $\dist{b}{\point_j} \geq \dist{b}{\point_i}$.
Therefore, if 
$\dist{a}{\point_i} > \radius$ or $\dist{b}{\point_i} > \radius$,
we will get that
$\forall \point\in\packing: \dist{a}{\point} > \radius$ or $\forall \point\in\packing: \dist{b}{\point} > \radius$,
which is a contradiction to the covering property of $\packing$.
Hence, we get that
$\dist{a}{\point_i}\leq \radius$ and $\dist{b}{\point_i}\leq \radius$ which, by the triangle inequality, result in 
$\dist{a}{b}\leq \radius$.
\end{proof}

\begin{lemma}
    \label{lem:doublingcubes}
  For any $k=k(n)$ such that $k(n)=o(n^{1/4})$ we have 
  $\Pr[\vorsize(x)\leq k] \xrightarrow[n\to \infty]{} 0,$ 
  where the probability is over sampling $S_n\sim\measure^n$ and sampling $x\sim\measure$.
\end{lemma}

\begin{remark}
This theorem (and its proof) holds for both bounded and unbounded spaces. We decided to provide it in this general form as we also make use of it to analyze the unbounded case later on.
\end{remark}

\begin{proof}
Let $\dradius = n^{1/(2d)}$, let $\ball \subseteq \domain$ be a ball of radius $\dradius$, and denote
$\bar{\ball} := \domain\setminus \ball$. 
Also let $\ball_{\rm big}$ be a ball cantered at the same point as $\ball$, but with twice the radius.


By the doubling dimension of the domain, it is possible to cover $\ball_{\rm big}$ with $\left(\frac{4\dradius}{\radius}\right)^d$ small balls each of radius $\radius/2$. 
By Theorem~\ref{thm:coverPacking}, this implies that {\em any} $r$-packing of $\ball$ is of size at most $\left(\frac{4\dradius}{\radius}\right)^d$.
In particular, $\packing\cap\ball_{\rm big}$ is of size at most $\left(\frac{4\dradius}{\radius}\right)^d$. Now observe that any Voronoi cell that intersects $\ball$ is contained in $\ball_{\rm big}$. As every such Voronoi cell corresponds to a unique point in $\packing\cap\ball_{\rm big}$, we get that there are at most $\left(\frac{4\dradius}{\radius}\right)^d$ Voronoi cell that intersects $\ball$.
As we set $\radius = \frac{1}{n^{1/(4d)}}$, this quantity equals $(4\dradius\cdot n^{1/(4d)})^d$.
{\small
\begin{align*}
&\Pr[\vorsize(x)\leq k]
\leq
\sum_{\voronoi\in\partition:\voronoi\cap \ball \neq \emptyset}\Pr(\vorsize(x)\leq k, x \in \voronoi) + \Pr(\bar{\ball}) \\
&\leq
\sum_{\voronoi\cap \ball \neq \emptyset, \Pr(\voronoi)\leq 2k/n}\Pr(\voronoi) 
+ \sum_{\substack{\voronoi\cap \ball \neq \emptyset\\ \Pr(\voronoi) > 2k/n}} Pr(\voronoi)\Pr\left(\vorsize(\voronoi)\leq \frac{k}{n}\right) + \Pr(\bar{\ball}) \\
&\leq
\frac{2k}{n}(4\dradius\cdot n^{1/(4d)})^d + \Pr(\bar{\ball}) 
+ 
\sum_{\substack{\voronoi\cap \ball \neq \emptyset\\ \Pr(\voronoi) > 2k/n}} Pr(\voronoi)
\Pr
\left(
\vorsize(\voronoi) - \E\left[\vorsize(\voronoi)\right]
\leq \frac{k}{n} - \Pr(\voronoi)
\right) \\
&\leq
\frac{2k}{n}(4\dradius\cdot n^{1/(4d)})^d + \Pr(\bar{\ball}) 
+ 
\sum_{\substack{\voronoi\cap S \neq \emptyset\\ \Pr(\voronoi) > 2k/n}} Pr(\voronoi) 
\Pr
\left(
\vorsize(\voronoi) - \E\left[\vorsize(\voronoi)\right]
\leq -\frac{\Pr(\voronoi)}{2}
\right) \numberthis\label{eq:ddimlemma}
\end{align*}
}
From this point, the proof proceeds in the same steps as in \citet[Theorem 6.2]{devroye2013probabilistic}. By Chebyshev's inequality,
\small{
\begin{align*}
\eqref{eq:ddimlemma} &\leq
\frac{2k}{n}(4\dradius\cdot n^{1/(4d)})^d + \Pr(\bar{\ball}) 
+ 
\sum_{\voronoi\cap S \neq \emptyset, \Pr(\voronoi)\geq 2k/n}
4\Pr(\voronoi)\frac{Var(\vorsize(\voronoi))}{\Pr(\voronoi)^2}\\
&\leq 
\frac{2k}{n}(4\dradius\cdot n^{1/(4d)})^d + \Pr(\bar{\ball}) 
+ 
\sum_{\voronoi\cap S \neq \emptyset, \Pr(\voronoi)\geq 2k/n}
4\Pr(\voronoi)\frac{\Pr(\voronoi)(1-\Pr(\voronoi))}{n\Pr(\voronoi)^2} 
\end{align*}
\begin{align*}
&\leq 
\frac{2k}{n}(4\dradius\cdot n^{1/(4d)})^d + \Pr(\bar{\ball}) 
+ 
\sum_{\voronoi\cap S \neq \emptyset, \Pr(\voronoi)\geq 2k/n}
4\Pr(\voronoi)\frac{\Pr(\voronoi)}{n\Pr(\voronoi)^2} \numberthis\label{eq:ddimlemma2}
\end{align*}
}
When the second inequality is due to the variance of the binomial variable $\vorsize(\voronoi)$.
\small{
\begin{align*}
\eqref{eq:ddimlemma2}
&\leq
\frac{2k+4}{n}(4\dradius\cdot n^{1/(4d)})^d + \Pr(\bar{\ball}) \\
&=
\frac{(4\dradius)^d}{n^{3/4}}(2k+4) + \Pr(\bar{\ball}) 
=
\frac{4^d}{n^{1/4}}(2k+4) + \Pr(\bar{\ball}) 
\end{align*}
}
Clearly, the first summand goes to zero when $n\to \infty$ (recall that $k=o(n^{1/4})$). As for the second summand, recall that $\dradius$ goes to $\infty$ when $n\to \infty$, and so $\Pr(\bar{\ball})$ goes to zero when $n\to \infty$.
\end{proof}

We will make use of the following theorem.

\begin{theorem}
\label{thm:plug-in-for-metric}
Given a separable metric space, a partition based classification rule is universally-consistent if 
\begin{enumerate}
    \item $\diam{\voronoi(x)} \xrightarrow[n\to \infty]{} 0$
    \item For every constant $k\in\N$ it holds that $\Pr[\vorsize(x)\leq k] \xrightarrow[n\to \infty]{} 0$
\end{enumerate}
\end{theorem}

This theorem is an extension of \citet[Theroem 6.1]{devroye2013probabilistic}, where it is stated only for $\R^d$. The proof of this theorem appears in Section~\ref{sec:additional} for completeness.

Putting it all together, we now prove the following theorem.

\begin{theorem}
\label{thm:pcl2_is_ucl}
  Algorithm~\ref{alg:pcl2} is universally-consistent. 
\end{theorem}

\begin{proof}

Define
\begin{itemize}
    \item $\hat\reg_n(x) := \frac{1}{\vorsize(x)} \Sigma_{i:x_i \in \vorsample(x) }y_i
    $
    \item $\hat\reg^{\eps}_n(x) := \hat\reg_n(x) + w_j$, where $w_j$ is the noise added to $\voronoi(x)$.
\end{itemize}

The proof is close in nature to the proof of Theorem~\ref{thm:const}.
We note, that algorithm \algname{PCL2} is a {\em plug-in classifier} w.r.t.\ $\hat\reg^{\eps}_n$. Hence, by Theorem~\ref{thm:plugin}, in order to prove that it is consistent it suffices to show that
\[
  \lim_{n\to\infty}\E\left[|\hat\reg_n^\eps(x)-\reg(x) |\right] = 0.
\]

By the triangle inequality, 
$
\label{eq:classif2}
  \E\big[|\hat\reg^{\eps}_n(x)-\reg(x) |\big]
  \leq 
         \E\big[|\hat\reg^{\eps}_n(x)-\hat\reg_n(x) |\big]
  + \E\big[|\hat\reg_n(x) - \reg(x) |\big].    
$
By the same arguments as in Theorem~\ref{thm:const} we get that
\begin{equation}
\label{eq:ddim_noise_bound}
 \E_{S,x}\E_{\alg}\big[|\hat\eta^{\eps}_n(x)-\hat\eta_n(x) |\big] \leq  
    \frac{1}{\eps} \left(\Pr(\vorsize(x) < M) 
    + \frac{1}{M}
    \right) 
\end{equation}

Since this is true for every choice of $M$
and by using  Lemma~\ref{lem:doublingcubes},
this also can be made arbitrarily small using sufficiently large sample size.
Hence, 
\begin{equation}
    \label{eq:doublin-noise-decay}
    \mathbb{E}\big[|\hat\eta^{\eps}_n(x)-\hat\eta_n(x) |\big] \xrightarrow[]{n\to\infty} 0.
\end{equation}

In order to show that 
$\lim_{n\to\infty}\E\big[|\hat\reg_n(x) - \reg(x) |\big] = 0$, by Theorem~\ref{thm:plug-in-for-metric}, it suffices to show that the following two conditions hold:
\begin{enumerate}
    \item $\diam{\voronoi(x)} \xrightarrow[n\to \infty]{} 0$
    \item $\Pr[\vorsize(x)\leq k] \xrightarrow[n\to \infty]{} 0$
\end{enumerate}
The first condition follows from Lemma~\ref{lem:vordiam} and the second condition follows from Lemma~\ref{lem:doublingcubes}.
\end{proof}

\subsection{Unbounded Doubling Metric Spaces}

We now extend the previous result to the case of {\em unbounded} doubling metric spaces. This extension comes at the cost of relaxing the privacy requirement from pure-privacy to approximated-privacy.  Formally, we show the following theorem.

\begin{theorem}\label{thm:unbounded-doubling-main}
Let $\eps\leq1$ be a constant and let $\delta:\N\rightarrow[0,1]$ be a function satisfying $\delta(n)=\omega(2^{-n^{1/4}})$.
There is an $(\eps,\delta(n))$-differentially private universal consistent learner for every separable (possibly unbounded) metric space with finite doubling dimension.
\end{theorem}




Let $(\domain,\metric)$ be a separable doubling metric space with doubling dimension $d$. Consider Algorithm~\ref{alg:pcl2b}.

\begin{algorithm}
  \caption{PCL2b}\label{alg:pcl2b}
  \begin{algorithmic}[1]
    \State Input: Sample $S_n = \{(x_i,y_i)\}_{i=1}^n$ 
    \State Set $\radius=\frac{1}{n^{1/(4d)}}$
    \State Let $\packing$ be a countable $\radius$ maximal packing of $\domain$.
    \State Partition the space into Voronoi cells centered in the elements of $\packing$: $\partition = \voronoi_1,\voronoi_2,\dots$.
    \State For any $x$ denote $\voronoi(x)$ the cell $\voronoi$ s.t.\ $x \in \voronoi$   
    \State Apply \algname{Stability based Histogram} with input $S_n$ to obtain estimates $\hat{c}_1,\hat{c}_2,\dots$ such that $\hat{c}_j\approx|\{x\in S_n : x\in \voronoi_j\}|$.   
    \State For any $x$ denote $\dpcount(x)=\dpcount_j$ such that $x\in \voronoi_j$.
    \State Apply \algname{Stability based Histogram} with input $S_n^1:=\{x\in S_n : y=1\}$ to obtain estimates $\hat{y}_1,\hat{y}_2,\dots$ such that $\hat{y}_j\approx|\{x\in S_n : y=1, x\in \voronoi_j\}|$.   
    \State For any $x$ denote $\dplabels(x)=\min\{\dplabels_j,\dpcount_j\}$ such that $x\in \voronoi_j$.    
    \State Define the hypothesis $h_{\partition}$ s.t.
    $h_{\partition}(x) = \indicator{\dplabels(x) > \frac{\dpcount(x)}{2}}$ \label{alg:PCL2b_define_h}
    \State Return $h_{\partition}$
  \end{algorithmic}
\end{algorithm}

Note that, as $\domain$ is separable, it has a countable covering and countable maximal packing for every $\radius$, and hence step $3$ is well-defined. 
\footnote{Clearly, every separable space has a countable covering. As the cardinality of a packing can be bounded by the cardinality of a cover, we get that the cardinality of every packing  must also be countable. Formally, 
given a $2\radius$-packing $\packing$ and an $\radius$-cover $\cover$,
as every $\radius$-ball centered around a point in $\cover$ contains at most one point from $\packing$, we get that there is an injection from $\packing$ to $\cover$. Hence the cardinality of $\packing$ is bounded by that of $\cover$.}
Moreover, by Theorem~\ref{thm:sbh}
the number of non-empty cells will be finite, hence the hypothesis defined at step $10$ is well-defined.



\begin{theorem}
\label{thm:unbounded-doubling}
Algorithm~\ref{alg:pcl2b} is $(2\eps,2\delta)$-differentially private.
\end{theorem}

\begin{proof}[Proof of Theorem~\ref{thm:unbounded-doubling}]
As \algname{Stability based Histogram} is $(\eps,\delta)$-differentially private, 
and since differential privacy is closed under post-processing, by standard composition theorems the output of \algname{PCL2b} is  $(2\eps,2\delta)$-differentially private.
\end{proof}

\begin{theorem}
  Algorithm~\ref{alg:pcl2b} is universally-consistent. 
\end{theorem}

\begin{proof}
Define
\begin{itemize}
    \item $\hat\reg_n(x) := \frac{1}{\vorsize(x)} \Sigma_{i:x_i \in \vorsample(x) }y_i$
    \item $\hat\reg^{\eps,\delta}_n(x) := \begin{cases}
\frac{\dplabels(x)}{\dpcount(x)} & \dpcount(x)\neq 0\\
0 & \dpcount(x) = 0 
\end{cases}.$
\end{itemize}

Most of the arguments which were made for Algorithm~\ref{alg:pcl2} in the proof of Theorem~\ref{thm:pcl2_is_ucl}
can be made also for Algorithm~\ref{alg:pcl2b}.
The only part of the proof that requires attention is to show that
$\lim_{n\to\infty}\E\big[|\hat\reg^{\eps,\delta}_n(x)-\hat\reg_n(x) |\big] = 0$. We calculate,

{\small
\begin{align*}
    &\E_{S,x,\alg}\big[|\hat\eta^{\eps,\delta}_n(x)-\hat\eta_n(x) |\big] \\
    &=
    \E_{S,x}\left[
    \E_{\alg}\left[
    |\hat\eta^{\eps,\delta}_n(x)-\hat\eta_n(x) |
    \right]  \right]  \\
    &\leq
    \E_{S,x}\left[
    \E_{\alg}\left[
    |\hat\eta^{\eps,\delta}_n(x)-\hat\eta_n(x) | \right]
    \cdot\indicator{N(x)>0}
      \right]+\Pr[N(x)=0]  \\
    &=
    \E_{S,x}\left[
    \E_{\alg}\left[
    \left|\frac{\dplabels(x)}{\dpcount(x)} - \frac{\sum_{i:x_i \in \vorsample(x) }y_i}{\vorsize(x)} \right|
    \right] \cdot\indicator{N(x)>0} \right]+\Pr[N(x)=0]  \\
    &\leq
    \E_{S,x}\left[
    \E_{\alg}\left[
    \left|\frac{\sum_{i:x_i \in \vorsample(x) }y_i}{\vorsize(x)}  - \frac{\dplabels(x)}{\vorsize(x)}\right|
     +
    \left|\frac{\dplabels(x)}{\vorsize(x)} - \frac{\dplabels(x)}{\dpcount(x)}\right| 
    \right] \cdot\indicator{N(x)>0}  \right] \\
    &\quad +\Pr[N(x)=0]  \\
    &\leq
    \E_{S,x}\left[
    O\left(\frac{\frac{1}{\eps}\log\frac{1}{\delta}}{N(x)} \right)  
    \cdot\indicator{N(x)>0}
    \right]+\Pr[N(x)=0]
\end{align*}
}
\small{
\begin{align*} 
    &\approx
    \frac{1}{\eps}\log\frac{1}{\delta}\cdot
    \E_{S,x}\left[ \frac{1}{N(x)} \cdot\indicator{N(x)>0} \right] +\Pr[N(x)=0]
    \\
    &= \frac{1}{\eps}\log\frac{1}{\delta}\cdot
    \Bigg(
    \E\left[\left. \frac{1}{N(x)} \cdot\indicator{N(x)>0} \right|N(x)< M\right]\\ &\qquad \cdot\Pr[N(x)<M]
    + 
    \E\left[\left. \frac{1}{N(x)} \right|N(x)\geq M\right]
    \cdot \Pr[N(x) \geq M] \Bigg)\\
    &\qquad +\Pr[N(x)=0]\\
    &\leq \frac{1}{\eps}\log\frac{1}{\delta}\cdot
    \Bigg(
    \Pr[N(x)<M]+ \frac{1}{M} \Bigg) +\Pr[N(x)=0]\\
    &\leq
    \frac{2}{\eps}\log\frac{1}{\delta} \left(\Pr(N(x) < M)
    + \frac{1}{M}
    \right)   
    \numberthis\label{eq:noise_bound_b}
\end{align*}
}

Since this is true for every choice of $M$
and by using  Lemma~\ref{lem:doublingcubes},
this also can be made arbitrarily small using sufficiently large sample size
\footnote{Note that, $\delta$ can decay exponentially fast as a function of $M$ and hence also as a function of $n$, allowing the same $\delta(n)=\omega(2^{-\sqrt{n}})$ dependency as in Theorem~\ref{thm:introDE}}.
Hence, 
\begin{equation}
    \label{eq:doublin-noise-decay-b}
    \mathbb{E}\big[|\hat\eta^{\eps,\delta}_n(x)-\hat\eta_n(x) |\big] \xrightarrow[]{n\to\infty} 0.
\end{equation}
\end{proof}

\begin{remark}
Unlike our results for the (unbounded) euclidean case, where we showed a construction for a {\em density estimator}, for (unbounded) metric spaces with finite doubling dimension we only show a {\em learner}. The reason is that in our construction of a density estimator for the euclidean case we needed to compute {\em volumes} of the cells in the partition. In general metric spaces, however, we do not have a canonical analogue for the volume of a cell.
\end{remark}


\section{Additional Details for Completeness}\label{sec:additional}

The proofs provided in this section are taken from \cite{devroye2013probabilistic}. We include them here for completeness, as in \cite{devroye2013probabilistic} these theorems are stated only for $\R^d$.

\begin{theorem}
\label{thm:our-plug-in-rule}
For a probability space $(\domain, \measure)$, 
let $\hat{\reg}:\domain\rightarrow[0,1]$ be any function,
and let $\hat{h}$ be the \emph{plug-in classification rule} w.r.t.\ $\hat{\reg}$. 
Then the following holds
\[
\Pr_{(X,Y)}[\hat{h}(X)\neq Y] - L^* \leq 
2 \E\left[\abs{\reg(X)-\hat{\reg}(X)}\right]
\]
\end{theorem}

\begin{proof}[Proof of Theorem~\ref{thm:our-plug-in-rule}]
Given $x\in\domain$,
if $h^*(x)=\hat{h}(x)$,
then
\[
\Pr_{(X,Y)}[\hat{h}(X)\neq Y \mid X=x]
=
\Pr_{(X,Y)}[h^*(X)\neq Y \mid X=x].
\]
On the other hand if
$h^*(x)\neq\hat{h}(x)$,
then
\[
\abs{\reg(X)-\hat{\reg}(X)} 
\geq
\abs{\reg(X)-\frac{1}{2}}.
\]
Therefore
\begin{align*}
    &\Pr_{(X,Y)}[h^*(X)\neq Y \mid X=x]
    -
    \Pr_{(X,Y)}[\hat{h}(X)\neq Y \mid X=x] \\
    &=(2\reg(x)-1)
    (\indicator{h^*(X)=1}-\indicator{\hat{h}(x)=1}) 
    =\abs{2\reg(x)-1}
    \cdot\indicator{h^*(X)\neq \hat{h}(x)} 
\end{align*}
By the law of total probability
\begin{align*}
    &\Pr_{(X,Y)}[\hat{h}(X)\neq Y] - L^* \\
    &=
    \int_{x\in\domain}
    \Pr_{(X,Y)}[h^*(X)\neq Y \mid X=x] 
    -
    \Pr_{(X,Y)}[\hat{h}(X)\neq Y \mid X=x]
    dx \\
    &=
    \int_{x\in\domain} \abs{2\reg(x)-1}
    \cdot\indicator{h^*(X)\neq \hat{h}(x)} \measure(x) dx \\
    &=\int_{x\in\domain} 2\abs{\reg(x)-\frac{1}{2}}
    \cdot\indicator{h^*(X)\neq \hat{h}(x)} \measure(x) dx \\ 
    &= 
    \E\left[2\abs{\reg(X)-\frac{1}{2}}\cdot\indicator{h^*(X)\neq\hat{h}(X)}\right] \\
    &\leq 
    2 \E\left[\abs{\reg(X)-\hat{\reg}(X)}\right]
\end{align*}
\end{proof}

\begin{proof}[Proof of Theorem~\ref{thm:plug-in-for-metric}]
As any partition rule is a special case of a plug-in estimator,
we need to show that
$\E\left[\abs{\reg(x)-\hat{\reg}_n(x)}\right] \xrightarrow[n\to \infty]{}0.$
Define $\bar{\reg}(x) := \E[\reg(z) \mid z\in \voronoi(x)]$.
By the triangle inequality
\begin{align*}
    \E\left[\abs{\reg(x)-\hat{\reg}_n(x)}\right]
    \leq 
    \E\left[\abs{\reg(x)-\bar{\reg}(x)}\right]
    +
    \E\left[\abs{\bar{\reg}(x)-\hat{\reg}_n(x)}\right]
\end{align*}
Examine the random variable $\vorsize(x)\hat{\reg}_n(x)$, 
which is the number of labeled-one points falling in the same "bucket" as $x$.
By conditioning upon which points fall in this bucket, %
the remaining randomness in this r.v. is only which of them will be labeled one. 
This is then simply a binomial random variable with "success" probability $\bar{\reg}(x)$ and $\vorsize(x)$ trials.
Thus, 
\small{
\begin{align*}
    &\E\Bigg[\abs{\bar{\reg}(x)-\hat{\reg}_n(x)} \mid \indicator{x_1 \in \voronoi(X) },\ldots, \indicator{x_n \in \voronoi(X) }\Bigg] \\
    &\leq
    \E\Bigg[\abs{\frac{\vorsize(x)\hat{\reg}_n(x)}{\vorsize(x)}-\bar{\reg}(x)}
    \mid \indicator{\vorsize(x) > 0}  , \indicator{x_1 \in \voronoi(X) },\ldots, \indicator{x_n \in \voronoi(X) }\Bigg] \\
    &\leq
    \E\Bigg[\left(\frac{\vorsize(x)\hat{\reg}_n(x)}{\vorsize(x)}-\bar{\reg}(x)\right)^2
    \mid \indicator{\vorsize(x) > 0}  , \indicator{x_1 \in \voronoi(X) },\ldots, \indicator{x_n \in \voronoi(X) }\Bigg]^{1/2} \\    
    &\leq 
    \E\Bigg[\frac{\bar{\reg}(x)(1-\bar{\reg}(x))}{\vorsize(x)} \indicator{\vorsize(x) > 0} 
    \mid \indicator{x_1 \in \voronoi(X) },\ldots, \indicator{x_n \in \voronoi(X) }\Bigg]^{1/2} \numberthis\label{eq:proof-from-a-book}
\end{align*}
}
When the second inequality is by the  Jensen inequality and the third by the variance of a binomial distribution.
Next, note that $\bar{\reg}(x)(1-\bar{\reg}(x))\leq\frac{1}{4}$ and hence,
\begin{align*}
    &\eqref{eq:proof-from-a-book}    
    \leq
    \E\left[\frac{1}{4\vorsize(X)} \mid \vorsize(X) > 0\right]^{1/2}\Pr[\vorsize(n) > 0] 
    + \Pr[\vorsize(X) = 0] \\
    &\leq 
    \E\left[\frac{1}{4\vorsize(X)} \mid \vorsize(X) > 0\right]^{1/2} \Pr[\vorsize(n) > 0] 
    + \Pr[\vorsize(X) = 0] \\
    &\leq 
    \frac{1}{2}\Pr[\vorsize(x) \leq k] + \frac{1}{2\sqrt{k}} + \Pr[\vorsize(X) = 0] \numberthis\label{eq:proof-from-a-book2}
\end{align*}
This is true for any k. Therefore \eqref{eq:proof-from-a-book2} can be made arbitrarily small by choosing k large enough and then by condition $(2)$ in the theorem's conditions.

Moving on to the first summand.
For any $\tau > 0$,
there exists a uniform continuous real-valued function $\reg_\tau$,
such that
$\E\left[\abs{\reg(x)-\reg_\tau(x)}\right] < \tau$.
Such a function exists, since for a separable metric space, 
the set of uniformly continuous, real valued functions is dense, in $\ell_1$ norm, in the set of all  continuous, real valued, functions.
Define
$\bar{\reg}_\tau(x) := \E[\reg_\tau(z) \mid z\in \voronoi(x)]$
and by the triangle inequality,
\begin{align*}
    &\E\left[\abs{\reg(x)-\bar{\reg}(x)}\right] \\
    &\leq   
    \E\left[\abs{\reg(x)-\reg_\tau(x)}\right]
    +
    \E\left[\abs{\reg_\tau(x)-\bar{\reg}_\tau(x)}\right] 
    +
    \E\left[\abs{\bar{\reg}_\tau(x)-\bar{\reg}(x)}\right] \\
    & =: (*) + (**) + (***).
\end{align*}
By the choice of $\reg_\tau(x)$, the $(*) \leq \tau$.
Also, by the definitions for $\bar{\reg}$ and $\bar{\reg}_\tau(x)$
the $(***) \leq (*) \leq \tau$.
Finally, as $\reg_\tau(x)$ is uniformly continuous, there exist some $\theta$ 
s.t. the difference between points which are $\theta$-close is bounded by $\tau$. 
Hence, we get that
$(**) \leq \tau + \Pr(\diam{\voronoi(x)} > \theta)$, when by condition (1) of the theorem's conditions, can be made less than $\tau$ for large enough $n$.
All in all, we showed that for any given $\tau$ we can ensure that $\E\left[\abs{\reg(x)-\bar{\reg}(x)}\right] < \tau,$ for large enough $n$.
\end{proof}

\chapter{Adaptive Data Analysis}

In this chapter, we investigate various extensions enabling adaptive data analysis for correlated observations. 
We show that some of the directions and tools from the literature can be used for this setting, if we look at the right setting or the appropriate measurements.

\section{Adaptive Generalization via Differential Privacy}
\label{sec:learning-with-low}

We start by extending the connection between differential privacy and adaptive data analysis into settings where the data is not sampled in an i.i.d.\ fashion, but rather there are some small/bounded dependencies.
We start by proving the following lemma, showing that differential privacy guarantees generalization in expectation. %
The proof of this lemma mimics the analysis of \citet{bassily2016algorithmic} for the i.i.d.\ setting. We extend the proof to the case where there are dependencies in the data, and show that we can "pay" for these dependencies in a way that scales with $\psi$.\\

\begin{lemma}[Expectation bound]
  \label{lem:dp-expect-bound}
  Let $\alg':(\X^n)^T\to 2^\X\times [T]$
  be an $(\varepsilon,\delta)$-differentially private algorithm.
  Let $\measure$ be a distribution over $\X^n$
  which has $\mixing$-Gibbs-dependence.
  Let $\vec{S} = (S_1,\ldots,S_T)$
  where for every $i$ $S_i\sim \measure$.
  Denote by $(h,t)$ the output of $\alg'(\vec{S})$.
  Then 
  $
    \abs{\E_{\vec{S},\alg'}\left[h(\measure)-h(S_t)\right]} \leq e^\varepsilon + T\delta + \mixing - 1.
  $
\end{lemma}




\remove{
\begin{proof} 
  \begin{align*}
    &\E_{\vec{S}}\left[\E_{(h,t)\sim\alg'(\vec{S})}[h(S_t)]\right] \\
    &= 
      \E_{\vec{S}}\left[      \E_{(h,t)\sim\alg'(\vec{S})}\left[\uf{n}\sum_{i=1}^nh(x_{t,i})\right]
      \right] \\
    =& 
      \uf{n}\sum_{i=1}^n\left[
      \E_{\vec{S}}\left[
      \E_{(h,t)\sim\alg'(\vec{S})}\left[h(x_{t,i})\right]
      \right]   
      \right] \\
    &= 
      \uf{n}\sum_{i=1}^n\left[
      \E_{\vec{S}}\left[
      \Pr_{(h,t)\sim\alg'(\vec{S})}\left[h(x_{t,i}) = 1\right]
      \right]    
      \right] \\ 
    =&
      \uf{n}\sum_{i=1}^n\left[
      \E_{\vec{S}}\left[
      \sum_{m=1}^T\Pr_{(h,t)\sim\alg'(\vec{S})}\left[
      h(x_{m,i}) = 1 \wedge t=m
      \right]
      \right]    
      \right] \\
    =& 
      \uf{n}\sum_{i=1}^n\Bigg[
      \E_{\vec{S}}\Bigg[
      \sum_{m=1}^T \E_{z\sim \measure_i(\cdot\mid x_m^{-i})} 
      \Bigg[ \\
      &\Pr_{(h,t)\sim\alg'(\vec{S})}
      \left[ h(x_{m,i}) = 1 \wedge t=m \right]
      \Bigg]
      \Bigg]
      \Bigg] \numberthis \label{eq:5} .
  \end{align*}
  If we replace the $i^{th}$ element
  on the $m^{th}$ sample in $\vec{S}$
  by $z$,
  denoting the modified multi-sample by
  $\vec{S}^{(m,i)\leftarrow z}$.

  Since $\alg'$ is $(\varepsilon,\delta)$-differentially private, we get that the above is at most
  \begin{align*}
    \eqref{eq:5} 
    \leq& 
          \uf{n}\sum_{i=1}^n\Bigg[
          \E_{\vec{S}}\Bigg[
          \sum_{m=1}^T 
          \E_{z\sim \measure_i(\cdot\mid x_m^{-i})}
          \Bigg[
          e^\varepsilon 
         \\ &\Pr_{(h,t)\sim\alg'(\vec{S}^{(m,i)\leftarrow z})}
          \Bigg[ h(x_{m,i}) = 1 \wedge t=m \Bigg]
          + \delta
          \Bigg]    
          \Bigg]
          \Bigg] \\
    =& 
       T\delta + 
       e^\varepsilon\cdot
       \uf{n}\sum_{i=1}^n
       \sum_{m=1}^T\Bigg[
       \E_{\vec{S}} \Bigg[
       \E_{z\sim \measure_i(\cdot\mid x_m^{-i})} \\ 
         &\Bigg[ \Pr_{(h,t)\sim\alg'(\vec{S}^{(m,i)\leftarrow z})}
          \Bigg[ h(x_{m,i}) = 1 \wedge t=m \Bigg]
          \Bigg]    
          \Bigg]
          \Bigg] \\
    =& 
       T\delta + 
       e^\varepsilon\cdot
       \uf{n}\sum_{i=1}^n
       \sum_{m=1}^T\Bigg[      
       \E_{\vec{S}}\Bigg[
       \E_{z\sim \measure_i(\cdot\mid x_m^{-i})} 
         \Bigg[\\ &\Pr_{(h,t)\sim\alg'(\vec{S})}
          \left[ h(z) = 1 \wedge t=m \right]
          \Bigg]    
          \Bigg]
          \Bigg] 
    \numberthis \label{eq:6}
  \end{align*}
  \todo[inline]{Add different explanation instead of the following.}
  Now, as $\measure$ is \mynotes{continue this.}
  we get that the above is at most
  \begin{align*}
    \eqref{eq:6}
    \leq&
       T\delta + 
       e^\varepsilon\cdot
       \uf{n}\sum_{i=1}^n
       \sum_{m=1}^T\Bigg[      
       \E_{\vec{S}}\Bigg[
       \mixing \E_{z\sim \measure_i} 
         \Bigg[ \\ 
      &\Pr_{(h,t)\sim\alg'(\vec{S})}
          \left[ h(z) = 1 \wedge t = m\right]
          \Bigg]    
          \Bigg]
          \Bigg]  \\
    \leq&
       T\delta + 
       \mixing e^\varepsilon
       \uf{n}\sum_{i=1}^n
       \sum_{m=1}^T\Bigg[      
       \E_{\vec{S}}\Bigg[
       \E_{z\sim \measure_i} 
         \Bigg[ \\
      &\Pr_{(h,t)\sim\alg'(\vec{S})}
          \left[ h(z) = 1 \wedge t = m\right]
          \Bigg]    
          \Bigg]
          \Bigg] \\
    \leq&
      T\delta + 
      \mixing e^\varepsilon
      \E_{\vec{S}}\Bigg[
      \E_{(h,t)\sim\alg'(\vec{S})} 
        \Big[ \uf{n}\sum_{i=1}^n
        \big[
      \E_{z\sim \measure_i}\big[      
      h(z)
      \big]
      \big]    
      \Big]
      \Bigg] \\
    =&
    T\delta + 
      \mixing e^\varepsilon\
      \E_{\vec{S},\alg'(\vec{S})}\left[ 
      h(\mu)
      \right] \\
    &\leq
      e\mixing + T\delta +  e^\varepsilon - 1 + 
      \E_{\vec{S},\alg'(\vec{S})}\left[ 
      h(\mu)
      \right],       
  \end{align*}
  where the last inequality is due to the fact that
  $ye^\varepsilon \leq ey + e^\varepsilon - 1 + y$
  for $y\leq 1$, $a \geq 1$ and $\varepsilon \geq 0$.

In summary,
  \begin{align*}
    \E_{\vec{S}} &
    \left[
      \E_{(h,t)\sim\alg'(\vec{S})}[h(S_t)]
    \right] \\
    &\leq
    \mixing + T\delta +  e^\varepsilon - 1 + 
    \E_{\vec{S},\alg'(\vec{S})}\left[ 
      h(\mu)
    \right].       
  \end{align*}

An identical argument yields,
  \begin{align*}
    \E_{\vec{S}} &
    \left[
      \E_{(h,t)\sim\alg'(\vec{S})}[h(S_t)]
    \right] \\
    &\geq
    \mixing + T\delta +  e^\varepsilon - 1 + 
    \E_{\vec{S},\alg'(\vec{S})}\left[ 
      h(\mu)
    \right];       
  \end{align*}
  combining the two completes the proof.
\end{proof}
}%


\begin{proof} 

We consider a {\em multi sample} $\vec{S}=(S_1,\dots,S_T)$, where $S_t=(x_{t,1},\dots,x_{t,n})\sim\measure$. We calculate,

  \begin{align*}
    &\E_{\vec{S}\sim\measure^T}\left[\E_{(h.t)\sim\alg'(\vec{S})}[h(S_t)]\right] \\
    =& 
      \E_{\vec{S}\sim\measure^T}\left[      \E_{(h.t)\sim\alg'(\vec{S})}\left[\uf{n}\sum_{i=1}^nh(x_{t,i})\right]
      \right] \\
    =& 
      \uf{n}\sum_{i=1}^n\left[
      \E_{\vec{S}\sim\measure^T}\left[
      \E_{(h.t)\sim\alg'(\vec{S})}\left[h(x_{t,i})\right]
      \right]   
      \right] \\
    =& 
      \uf{n}\sum_{i=1}^n\left[
      \E_{\vec{S}\sim\measure^T}\left[
      \Pr_{(h.t)\sim\alg'(\vec{S})}\left[h(x_{t,i}) = 1\right]
      \right]    
      \right] \\ 
    =&
      \uf{n}\sum_{i=1}^n\left[
      \E_{\vec{S}\sim\measure^T}\left[
      \sum_{m=1}^T\Pr_{(h.t)\sim\alg'(\vec{S})}\left[
      h(x_{m,i}) = 1 \wedge t=m
      \right]
      \right]    
      \right] \\
    =& 
      \uf{n}\sum_{i=1}^n\Bigg[
      \E_{\vec{S}\sim\measure^T}\Bigg[
      \E_{\vec{z}\sim \vec{\measure}_i\mid \vec{S}} 
      \Bigg[ \\
      &\sum_{m=1}^T
      \Pr_{(h.t)\sim\alg'(\vec{S})}
      \left[ h(x_{m,i}) = 1 \wedge t=m \right]
      \Bigg]
      \Bigg]
      \Bigg] \numberthis \label{eq:5} ,
  \end{align*}
 where $\vec{z}=(z_1,\dots,z_T)$ is a vector s.t.\ $z_t\sim\measure_i(\cdot\mid S_t^{-i})$. 
 Given a multi-sample $\vec{S}$ and an element $z$, we write $\vec{S}^{(m,i)\leftarrow z}$ to denote the multi-sample $\vec{S}$ after replacing the $i^{th}$ element
  in the $m^{th}$ sample $S_m$ with $z$. Since $\alg'$ is $(\varepsilon,\delta)$-differentially private we get that the above is at most
  \begin{align*}
    \eqref{eq:5} 
    \leq& 
          \uf{n}\sum_{i=1}^n\Bigg[
          \E_{\vec{S}\sim\measure^T}\Bigg[
          \E_{\vec{z}\sim \vec{\measure}_i\mid \vec{S}} 
          \Bigg[
          \sum_{m=1}^T 
          e^\varepsilon 
          \Pr_{(h.t)\sim\alg'(\vec{S}^{(m,i)\leftarrow z_m})} \\
          &\Bigg[ h(x_{m,i}) = 1 \wedge t=m \Bigg]
          + \delta
          \Bigg]    
          \Bigg]
          \Bigg] \\
    =& 
       T\delta + 
       e^\varepsilon\cdot
       \uf{n}\sum_{i=1}^n
       \sum_{m=1}^T\Bigg[
       \E_{\vec{S}\sim\measure^T}\Bigg[
       \E_{\vec{z}\sim \vec{\measure}_i\mid \vec{S}} 
         \Bigg[ \\ &\Pr_{(h.t)\sim\alg'(\vec{S}^{(m,i)\leftarrow z_m})}
          \Bigg[ h(x_{m,i}) = 1 \wedge t=m \Bigg]
          \Bigg]    
          \Bigg]
          \Bigg] \\
    =& 
       T\delta + 
       e^\varepsilon\cdot
       \uf{n}\sum_{i=1}^n
       \sum_{m=1}^T\Bigg[      
       \E_{\vec{S}\sim\measure^T}\Bigg[
       \E_{\vec{z}\sim \vec{\measure}_i\mid \vec{S}} 
         \Bigg[ \\
         &\Pr_{(h.t)\sim\alg'(\vec{S})}
          \left[ h(z_m) = 1 \wedge t=m \right]
          \Bigg]    
          \Bigg]
          \Bigg]  
\end{align*}
\begin{align*}  
    =& 
       T\delta + 
       e^\varepsilon\cdot
       \uf{n}\sum_{i=1}^n
       \sum_{m=1}^T\Bigg[      
       \E_{\vec{S}\sim\measure^T}\Bigg[
       \E_{\vec{z}\sim \vec{\measure}_i\mid \vec{S}} 
         \Bigg[ \\
         &\Pr_{(h.t)\sim\alg'(\vec{S})}
          \left[ h(z_t) = 1 \wedge t=m \right]
          \Bigg]    
          \Bigg]
          \Bigg]  \\
    =& 
       T\delta + 
       e^\varepsilon\cdot
       \uf{n}\sum_{i=1}^n
       \Bigg[      
       \E_{\vec{S}\sim\measure^T}\Bigg[
       \E_{\vec{z}\sim \vec{\measure}_i\mid \vec{S}} 
         \Bigg[ \\
         &\sum_{m=1}^T
         \Pr_{(h.t)\sim\alg'(\vec{S})}
          \left[ h(z_t) = 1 \wedge t=m \right]
          \Bigg]    
          \Bigg]
          \Bigg]  \\
    =& 
       T\delta + 
       e^\varepsilon\cdot
       \uf{n}\sum_{i=1}^n\Bigg[      
       \E_{\vec{S}\sim\measure^T}\Bigg[ \\
       &\E_{\vec{z}\sim \vec{\measure}_i\mid \vec{S}} 
         \left[ \Pr_{(h.t)\sim\alg'(\vec{S})}
          \left[ h(z_t) = 1 \right]
          \right]    
          \Bigg]
          \Bigg]  \\
    =& 
       T\delta + 
       e^\varepsilon
       \uf{n}\sum_{i=1}^n\left[      
       \E_{\vec{S}\sim\measure^T}\left[ \E_{\vec{z}\sim \vec{\measure}_i\mid \vec{S}}\left[
          \E_{(h.t)\sim\alg'(\vec{S})}
          \left[ h(z_t) \right]
          \right]    
          \right]
          \right]  \\
    =& 
       T\delta + 
       e^\varepsilon
       \uf{n}\sum_{i=1}^n\left[      
       \E_{\vec{S}\sim\measure^T}\left[
       \E_{(h.t)\sim\alg'(\vec{S})} 
         \left[
          \E_{\vec{z}\sim \vec{\measure}_i\mid \vec{S}}
          \left[      
          h(z_t)
          \right]
          \right]    
          \right]
          \right] \\
    =& 
       T\delta + 
       e^\varepsilon\cdot
       \uf{n}\sum_{i=1}^n
       \Bigg[   \E_{\vec{S}\sim\measure^T}
       \Bigg[\\   
       &\E_{(h.t)\sim\alg'(\vec{S})} 
         \left[
          \E_{z\sim \measure_i(\cdot\mid S_t^{-i})}
          \left[      
          h(z)
          \right]
          \right]    
          \Bigg]
          \Bigg]. \numberthis \label{eq:6}
  \end{align*}
Since total variation is 
a special case of the Wasserstein metric
$\mathcal{W}_1$, 
\small{Kantorovich-Rubinstein}
duality implies that
for two probability measures $\mu,\nu$
  on a space $\X$
  and any function $h:\X\to[0,1]$,
  we have
$\abs{\E_{z\sim\mu}[h(x)] - \E_{z\sim\nu}[h(z)]}\leq \tv{\mu}{\nu}$. Applying this to
  $\measure_i(\cdot\mid S_t^{-i})$ and $\measure_i$
  we get that the above is at most
  \begin{align*}
    \eqref{eq:6}
    \leq&
      T\delta + 
      e^\varepsilon\cdot
      \uf{n}\sum_{i=1}^n\bigg[
      \E_{\vec{S}\sim\measure^T}\Big[ 
      \E_{(h.t)\sim\alg'(\vec{S})}
        \big[\\
      &\E_{z\sim \measure_i}\big[
      h(z)
      \big]
      +
      \tv{\measure_i(\cdot\mid S_t^{-i})}{\measure_i}
      \big]    
      \Big]
      \bigg] \\
    \leq&
      \mixing +       
      T\delta + 
      e^\varepsilon\cdot
      \E_{\vec{S}\sim\measure^T}\Bigg[
      \E_{(h.t)\sim\alg'(\vec{S})} 
        \Big[\\
        &\uf{n}\sum_{i=1}^n
      \E_{z\sim \measure_i}\big[      
      h(z)
      \big]
      \Big]
      \Bigg] \\
    =&
      \mixing +       
      T\delta + 
      e^\varepsilon\cdot
      \E_{\vec{S},\alg'(\vec{S})}\left[ 
      h(\mu)
      \right] \\
    \leq&
      \mixing + T\delta +  e^\varepsilon - 1 + 
      \E_{\vec{S},\alg'(\vec{S})}\left[ 
      h(\mu)
      \right],       
  \end{align*}
  where the last inequality is due to the fact that
  $ye^\varepsilon \leq e^\varepsilon - 1 + y$
  for $y \leq 1$ and $\varepsilon \geq 0$. In summary,
  
  \begin{align*}
    \E_{\vec{S}\sim\measure^T} &
    \left[
      \E_{(h.t)\sim\alg'(\vec{S})}[h(S_t)]
    \right] \\
    &\leq
    \mixing + T\delta +  e^\varepsilon - 1 + 
    \E_{\vec{S},\alg'(\vec{S})}\left[ 
      h(\mu)
    \right].       
  \end{align*}
The other direction is symmetric.
\end{proof}



We use Lemma~\ref{lem:dp-expect-bound} to prove the following high-probability generalization bound for differentially private algorithms. 

\begin{theorem}[High probability bound]
  \label{thm:dp-high-prob-bound}
  Let $\varepsilon\in (0,1/3)$, $\delta \in (0,\varepsilon/4)$
  and $n \geq \frac{\log(2k\varepsilon/\delta)}{\varepsilon^2}$.
  Let $\alg:\X^n \to (2^\X)^k$ be an $(\varepsilon,\delta)$-differentially private algorithm.
  Let $\measure$ be a distribution over $\X^n$
  and
$S$ be a sample of size $n$
  drawn from $\measure$,
  and let $h_1,\ldots,h_k$ be the output of $\alg(S)$.
  Then
  \begin{equation*}
    \Pr_{S,\alg(S)}\left[\max_{i\in [k]}\abs{h_i(\measure)-h_i(S)}\geq 10\varepsilon+2\mixing\right]\leq \frac{\delta}{\varepsilon}      .
  \end{equation*}
\end{theorem}


The proof of Theorem~\ref{thm:dp-high-prob-bound} is almost identical to the analysis of \citet{bassily2016algorithmic}. It appears in the appendix for completeness. Intuitively, the proof is as follows.
We assume, towards contradiction, that there may be a differentially private algorithm that does not enjoy strong generalization guarantees.
We then use this mechanism to describe a different differentially private algorithm with a "boosted  inability" to generalize. That is, the proof goes by saying that if there is a differentially private algorithm whose generalization properties are not "very good" then there must exist a differentially private algorithm whose generalization properties are "bad", to the extent that contradicts Lemma~\ref{lem:dp-expect-bound}. 



\medskip
Our connection between Gibbs-dependence and differential privacy (Theorem~\ref{thm:main-result-dp}) now follows as a corollary of  Theorem~\ref{thm:dp-high-prob-bound}.

\begin{proof}[Proof of Theorem~\ref{thm:main-result-dp}]
  $\mechanism$ is $(\varepsilon,\delta)$-differentially private.
  Since $\adversary$ can only access the data via $\mechanism$,
  we can treat the pair $\adversary,\mechanism$ as a single algorithm $\alg$, which gets a sample $S\sim\measure$ as input and returns $k$ predicates, as output.
  By closure to post-processing, $\alg$
  is also $(\varepsilon,\delta)$-differentially private.
  Applying Theorem~\ref{thm:dp-high-prob-bound} on $\alg$ we get that
  \[
    \Pr\left[
      \max_{i\in [k]}\abs{h_i(\measure)-h_i(S)}
      \geq 10\varepsilon+2\mixing
    \right]
    \leq \frac{\delta}{\varepsilon}.
  \]
  
  Since $\mechanism$ is $(\alpha,\beta)$-empirically-accurate
  it holds that
  \[
    \Pr\left[
      \max_{i\in [k]} \abs{q_i(S) - a_i} > \alpha
    \right] \leq \beta.
  \]
Combining these two bounds with the triangle inequality, we get
  \[
    \Pr\left[
      \max_{i\in [k]} \abs{q_i(\measure) - a_i}
      > \alpha + 10\varepsilon+2\mixing
    \right]
    < \beta + \frac{\delta}{\varepsilon}. 
  \]
\end{proof}

\subsection[A Tight Negative Result]{A Tight Negative Result for Differential Privacy and Gibbs-Dependence}

In this section, we construct a distribution which is $\mixing$-Gibbs-Dependant,
and describe a differentially-private algorithm whose generalization gap w.r.t.\ this distribution is at least $\mixing$.
Hence, in general, the $\mixing$ factor attained on Theorem~\ref{thm:main-result-dp} is tight up to a constant.
Let $\X=[0,1]$ and define a measure $\measure$ over $\X^n$ by the following random process:
\begin{enumerate}[itemsep=0pt]
\item Sample a point $\seed\sim\uniform{[0,1]}$.
\item For every $i \in [n]:$ 
  \begin{enumerate}
  \item Sample $\sigma \sim \bernulli{\mixing}$.
    \begin{enumerate}
    \item If $\sigma = 1$ then $x_i = \seed$.
    \item Otherwise $x_i \sim \uniform{[0,1]}$
    \end{enumerate}
  \end{enumerate}
  
\item Return $\sample = \left(x_1,\ldots,x_n\right)$
\end{enumerate}

\begin{lemma}
  \label{lem:negative-1}
  The measure defined by the above process
  has $\mixing$-Gibbs-dependency.
\end{lemma}

\begin{proof}
  Initially, every marginal distribution is just uniform, i.e.
  $\measure_i \sim \uniform{[0,1]}$ and hence,
  for every $A\subseteq [0,1]$ it holds that
  $\measure_i(A) = |A|$.
  After conditioning, for every possible $x^{-i}$ and $x^*$, 
  we get that
  \begin{align*}
    \measure_i&(A\mid x^{-i},x^*) \\
    =& \measure_i(A\setminus\{\seed\}\mid x^{-i},x^*) + \measure_i(A\cap\{\seed\}\mid x^{-i},x^*) \\
    &\in \Big( |A|(1-\mixing),\; |A|(1-\mixing)+\mixing\Big).      
  \end{align*}
  Since the above holds for every choice of $x^*$, we also have that
    \[
    \measure_i(A\mid x^{-i})
    \in \Big( |A|(1-\mixing),\; |A|(1-\mixing)+\mixing\Big).
  \]
  Therefore, for every $A\subseteq [0,1]$ it holds that
  \begin{align*}
    &\abs{\measure_i(A) - \measure_i(A\mid x^{-i})} \\
    &\leq 
    \max\left\{ |A|-|A|(1-\mixing) \;,\;  
    |A|(1-\mixing)+\mixing - |A|
    \right\} \leq \mixing.      
  \end{align*}
  So
  $\tv{\measure_i(\cdot)}{\measure_i(\cdot\mid x^{-i})} \leq \mixing$.
  Plugging this bound to the Gibbs-dependency definition yields
  \begin{align*}
    \mixing (\measure)
    =
    \sup_{x\in\X^n}
    \E_{i\sim[n]}
    \tv{\measure_i(\cdot)}{\measure_i(\cdot\mid x^{-i})}
    \leq
    \mixing.
  \end{align*}
\end{proof}

We next describe an algorithm that, despite being differentially private, performs "badly" when executed on samples from the above measure $\measure$. Specifically, this algorithm is capable of identifying a predicate with generalization error $\Omega(\psi)$. This shows that our connection between differential privacy and generalization (in the correlated setting) is tight, in the sense that the generalization error of differentially private algorithms {\em can} grow with $\psi$. This matches our positive result (see Theorem~\ref{thm:main-result-dp}).

Our algorithm is specified in Algorithm~\ref{alg:dp-negative}. As a subroutine, we use the following result of \citet{DBLP:journals/jmlr/BunNS19} for privately computing histograms.
\begin{theorem}[Private histograms, \cite{DBLP:journals/jmlr/BunNS19}]\label{thm:hist}
There exists an $(\varepsilon,\delta)$-\hspace{0pt}differentially private algorithm that takes an input dataset $S\in\X^n$ and returns an a list $L\subseteq\X$ such that with probability at least $1-\beta$, every $x\in\X$ that appears at least $\bigO{\frac{1}{\varepsilon}\log\frac{1}{\beta\delta}}$ times in $S$ is included in $L$, and furthermore, every $x\in L$ appears at least {\em twice} in $S$.
\end{theorem}


\begin{algorithm}
  \caption{Deviating Private Algorithm}
  \label{alg:dp-negative}
\begin{algorithmic}
  \State {\bfseries Input:} A sample $\sample$, privacy parameters $\varepsilon,\delta$. 
  \State {\bfseries Tool used:} An $(\varepsilon,\delta)$-DP algorithm $\histo$ for histograms.
  \State $L \leftarrow \histo(\sample,\varepsilon,\delta)$
  \If{$L$ is empty}
  \State Return $h\equiv0$
  \Else
  \State Let $x$ be an arbitrary element in $L$
  \State Define $h:\X\to [0,1]$ as $h = \indicator{x}$
  \State Return $h$
  \EndIf
\end{algorithmic}
\end{algorithm}

\begin{lemma}
  \label{lem:negative-2}
  For every 
  $\beta>0$,
  every $n \geq \bigO{\frac{1}{\mixing\varepsilon}\log\frac{1}{\beta\delta}}$,
  and for every $\mixing < 1$
  Algorithm~\ref{alg:dp-negative}
  is $(\varepsilon,\delta)$-differentially private
  and it outputs a predicate $h:\X\to [0,1]$ s.t.
  \[
    \Pr\left[\abs{h(S) - h(\measure)} \geq \frac{\mixing}{2}\right]
    > 1 - \beta - \exp\left(-\frac{n}{8} \right).
  \]
\end{lemma}

\begin{proof}
First observe that Algorithm~\ref{alg:dp-negative} is $(\varepsilon,\delta)$-differentially private, as it merely post-processes the outcome of the private histogram algorithm.

Next observe that, by the definition of the underlying measure $\measure$, and by our choice of $n$, w.h.p.,\ there are many copies of $x^*$ in the dataset $S$. Formally, by the Chernoff bound,
  \begin{align*}
    \label{eq:negative-1}
    \Pr&\left[\uf{n}\abs{\{x' \in \sample \mid x' = \seed\}} < \uf{2}\mixing\right] \\
    &=
    \Pr\left[\uf{n}\sum_{i=1}^{n}\sigma_i < \uf{2}\mixing\right]
    \leq
    \exp\left(-\frac{n}{8} \right).    
  \end{align*}
In addition, the probability of any element $x\neq x^*$ appearing more than once in $S$ is simply zero. Thus, with probability at least $1-\exp\left(-\frac{n}{8} \right)$ we have that $x^*$ appears in $S$ at least $n\psi/2=\Omega(\frac{1}{\varepsilon}\log(\frac{1}{\beta}{\delta}))$ times, and every other element appears in $S$ at most once. By the properties of the private histogram algorithm (see Theorem~\ref{thm:hist}), in such a case, with probability at least $1-\beta$ we have that $L=\{x^*\}$, and Algorithm~\ref{alg:dp-negative} returns the hypothesis $h = \indicator{\seed}$. As $x^*$ appears many times in $S$, this predicate has "large" empirical value. On the other hand,
for such predicate it holds that 
\begin{align*}
  h(\measure)&
  = \E_{\bar{\seed},\bar{x_1},\ldots,\bar{x_n}}
  \left[\uf{n}\left(\sum_{i=1}^{n}h(\bar{x_i})\right)\right] \\
  &= \uf{n}\left(\sum_{i=1}^{n}
  \Pr_{\bar{\seed},\bar{x_1},\ldots,\bar{x_n}}
  \left[\bar{x_i}=\seed\right]\right)
  = 0    
\end{align*}
as the probability that for a fresh new sampling we will get $\bar{\seed}=\seed$ is zero,
implying that the probability that any point in the sample to be $\seed$ is also zero.

Overall, with probability at least $1-\beta-\exp\left(-\frac{n}{8} \right)$, the algorithm returns a predicate $h$ such that $h(S)\geq\psi/2$ but $h(\measure)=0$. 
  
\end{proof}

\section{Adaptive Learning Via Transcript Compression}
\label{sec:compression}

In this section we show how the notion of \emph{transcript compressibility} can be used to derive generalization bounds %
even if the data is not i.i.d.\ distributed. We start by recalling the notion of {\em transcript compression} by \citet{dwork2015generalization}. 
We denote by $\game_{n,k}(\analyst,\mechanism,\sample)$ the \emph{transcript} of the interaction between the mechanism $\mechanism$ and the analysis $\analyst$ during the adaptive accuracy game defined in Algorithm~\ref{alg:adapt-pop-game} with sample of size $n$ and $k$ queries. 
\begin{definition}[Transcript Compression \citep{dwork2015generalization}]\label{def:compression}
  We say that a mechanism $\mechanism$  enables
  \emph{transcript compression} to $b(n,k)$-bits,
  if for every deterministic analyst $\analyst$ there exist
  a set of possible transcripts $\mathcal{H}_\analyst$,
  of size $\abs{\mathcal{H}_\analyst} \leq 2^{b(n,k)}$,
  s.t. for every sample $\sample$ it holds that 
  $\Pr\left[ \game_{n,k}(\analyst,\mechanism,\sample)\in \mathcal{H} \right] = 1$.
\end{definition}
 
 Following \citet{DBLP:journals/corr/BassilyF16}, in this section we aim to design mechanisms that answer adaptively chosen queries while providing statistical accuracy, under the assumption that the given queries are {\em concentrated} around their expected value. Unlike \citet{DBLP:journals/corr/BassilyF16}, we aim to achieve this goal using the notion of {\em transcript compression}, rather than {\em typical-stability}. As we show, this allows for a significantly simpler analysis (and definitions). Formally,
 
 \begin{definition}\label{def:concentrated}
 Given a measure $\measure$ over $\X$, a query  $q:\X^n\rightarrow\R$, 
 and a parameter $\delta\in[0,1]$, we write $\gamma(q,\measure,\delta)$ to denote the minimal number $\gamma\in[0,1]$ such that
 \[
    \Pr_{S\sim\measure}\left[
      \abs{q(S) - \E_{T\sim \measure}[q(T)]} > \gamma
    \right] < \delta.
  \]
 \end{definition}
 
 That is, $\gamma(q,\measure,\delta)$ denotes the minimal number such that, without adaptivity, $q(S)$ deviates from its expectation by more than $\gamma(q,\measure,\delta)$  with probability at most $\delta$ when sampling $S\sim\measure$. 
 
 \begin{remark}
 The results in this section are not restricted to statistical queries. The results in this section hold for arbitrary queries (mapping $n$-tuples to the reals). 
 \end{remark}
 
 Consider again Algorithm~\ref{alg:adapt-pop-game} and Definition~\ref{def:adaptiveaccuracy} (the definition of statistical accuracy). We now use Definition~\ref{def:concentrated} in order to introduce a relaxation for statistical accuracy, in which the mechanism is allowed to incur $\gamma(q,\measure,\delta)$ as an additional error.  
 
 \begin{definition}
  A mechanism $\mechanism$ is {\em $(\alpha,\beta,\delta)$-statistically-query-accurate} for $k$ rounds given $n$ samples, 
  if for every distribution $\measure$ over $n$-tuples, and every adversary $\adversary$, 
  it holds that 
  \[
    \Pr_{\substack{S\sim\measure\\\texttt{Game}(\mechanism,k,\adversary,S)}}\left[
      \max_{i\in [k]} \abs{q_i(\measure) - a_i} > \alpha+\gamma(q_i,\measure,\delta)\right]
      \leq \beta.
  \]
\end{definition}


\begin{remark}
  For a statistical query $q$ and a product measure  $\measure$, by Hoeffding's inequality, we get that 
  $\gamma(q,\measure,\delta) = \sqrt{\frac{1}{2n}\ln\frac{2}{\delta}}$.
  Hence, for the i.i.d.\ regime, for large enough samples, the definition of  $(\alpha,\beta,\delta)$-statistical-query-accuracy is in fact equivalent (up to factor 2) to the original definition of $(\alpha,\beta)$-statistical-accuracy (Definition~\ref{def:adaptiveaccuracy}).
\end{remark}

We observe that the analysis of \citet{dwork2015generalization} for transcript compression easily extends to non-i.i.d.\ measures when given concentrated queries. Somewhat surprisingly, this simple technique essentially matches the bounds obtained using typical stability \citep{DBLP:journals/corr/BassilyF16}.  In the next lemma we show that (w.h.p.)\ an analyst interacting with a transcript-compressing mechanism cannot identify a query that overfits to the data.
 
\begin{lemma}
  \label{lem:compression-gen}
  Let $\mechanism$ be a mechanism which enables
  transcript compression to $b(n,k)$-bits.
  For every measure $\measure$ and every analyst $\analyst$,
  $$
    \Pr_{S, \game_{n,k}}\left[ \exists i: \abs{q_i(S) - q_i(\measure)} \geq \gamma(q,\measure,\delta) \right]
    \leq \delta \cdot k \cdot 2^{b(n,k)}
  $$
\end{lemma}



\begin{proof}%
  Fix an analyst $\analyst$.
  By Definition~\ref{def:compression}, there exist a set of transcripts $H_\analyst$
  of size at most $2^{b(n,k)}$.
  As every transcript consists of at most $k$ queries,
  there can be at most $k2^{b(n,k)}$ possible queries over all possible interactions between $\analyst$ and $\mechanism$.
  Denote this set of possible queries as $Q_\analyst$. By a union bound we get that
    \[
    \Pr_{S \sim \measure}\left[ \bigvee_{q \in Q_{\analyst}} \abs{q_i(S) - q_i(\measure)} \geq \gamma(q,\measure,\delta) \right]
    \leq \delta \cdot  k \cdot 2^{b(n,k)},    
  \]
  and hence
  \[
    \Pr_{S, \game_{n,k}}\left[ \exists i: \abs{q_i(S) - q_i(\measure)} \geq \gamma(q,\measure,\delta) \right]
    \leq k \cdot \delta \cdot 2^{b(n,k)}.    
  \]
\end{proof}

Using the above lemma, we prove our main theorem for this section.

\begin{theorem}
  \label{thm:compression-accuracy}
  Let $\mechanism$ be a mechanism which enables transcript compression to $b(n,k)$ bits
  and also exhibits $(\alpha, \beta)$-empirical-accuracy for k rounds given n samples.
  Then $\mechanism$ is also 
  $(\alpha,\beta +\delta  k 2^{b(n,k)},\delta)$-statistically-query-accurate, for every choice of $\delta$.
\end{theorem}

\begin{proof}%
  As $\mechanism$ is $(\alpha, \beta)$-empirically-accurate and also
  enables transcript compression to $b(n,k)$ bits,
  by Lemma~\ref{lem:compression-gen} and the union bound 
  \begin{align*}
    \Pr_{S, \game_{n,k}}
    &\big[
      \left(\exists i: \abs{q_i(S) - q_i(\measure)} > \gamma(q,\measure,\delta)  \right) \\
      &\vee
      \left(\exists i: \abs{q_i(S) - a_i} > \alpha \right)      
    \big] \\
    \leq
    \beta + &\delta \cdot k \cdot 2^{b(n,k)}.
  \end{align*}
    Hence by the triangle inequality
    \begin{align*}
      \Pr&_{S, \game_{n,k}}
      \left[ \exists i: \abs{a_i - q_i(\measure)} \geq \alpha + \gamma(q,\measure,\delta)  \right]\\
      &\leq 
      \beta + \delta \cdot k \cdot 2^{b(n,k)}.
    \end{align*}
  \end{proof}


 
Applying Theorem~\ref{thm:compression-accuracy} together with the transcript-compressing mechanisms of \citet{dwork2015generalization}, we get the following two results.

\begin{theorem}
\label{thm:main-compression-efficient}
    For every $\alpha,\delta$, there exists an $(\alpha,\beta,\delta)$-statistically-query-accurate mechanism for $k$ rounds given $n$ samples, where
    $\beta = k \cdot \delta \cdot 2^{k\cdot\log\frac{1}{\alpha}}$. The mechanism is computationally efficient.
  \end{theorem}


\begin{theorem}
\label{thm:main-compression-inefficient}
    For every $\delta$, there exists an $(\alpha,\beta,\delta)$-statistically-query-accurate mechanism for $k$ rounds given $n$ samples, where
    $$
    \alpha = \bigO{\left(\frac{\ln k}{n}\right)^{1/4}}
    \text{ and }
    \beta = k \cdot \delta \cdot 2^{\tildeO{\sqrt{n}\cdot\log|\X|\cdot(\log k)^{3/2}}}.
    $$ 
    The mechanism is computationally inefficient.
  \end{theorem}

\section{Application to Markov Chains}
\label{sec:umc}
In this section, we demonstrate an application of our tools and results regarding Gibbs-dependency and differential privacy to the problem of learning \emph{Markov chains} adaptively.
For our notion of dependence, it will be more convenient to analyze the 
\emph{Undirected Markov Chains}.
By the Hammersley-Clifford theorem
\citep{HammersleyClifford:1971,Clifford90}, every Markov measure on a chain graph with nonzero transition probabilities can be factorized according to pairwise potential functions (formalized below),
which we refer to as the {\em undirected Markov chain} formalization \citep{kontorovich12}.

The formal definition of an undirected Markov chain measure is as follows.

\begin{definition}
A measure $\measure$ over $\Omega^n$ is an {\em undirected Markov chain} if there are positive
functions $\{\potential_i\}_{i\in [n-1]}$, called {\em potential functions}, such that for any $x\in\Omega^n$
\[
\measure(x) = \frac{\prod_{i=1}^{n-1}\potential_i(x_i,x_{i+1})}
{\sum_{x'\in\Omega^n}\prod_{i=1}^{n-1}\potential_i(x'_i,x'_{i+1})}.
\]
\end{definition}
This is a special case of the more general \emph{undirected graphical model} (see \citet{lauritzen1996graphical}).
For the sake of convenience, we will use the following notations.

\begin{definition}
Let $\measure$ be an undirected Markov chain with potential functions $\{g_i\}_{i\in[n-1]}$. We denote the maximal and minimal potentials as follows.
\begin{itemize}
    \item $R_i(\measure) = \max_{a,b\in \Omega}\potential_i(a,b)$,
    \item $r_i(\measure) = \min_{a,b\in \Omega}\potential_i(a,b)$,
    \item $R(\measure) = \max_i \{R_i(\measure)\}$,
    \item $r(\measure) = \min_i \{r_i(\measure)\}$,
    \item $\bar{R}(\measure) := \frac{R(\measure)^2-r(\measure)^2}{R(\measure)^2+r(\measure)^2}$.
\end{itemize}
When $\measure$ is clear from the context, we simply write $R_i,r_i,R,r,\bar{R}$ instead of $R_i(\measure),$ $r_i(\measure),$ $R(\measure),$ $r(\measure),$ $\bar{R}(\measure)$.
\end{definition}

In order to apply our techniques to the case where the underlying distribution is an undirected Markov chain, we need to bound the Gibbs-dependency of undirected Markov chains. We first show the following lemma. (The proof of this lemma is deferred to a later part of this section.)

\begin{lemma}\label{lem:markovbound}
For every undirected Markov chain $\measure$ we have
\[
\mixing(\measure) \leq \bar{R}:=
\frac{R^2 - r^2}{R^2 + r^2}.
\]
\end{lemma}

That is, the above lemma bounds the Gibbs-dependency of undirected Markov chains as a function of the potential functions. Combining this bound with Corollary~\ref{cor:main}, we obtain the following result.


\begin{corollary}\label{cor:barR}
There exists a computationally efficient mechanism for answering $k$ adaptively chosen queries with the following properties. When given $n\geq 
m
=
\tilde{O}\left( \frac{\sqrt{k}}{\alpha^2}\log\frac{1}{\beta} \right)$ samples (an $n$-tuple) from an (unknown) undirected Markov chain $\measure$, the mechanism guarantees $\left(\alpha+2\bar{R}(\measure),\beta\right)$-statistical-accuracy (w.r.t.\ the underlying distribution $\measure$).
\end{corollary}

In particular, Corollary~\ref{cor:barR} shows that if the underlying chain $\measure$ satisfies $\bar{R}(\measure)\leq\alpha$, then the dependencies in $\measure$ can be "accommodated for free", in the sense that we can efficiently answer the same amount of adaptive queries as if the underlying distribution is a product distribution. We are not aware of an alternative method for answering this amount of adaptive queries under these conditions. As we next explain, we can broaden the  applicability of our techniques even further, by reducing  dependencies in the data as follows. The idea is to access only a part of the chain, obtained by "skipping" a fixed number of elements between two random samples. Formally,

\begin{definition}[Skipping Samples]\label{def:skipping}
Given a measure $\measure$ over $n$-tuples, and an integer $t$, we define the measure $\measure_{\times t}$ over $\frac{n}{t}$-tuples as follows.\footnote{We assume here for simplicity that $t$ divides $n$.} To sample from $\measure_{\times t}$, let $(x_0,x_1,x_2,x_3,\dots,x_{n-1})\sim\measure$, and return $(x_0,x_{t},x_{2t},x_{3t},\dots,x_{n-t})$.
\end{definition}


Intuitively, as Markov chains are "memoryless processes", skipping points in our sample (as in Definition~\ref{def:skipping}), should significantly reduce dependencies within the remaining points. We formalize this intuition and prove the following theorem. (The proof of this theorem is deferred to a later part of this section.)

\begin{theorem}
  \label{thm:gibbs_chains}
  For every undirected Markov chain $\measure$ and for every $t$ we have
  \[\mixing(\measure_{\times t}) \leq \mixing(\measure)^t.\]
\end{theorem}

That is, Theorem~\ref{thm:gibbs_chains} states that by reducing our sample size {\em linearly} with $t$, we could reduce dependencies within our sample {\em exponentially} in $t$. Combining this bound with Corollary~\ref{cor:main}, we obtain the following result.



\begin{corollary}
There exists a computationally efficient mechanism that is $(3\alpha,\beta)$-statistically-accurate for $k$ adaptively chosen queries, given a sample (an $n$-tuple) drawn from an underlying distribution $\measure_{\times t}$, where $\measure$ is an undirected Markov-chain, and where
$$
n\geq \tildeO{\frac{\log(1/\beta)\sqrt{k}}{\alpha^2}}\qquad\text{and}\qquad
t\geq \frac{\log (1/\alpha)}{\log (1/\bar{R})}.
$$
\end{corollary}



\begin{remark}
As a baseline, one can choose the "skipping parameter" $t$ to be sufficiently big s.t.\ the Gibbs-dependency would drop below $\beta/n$. 
As we mentioned in Section~\ref{subsec:into-gd-dp},
in that case the dependencies in the data would be small enough to the extent we could simply apply existing tools for answering queries {\em w.r.t.\ product distributions}, in order to answer adaptive queries w.r.t.\ $\measure_{\times t}$. However, this would require the skipping parameter $t$ to be as big as 
$\frac{\log(n/\beta)}{\log(1/\bar{R})}$, i.e., to increase by (roughly) a $\log(n)$ factor, which in turn, would result in a larger sample complexity.
\end{remark}



We next prove Lemma~\ref{lem:markovbound} and Theorem~\ref{thm:gibbs_chains}.

\begin{proof}[Proof of Lemma~\ref{lem:markovbound}]
For any $i\in [2,n-1]$,
\footnote{The case of $i\in\set{1,n}$ 
has an almost identical argument;
only the $g_{i-1}(v_{i-1},a)$ (respectively, $g_n(a,v_{i+1})$) factor
is omitted.
This does not affect the rest of the argument for the upper bound.
}
$a \in \Omega$ and $u,v \in \Omega^n$
\begin{align*}
  \measure_i(a\mid v^{-i})
  = \frac{\potential_{i-1}(v_{i-1},a)\potential_{i}(a,v_{i+1})}
  {\sum_{a'}\potential_{i-1}(v_{i-1},a')\potential_{i}(a',v_{i+1})}
\end{align*}

We will be using the following lemma of \citet{kontorovich12}:
\begin{lemma}
  For $n\in\N$ and $0\leq r\leq R$, consider the vectors ${\alpha}\in [0,\infty)^n$ and $f,g\in [r,R]^n$.
  Then,
  \[
  \uf{2}\sum_{i=1}^n \abs{\frac{\alpha_i f_i}{\sum_{j=1}^n \alpha_j f_j} - \frac{\alpha_i g_i}{\sum_{j=1}^n \alpha_j g_j}}
  \leq
  \frac{R-r}{R+r}
  .
  \]
\end{lemma}
We apply the lemma using
\begin{itemize}
\item $f_a = \potential_{i-1}(v_{i-1},a)\potential_{i}(a,v_{i+1})$
\item $h_a = \potential_{i-1}(u_{i-1},a)\potential_{i}(a,u_{i+1})$
\item $\alpha_a = 1$
\end{itemize}

and get that

\begin{align*}
  \uf{2}\sum_{a}\abs{\measure_i(a\mid u^{-i}) - \measure_i(a\mid v^{-i})}
  \leq \frac{R_{i-1}R_i - r_{i-1}r_i}{R_{i-1}R_i + r_{i-1}r_i}.
\end{align*}
It follows that
\begin{align*}
  \tvleft \measure_i&(\cdot)- \measure_i(\cdot\mid v^{-i})\tvright \\
  =& \uf{2}\sum_{a}\abs{\measure_i(a) - \measure_i(a\mid v^{-i})} \\
  =& \uf{2}\sum_{a}\abs{
    \sum_{u^{-i}}\measure_i(a\mid u^{-i})\measure^{-i}(u^{-i}) - \measure_i(a\mid v^{-i})} \\
  =& \uf{2}\sum_{a}\Bigg|\sum_{u^{-i}}\measure_i(a\mid u^{-i})\measure^{-i}(u^{-i}) \\
    &- \sum_{u^{-i}}\measure^{-i}(u^{-i})\measure_i(a\mid v^{-i})\Bigg| \\
  =& \uf{2}\sum_{a}\abs{\sum_{u^{-i}}\measure^{-i}(u^{-i})\left[
    \measure_i(a\mid u^{-i}) - \measure_i(a\mid v^{-i})
    \right]} \\
  \leq& \uf{2}\sum_{a}\sum_{u^{-i}}\measure^{-i}(u^{-i})\abs{
    \measure_i(a\mid u^{-i}) - \measure_i(a\mid v^{-i})} \\
  =& \sum_{u^{-i}}\measure^{-i}(u^{-i})\uf{2}\sum_{a}\abs{
    \measure_i(a\mid u^{-i}) - \measure_i(a\mid v^{-i})} \\
  \leq& \sum_{u^{-i}}\measure^{-i}(u^{-i})\frac{R_{i-1}R_i - r_{i-1}r_i}{R_{i-1}R_i + r_{i-1}r_i}
    = \frac{R_{i-1}R_i - r_{i-1}r_i}{R_{i-1}R_i + r_{i-1}r_i}
    .
\end{align*}
Finally,
\begin{align*}
  \mixing(\measure)
  = \sup_{v}\E_{i}&\tv{\measure_i(\cdot)}{\measure_i(\cdot\mid v^{-i})} \\
  &\leq \frac{R^2 - r^2}{R^2 + r^2}
  .
\end{align*}
\end{proof}


In order to prove Theorem~\ref{thm:gibbs_chains}, we first
establish the following notations:
\begin{itemize}
\item $x_{i \pm t} = x_{i-1},x_{i+t}$

\item $c_i^t := \sup\limits_{x_{i\pm t},y_{i\pm t}} \tvleft\measure_{i\pm (t-1)}(\cdot\mid x_{i\pm t})$ \\
\hspace*{2cm} $- \measure_{i\pm(t-1)}(\cdot\mid y_{i\pm t})\tvright$
\item $\symmix{i}{t}
  := \sup\limits_{x_{i\pm t},y_{i\pm t}}\tv{\measure_i(\cdot\mid x_{i\pm t})}{\measure_i(\cdot\mid y_{i\pm t})}$
\end{itemize}
Note that $$c_i^1 = \symmix{i}{i+1}
= \sup_{x_{i\pm 1},y_{i\pm 1}}\tv{\measure_i(\cdot\mid x_{i\pm 1})}{\measure_i(\cdot\mid y_{i\pm 1})}.$$

We will be using the following two lemmas (we prove these two lemmas after the proof of Theorem~\ref{thm:gibbs_chains}).

\begin{lemma}
  \label{lem:chains}
  $\symmix{i}{t} \leq \prod_{j=1}^{t}c_i^j$
\end{lemma}

\begin{lemma}
  \label{lem:chains_technical}
  For every $t$ and every $i$, there exist some $j$ s.t.
  $c_i^t\leq c_j^1$.
\end{lemma}

We now prove Theorem~\ref{thm:gibbs_chains} using Lemmas~\ref{lem:chains} and~\ref{lem:chains_technical}.

\begin{proof}[Proof of Theorem~\ref{thm:gibbs_chains}]
  Combining Lemma~\ref{lem:chains} and Lemma~\ref{lem:chains_technical} yields that
  for every undirected Markov measure $\measure$ and for every $t$,
  \begin{align*}
  \label{eq:chains1}
    \max_{i}&\symmix{i}{t} 
    \leq \max_{i}\prod_{j=1}^{t}c_i^j
    \leq \max_{i}\prod_{j=1}^{t}c_{l(j)}^1 \\
    &\leq \max_{i}\max_{l}(c_{l}^1)^t 
    = (\max_{i} c_{l}^1)^t 
    = (\mixing(\measure))^t, \numberthis   
  \end{align*}
  where the first inequality is due to Lemma~\ref{lem:chains}, the second is by Lemma~\ref{lem:chains_technical} \footnote{The function $l:[n]\to [n]$ returns for every coordinate $i$ the appropriate coordinate $l(i)$ which is guaranteed by Lemma~\ref{lem:chains_technical} to bound it.}. 
  The Last equality holds by the definitions of $\mixing$ and $c_l^1$.
  Since
  \begin{align*}
    \measure_i(\cdot) = 
    \sum_{x_{i\pm t}\in\Omega^2}
    \measure_i(\cdot\mid x_{i\pm t})
    \measure_{i\pm t}(x_{i\pm t})
    ,
  \end{align*}
  we have, by the undirected Markov property,
  \begin{align*}
  \mixing&(\measure_{\times t})
      =\max_i\sup_{y_{i\pm t}}\tv{\measure_i(\cdot)}{\measure_i(\cdot\mid y_{i\pm t})} \\
      &=
      \max_i\sup_{y_{i\pm t}}\tvleft
      \sum_{x_{i\pm t}\in\Omega^2}
      (\measure_i(\cdot\mid x_{i\pm t}) - \measure_i(\cdot\mid y_{i\pm t}))
      \measure(x_{i\pm t})\tvright\\
      &\leq 
      \max_i\sup_{y_{i\pm t}}\sum_{x_{i\pm t}\in\Omega^2} \tv{\measure_i(\cdot\mid x_{i\pm t})}{\measure_i(\cdot\mid y_{i\pm t})}\measure(x_{i\pm t}) \\    
      &\leq
      \max_{i} \symmix{i}{t} \leq (\mixing(\measure))^t,
  \end{align*}
  where the last inequality is due to \eqref{eq:chains1}.
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lem:chains}]
  Let $x_{i\pm t},y_{i\pm t}$ be some pairs of realization for the $i-t,i+t$ variable in the chain. By the law of total probability,
  \begin{align*}
    &\tv{\measure_i(\cdot\mid x_{i\pm t})}{\measure_i(\cdot\mid y_{i\pm t})} \\
    &=
\tvleft
    \sum_{x_{i\pm (t-1)}} \measure_i(\cdot\mid x_{i\pm (t-1)})\measure_{i\pm (t-1)}(x_{i\pm (t-1)}\mid x_{i\pm t}) \\
    &- \sum_{y_{i\pm (t-1)}} \measure_i(\cdot\mid y_{i\pm (t-1)})\measure_{i\pm (t-1)}(y_{i\pm (t-1)}\mid y_{i\pm t})\tvright. \numberthis \label{eq:umc-total}
  \end{align*}
  Define a coupling measure $\Pi_{i\pm (t-1)}(\cdot,\cdot\mid x_{i\pm t},y_{i\pm t})$
  whose marginals
  are $\mu_{i\pm (t-1)}(\cdot\mid x_{i\pm t})$ and $\mu_{i\pm (t-1)}(\cdot\mid y_{i\pm t})$.
  Then
  \begin{align*}
    \eqref{eq:umc-total}&=\\ &\tvleft\sum_{x_{i\pm (t-1)}}\sum_{y_{i\pm (t-1)}}    
    (\measure_i(\cdot\mid x_{i\pm (t-1)}) - \measure_i(\cdot\mid y_{i\pm (t-1)}))\\
    &\Pi_{i\pm (t-1)}(x_{i\pm (t-1)},y_{i\pm (t-1)}\mid x_{i\pm t},y_{i\pm t})\tvright\\
    &\leq
    \sum_{x_{i\pm (t-1)}}\sum_{y_{i\pm (t-1)}}   
    \tv{\measure_i(\cdot\mid x_{i\pm (t-1)})}{\measure_i(\cdot\mid y_{i\pm (t-1)})} \\
    &\Pi_{i\pm (t-1)}(x_{i\pm (t-1)},y_{i\pm (t-1)}\mid x_{i\pm t},y_{i\pm t})\\
    &\leq
    \symmix{i}{t-1}
    \sum_{x_{i\pm (t-1)}}\sum_{y_{i\pm (t-1)}}   
    1_{x_{i\pm (t-1)} \neq y_{i\pm (t-1)}}\\
    &\Pi_{i\pm (t-1)}(x_{i\pm (t-1)},y_{i\pm (t-1)}\mid x_{i\pm t},y_{i\pm t}).
  \end{align*}
  By the dual form of the total variation distance,\footnote{
  By the Kantorovich-Rubinstein duality of the specific case of total-Variation distance 
  $
    \tv{P}{Q}
    =
    \min_{\Pi\in\Delta(P,Q)}\int_{\Omega}\int_{\Omega}
    1_{x\ne y}
    d\Pi(x,y)
  $
  when $\Delta(P,Q)$ is the set of all the possible coupling of $P$ and $Q$.}
we can choose $\Pi_{i\pm (t-1)}$ to be such that
\begin{align*}
  &\tv{\measure_{i\pm (t-1)}(\cdot\mid x_{i\pm t})}{\measure_{i\pm (t-1)}(\cdot\mid y_{i\pm t})} \\
  &= 
  \sum_{x_{i\pm (t-1)}}\sum_{y_{i\pm (t-1)}}   
  1_{x_{i\pm (t-1)} \neq y_{i\pm (t-1)}}\\
  &\quad\Pi_{i\pm (t-1)}(x_{i\pm (t-1)},y_{i\pm (t-1)}\mid x_{i\pm t},y_{i\pm t})  
\end{align*}
and therefore
\begin{align*}
  &\tv{\measure_i(\cdot\mid x_{i\pm t})}{\measure_i(\cdot\mid y_{i\pm t})} \\
  &\leq
  \symmix{i}{t-1}
  \tv{\measure_{i\pm (t-1)}(\cdot\mid x_{i\pm t})}{\measure_{i\pm (t-1)}(\cdot\mid y_{i\pm t})}\\
  &\leq
  \symmix{i}{t-1} c_i^t.  
\end{align*}
Hence we get that
\[
  \symmix{i}{t}
  = \sup_{x_{i\pm t},y_{i\pm t}}\tv{\measure_i(\cdot\mid x_{i\pm t})}{\measure_i(\cdot\mid y_{i\pm t})}
  \leq
  \symmix{i}{t-1} c_i^t
\]
and by induction we get the lemma's result.
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lem:chains_technical}]
First we will show that for any $j,k$ the following holds
\begin{align*}
    \sup 
    &\tv{\measure_{j}(\cdot\mid x_{j-1},x_{j+k})}
    {\measure_{j}(\cdot\mid y_{j-1},y_{j+k})} \\
    &\leq
    \sup 
    \tv{\measure_{j}(\cdot\mid x_{j-1},x_{j+k-1})}
    {\measure_{j}(\cdot\mid y_{j-1},y_{j+k-1})}. \numberthis \label{eq:one-step}
\end{align*}
Indeed,
\begin{align*}
    &\sup 
    \tv{\measure_{j}(\cdot\mid x_{j-1},x_{j+k})}
    {\measure_{j}(\cdot\mid y_{j-1},y_{j+k})} \\
    &=
    \sup\tvleft\sum_{x_{j+k-1}} \measure_j(\cdot\mid x_{j-1},x_{j+k-1})\measure_{j+k-1}(x_{j+k-1}\mid x_{j+k}) \\
    &- \sum_{y_{j+k-1}} \measure_j(\cdot\mid y_{j-1},y_{j+k-1})\measure_{j+k-1}(y_{j+k-1}\mid y_{j+k})\tvright.
\end{align*}
Let $\Pi_{j+k-1}(\cdot,\cdot\mid x_{j+k},y_{j+k})$ be a coupling distribution whose marginal distributions are
$\measure_{j+k-1}(y_{j+k-1}\mid y_{j+k})$
and
$\measure_{j+k-1}(x_{j+k-1}\mid x_{j+k})$,
we get that the above is equal to
\begin{align*}
    \sup\tvleft\sum_{x_{j+k-1}}&\sum_{y_{j+k-1}} (\measure_j(\cdot\mid x_{j-1},x_{j+k-1}) \\
    &- \measure_j(\cdot\mid y_{j-1},y_{j+k-1}))\\
    &\Pi_{j+k-1}(x_{j+k-1},y_{j+k-1}\mid x_{j+k},y_{j+k})\tvright \\
    \leq
    \sup\sum&_{x_{j+k-1}}\sum_{y_{j+k-1}} \tvleft\measure_j(\cdot\mid x_{j-1},x_{j+k-1}) \\
    &- \measure_j(\cdot\mid y_{j-1},y_{j+k-1})\tvright\\
    &\Pi_{j+k-1}(x_{j+k-1},y_{j+k-1}\mid x_{j+k},y_{j+k}) \\
    \leq
    \sup
    \tvleft\measure_j&(\cdot\mid x_{j-1},x_{j+k-1}) 
    - \measure_j(\cdot\mid y_{j-1},y_{j+k-1})\tvright.
\end{align*}
Now we turn to the quantity of interest:
\begin{align*}
\sup_{x_{i\pm t},y_{i\pm t}} 
&\tv{\measure_{i\pm (t-1)}(\cdot\mid x_{i\pm t})}
{\measure_{i\pm (t-1)}(\cdot\mid y_{i\pm t})} \\
=
\sup_{x_{i\pm t},y_{i\pm t}}& 
\tvleft\sum_{x_{i+t-1}}\measure_{i\pm (t-1)}(\cdot\mid x_{i\pm t},x_{i+t-1}) \\
&\measure_{i+t-1}(x_{i+t-1}\mid x_{i\pm t}) \\
&- \sum_{y_{i+t-1}}\measure_{i\pm (t-1)}(\cdot\mid y_{i\pm t},y_{i+t-1})\\
&\measure_{i+t-1}(y_{i+t-1}\mid y_{i\pm t}) \tvright \\
=
\sup_{x_{i\pm t},y_{i\pm t}} 
&\tvleft\sum_{x_{i+t-1}}\measure_{i-t+1}(\cdot\mid x_{i- t},x_{i+t-1})\\
&\measure_{i+t-1}(x_{i+t-1}\mid x_{i\pm t}) \\
&- \sum_{y_{i+t-1}}\measure_{i-t+1}(\cdot\mid y_{i- t},y_{i+t-1})\\
&\measure_{i+t-1}(y_{i+t-1}\mid y_{i\pm t}) \tvright
.
\numberthis \label{eq:before}
\end{align*}
Let $\Pi_{i+t-1}(\cdot,\cdot\mid x_{i\pm t}),y_{i\pm t})$ be a coupling distribution whose marginals are $\measure_{i+t-1}(x_{i+t-1}\mid x_{i\pm t})$ and $\measure_{i+t-1}(y_{i+t-1}\mid y_{i\pm t})$.
Then the above is then equal to
\begin{align*}
    \eqref{eq:before}
    = \sup&_{x_{i\pm t},y_{i\pm t}} 
    \tvleft\sum_{x_{i+t-1}}\sum_{y_{i+t-1}}\measure_{i-t+1}(\cdot\mid x_{i-t},x_{i+t-1})\\
    &- \measure_{i-t+1}(\cdot\mid y_{i-t},y_{i+t-1})\\
    &\Pi_{i+t-1}(x_{i+t-1},y_{i+t-1}\mid x_{i\pm t},y_{i\pm t})\tvright \\
    \leq
    \sup&_{x_{i\pm t},y_{i\pm t}} 
    \sum_{x_{i+t-1}}\sum_{y_{i+t-1}}
    \tvleft\measure_{i-t+1}(\cdot\mid x_{i-t},x_{i+t-1})\\
    &- \measure_{i-t+1}(\cdot\mid y_{i-t},y_{i+t-1})\tvright\\
    &\Pi_{i+t-1}(x_{i+t-1},y_{i+t-1}\mid x_{i\pm t},y_{i\pm t})  .
\end{align*}
Plugging $j=i-t+1$ and $k=t-2$
into \eqref{eq:one-step} yields
\begin{align*}
    \sup_{x_{i\pm t},y_{i\pm t}} 
    &\tv{\measure_{i\pm (t-1)}(\cdot\mid x_{i\pm t})}
    {\measure_{i\pm (t-1)}(\cdot\mid y_{i\pm t})} \\
    \leq&
    \sup_{x_{i-t,i-t+2},y_{i-t,i-t+2}} 
    \tvleft\measure_{i-t+1}(\cdot\mid x_{i-t,i-t+2})\\ 
    &- 
    \measure_{i-t+1}(\cdot\mid y_{i-t,i-t+2})\tvright
\end{align*}
which completes the proof.
\end{proof}



\section{Additional proofs}


\subsection{Product measure}
\label{apn:prod}
We show the following claim
\begin{claim}
For a measure $\measure\sim\X^n$,
if for every $i \in [n]$ and for every possible $x \in \X^n$ it holds that
$\measure_i = \measure_i(\cdot\mid x^{-i}$, then
$\measure$ is a product measure.
\end{claim}
\begin{proof}
  For convenience, we denote for every $i\leq j$ the following notation $a_{i:j} = a_i,\ldots,a_j$.
  Now, for every $a \in \X^n$,
  $\measure(a) = \prod_{i\in [n]} \measure(a_i \mid a_{1:i-1}$.
  For start, we show that
  $\measure(a_2\mid a_1)=\measure(a_2)$. Indeed,
  \begin{align*}
      &\measure(a_2\mid a_1) = \sum_{a_3,\ldots,a_n}\measure(a_2\mid a_{1},a_{3:n})\cdot \measure(a_{3:n}\mid a_1) \\
      &=
      \sum_{a_3,\ldots,a_n}\measure(a_2)\cdot \measure(a_{3:n}\mid a_1) 
      = \measure(a_2)
  \end{align*}
  In the same way it can be shown that $\measure(a_3)=\measure(a_3\mid a_{1:2})$ and so on.
  
\end{proof}

\remove{
\begin{example}
For any measure which is $\mixing$-cross-correlated, has also has
bounded Dobrushin's coefficient.
Namely, if we denote the Dobrushin influence matrix by $\dobrushin$ then it holds that
$\norm{\dobrushin}_{\ell_1} \leq 2n\mixing$.
In particular if $\mixing < \frac{1}{2n}$
then the measure satisfies Dobrushin's uniqueness condition 
which implies various mathematical properties such as the uniqueness of the Gibbs measure \cite{Dobrushin1970PrescribingAS}, concentration of measure \cite{kontorovich2017concentration} and learnability \cite{DBLP:conf/colt/DaganDDJ19}.
See \mynotes{ref} for the proof of this connection.
\end{example}
}

\remove{
%
\begin{example}[Ising Model]
  The Ising Model is a fundamental, well known statistical  model of ferromagnetism and spin-systems \cite{ellis2012entropy,ising1925beitrag}.
  The Ising Model had been found fruitful on various fields outside of its original motivation, 
  such as computer vision \cite{inoue2001image,cohen2012image}, social science \cite{stauffer2008social} and neuroscience \cite{pmid25276772}.
  
  On the Ising model, the space is a set of electrons and a probability measure which determines their spin. 
  Mathematically, the state-space is $\{-1,1\}^n$ 
  and the distribution $\measure$ is parameterized by 
  $\beta$ and $h$, which stands for the inverse temperature and the external field respectfully.
  The formal definition of the distribution is 
  \[
    \measure(x) = Z \cdot e^{-\beta H_h(x)}
  \]
  when 
  \[
    H_h(x) 
    := -\sum_{i=1}^{n} x_i x_{i+1}
    - h \sum_{i=1}^{n} x_i
  \]
  and $Z$ is a normalization factor.
  The quantity of interest is the magnetic field of the system $m$,
  which is defined by the \emph{empirical average} spin, 
  or, in the various applications of the model, an average of some function applied to all of the variables.
  As the expected value is, relatively, simple to compute,
  the main question is the deviation between the two
  as a function of the parameters.
  Central-limit theorems are known to exist for the 
  \emph{high temperature} regime, i.e. for small $\beta$, on which the dependencies between the electrons are low \cite{ellis2012entropy},
  and recently also concentration results where proven \cite{chatterjee2010applications,chazottes2007concentration}.

  For simplicity we will treat the special case when the external field is everywhere zero.
  On this scenario the marginals are symmetric and hence,
for every $i$,
$\measure_i(1) = \measure_i(-1)$.
By the definition of the model
\[
\measure_o(\cdot\mid x^{-i})
= \left(\frac{e^v}{e^v+e^{-v}},\frac{e^{-v}}{e^{-v}+e^{-v}}\right)
\]
when $v := \beta(x_{i-1} + x_{i+1})$.
So
\begin{align*}
  &\tv{\measure_i(\cdot)}{\measure_i(\cdot\mid x^{-i})}\\ 
  &=
  \frac{1}{2}
  \left(
    \abs{\frac{1}{2} - \frac{e^v}{e^v+e^{-v}}}
    +
    \abs{\frac{1}{2} - \frac{e^{-v}}{e^v+e^{-v}}}
  \right).
\end{align*}
By linearity of expectation and by direct computation of the possible values of $v$
we get that
\begin{align*}
  \mixing(\measure) =
  \frac{1}{2}
  \mean{x}\left[
    \abs{\frac{1}{2} - \frac{e^v}{e^v+e^{-v}}}
  \right] + 
  \mean{x}\left[
    \abs{\frac{1}{2} - \frac{e^{-v}}{e^v+e^{-v}}}
  \right]\\
  =
  q\left(
    \abs{\frac{1}{2} - \frac{e^{2\beta}}{e^{2\beta}+e^{-2\beta}}}
    +
    \abs{\frac{1}{2} - \frac{e^{-2\beta}}{e^{2\beta}+e^{-2\beta}}}
  \right).    
\end{align*}
when $q := \measure_{i-1,i+1}((1,1)) + \measure_{i-1,i+1}((0,0))$.
Finally, by the possible values of $q$ and $\beta$ we get that
$\mixing(\measure) \leq \beta/2$.
This corresponds to the fact that high-temperature
induce low-correlation system.
  Such connection enables the use of the tools developed below for
  finite sample analysis of problems which are models using this model. 
\end{example}
}




\remove{
}

\remove{

  Next, defining the, stricter, notion of
  \emph{max}-cross-correlation,
  \[
    \mixing_{max}(\measure)
    = \max_{i\in [n]} \sup_{x \in \X^n}\tv{\measure_i(\cdot)}{\measure_i(\cdot\mid x^{-i})}
  \]
  and noting that as $\mixing(\measure) \leq \mixing_{max}(\measure)$,
  all the results in this paper regarding cross-correlation
  holds also for \emph{max}-cross-correlation.
  }
  

  
  
  \remove{
  Note that, Markov Fields 
  was introduces as a generalization of the Ising Model, 
  when the Ising Model's graph is a simple line.
  This implies that the maximal degree on the Ising Model's 
  graph has a maximal degree of 2 and hence 
  $\norm{\dobrushin}_1 \leq 2\mixing(\measure)$.
  }

\subsection{Proofs from Section~\ref{sec:learning-with-low}}

\begin{proof}[Proof of Theorem ~\ref{thm:dp-high-prob-bound}]
  Fix a measure $\measure$ on $\X^n$ with Gibbs-dependence $\psi_n$, and fix an $(\varepsilon,\delta)$-differentially private algorithm that takes a sample $S\in\X^n$ and returns $k$ predicates $h_1,\dots,h_k:\X\rightarrow\{0,1\}$. Assume towards contradiction that
  \begin{equation}
    \label{eq:3}
    \Pr_{S,\alg(S)}\left[
      \max_{i\in [k]}\abs{h_i(\measure)-h_i(S)}\geq 10\varepsilon + 2\mixing
    \right]
    \geq \frac{\delta}{\varepsilon}.
  \end{equation}


\begin{algorithm}[tb]
   \caption{Auxiliary Algorithm $\alg'$}
    \label{alg:contra-alg}
\begin{algorithmic}
   \State {\bfseries Input:} $\vec{S}=(S_1,\ldots,S_T)$, where $T=\frac{\varepsilon}{\delta}$.

\State $F \leftarrow \emptyset$ 
   \For{$t \in [T]$}
   \State $(h^t_1,\ldots,h^t_k) \leftarrow \alg(S_t)$
   \State $H_t \leftarrow \{(h^t_1,t),\ldots,(h^t_k,t)\}$
   \State $\bar{H_t} \leftarrow \{1-h\mid h\in H_t\}$
   \State $F \leftarrow F \cup H_t \cup \bar{H_t}$
   \EndFor
   \State Sample $(h^*,t^*)$ from $F$ using the exponential mechanism. Specifically, sample $(h^*,t^*)\in F$ with probability proportional to $\exp\big(\frac{\varepsilon n}{2}\left(h^*(S_{t^*})-h^*(\measure)\right)\big)$.
   \State Return $(h^*,t^*)$
\end{algorithmic}
\end{algorithm}

  Consider the procedure
  described in Algorithm~\ref{alg:contra-alg}. As differential private algorithms are immune to post-processing
  and by the composition theorem,
  $\alg'$ is by itself $(2\varepsilon, \delta)$-differentially private. 
  Given a multi-set $\vec{S}$ sampled from $\measure^T$,
  by \eqref{eq:3} we get that 
  \[\forall t: \Pr_{S_t,\alg(S_t)}\left[
      \max_{i\in [k]}\abs{h^t_i(\measure)-h^t_i(S_t)}\geq 10\varepsilon + 2\mixing
    \right] \geq \frac{\delta}{\varepsilon},\]
  and hence, by setting $T=\frac{\varepsilon}{\delta}$, we have that 
  \begin{align*}
    \Pr_{\vec{S},\alg'(\vec{S})} & 
    \left[
      \max_{t\in [T],i\in [k]}\abs{h^t_i(\measure)-h^t_i(S_t)} 
      \geq 10\varepsilon + 2\mixing
    \right] \\
    &\geq 
    1 - \left(1-\frac{\delta}{\varepsilon}\right)^T\geq \uf{2}.
  \end{align*}
  By Markov's inequality,
  \[\E_{\vec{S},\alg'(\vec{S})}\left[
      \max_{t\in [T],i\in [k]}\abs{h^t_i(\measure)-h^t_i(S_t)}
    \right]
    \geq 5\varepsilon + \mixing.\]
    Now
  the set constructed in the algorithm's run, $F$,  contains also the negation of each predicate,
  and hence
  \begin{align*}
    &\E_{\vec{S},\alg'(\vec{S})}
    \left[
      \max_{(h,t)\in F}\big\{h(S_t) - h(\measure)\big\}
    \right] \\
    &=\E_{\vec{S},\alg'(\vec{S})}\left[
      \max_{t\in [T],i\in [k]}\abs{h^t_i(\measure)-h^t_i(S_t)}
    \right] 
    \geq 5\varepsilon + \mixing      .
  \end{align*}

By the properties of the exponential mechanism
  (see \citet{mcsherry2007mechanism} or \citet{bassily2016algorithmic}), denoting the output of the algorithm by $(h^*,t^*)$ we get that
  \begin{align*}
    &\E_{(h^*,t^*)}
    \left[
    h^*(S_{t^*}) - h^*(\measure)
    \right] \\
    &\geq
    \max_{(h,t)\in F} \{ h^*(S_{t^*}) - h^*(\measure) \}
    - \frac{2}{\varepsilon n}\log(2Tk).
  \end{align*}
  Taking expectation on both sides yields
  \begin{align*}
    &\E_{\vec{S},\alg'(\vec{S})} 
    \left[
    h^*(S_{t^*}) - h^*(\measure)
    \right] \\
    &\geq
    \E_{\vec{S},\alg'(\vec{S})}\left[
    \max_{(h,t)\in F} \{ h^*(S_{t^*}) - h^*(\measure) \}
    \right]
    - \frac{2}{\varepsilon n}\log(2Tk) \\
    &\geq
    5\varepsilon + \mixing - \frac{2}{\varepsilon n}\log(2k\varepsilon/\delta).
  \end{align*}
For $n \geq \frac{\log(2k\varepsilon/\delta)}{\varepsilon^2}$,
  this is at least $2\varepsilon + \mixing$ which contradicts Lemma~\ref{lem:dp-expect-bound}.
\end{proof}

\cleardoublepage

\phantomsection

\addcontentsline{toc}{chapter}{Bibliography}

\bibliography{refs}
\bibliographystyle{plainnat}

\includepdf[pages=-]{abstract-hebrew-reverse.pdf}
\includepdf[pages=-]{phd-cover-example-signed.pdf}


\end{document}
